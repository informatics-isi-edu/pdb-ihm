#!/usr/bin/python3
# 
# Copyright 2020 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Client for PDB workflow processing.
"""

import os
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
from email.mime.text import MIMEText
import socket
from dateutil.parser import parse
from socket import gaierror, EAI_AGAIN
from requests import HTTPError
import pickle
import csv
from ast import literal_eval

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote

mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'

class PDBClient (object):
    """Network client for PDB workflow processing.
    """
    ## Derived from the ermrest iobox service client

    def __init__(self, **kwargs):
        self.baseuri = kwargs.get("baseuri")
        o = urlparse(self.baseuri)
        self.scheme = o[0]
        host_port = o[1].split(":")
        self.host = host_port[0]
        self.path = o.path
        self.port = None
        if len(host_port) > 1:
            self.port = host_port[1]
        self.make_mmCIF = kwargs.get("make_mmCIF")
        self.py_rcsb_db = kwargs.get("py_rcsb_db")
        self.python_bin = kwargs.get("python_bin")
        self.pickle_file = kwargs.get("pickle_file")
        self.vocab_data_map_file = kwargs.get("vocab_data_map_file")
        self.tables_groups = kwargs.get("tables_groups")
        self.entry = kwargs.get("entry")
        self.credentials = kwargs.get("credentials")
        self.store = HatracStore(
            self.scheme, 
            self.host,
            self.credentials
        )
        self.catalog = PollingErmrestCatalog(
            self.scheme, 
            self.host,
            self.path.split('/')[-1],
            self.credentials
        )
        self.catalog.dcctx['cid'] = 'pipeline/pdb'
        self.mail_server = kwargs.get("mail_server")
        self.mail_sender = kwargs.get("mail_sender")
        self.mail_receiver = kwargs.get("mail_receiver")
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')

    """
    Send email notification
    """
    def sendMail(self, subject, text):
        if self.mail_server and self.mail_sender and self.mail_receiver:
            retry = 0
            ready = False
            while not ready:
                try:
                    msg = MIMEText('%s\n\n%s' % (text, mail_footer), 'plain')
                    msg['Subject'] = subject
                    msg['From'] = self.mail_sender
                    msg['To'] = self.mail_receiver
                    s = smtplib.SMTP(self.mail_server)
                    s.sendmail(self.mail_sender, self.mail_receiver.split(','), msg.as_string())
                    s.quit()
                    self.logger.debug('Sent email notification.')
                    ready = True
                except socket.gaierror as e:
                    if e.errno == socket.EAI_AGAIN:
                        time.sleep(100)
                        retry = retry + 1
                        ready = retry > 10
                    else:
                        ready = True
                    if ready:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    ready = True

    """
    Start the process for generating pyramidal tiles
    """
    def start(self):
        try:
            rid = os.getenv('RID', None)
            if rid == None:
                self.logger.error('RID was not specified in the environment')
                return
            
            action = os.getenv('action', None)
            if action == None:
                self.logger.error('"action" was not specified in the environment')
                return
                
            if action == 'entry':
                self.process_mmCIF('PDB','entry', rid)
            elif action == 'Entry_Related_File':
                self.process_Entry_Related_File('PDB', 'Entry_Related_File', rid)
            else:
                self.logger.error('Unknown action: "%s".' % action)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: unexpected exception', '%s\nThe process might have been stopped\n' % ''.join(traceback.format_exception(et, ev, tb)))
            raise
        
    """
    Process the csv/tsv file of the Entry_Related_File table
    """
    def process_Entry_Related_File(self, schema, table, rid, status='in progress'):
        """
        Query for detecting the record to be processed
        """
        url = '/entity/%s:%s/RID=%s/Process_Status=%s' % (urlquote(schema), urlquote(table), urlquote(rid), urlquote(status))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row = resp.json()[0]
        filename = row['File_Name']
        file_url = row['File_URL']
        md5 = row['File_MD5']
        structure_id = row['structure_id']
            
        creation_time = row['RCT']
        
        """
        Extract the file from hatrac
        """
        f = self.getHatracFile(filename, file_url)
        
        if f == None:
            return
        
        if md5 == None:
            md5 = self.md5hex(f)
            self.logger.debug("The MD5 was computed and it is: %s" % md5)
                                        
        """
        Load data from the csv/tsv files
        """
        url = '/attribute/%s:%s/RID=%s/Vocab:File_Type/Name' % (urlquote(schema), urlquote(table), urlquote(rid))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        tname = resp.json()[0]['Name']

        url = '/attribute/%s:%s/RID=%s/Vocab:File_Format/Name' % (urlquote(schema), urlquote(table), urlquote(rid))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        file_type = resp.json()[0]['Name']
        delimiter = '\t' if file_type=='TSV' else ','
        
        # see where the csv/tsv file is
        # suppose it is fpath
        returncode = self.loadTableFromCVS(f, delimiter, tname, structure_id)

        if returncode != 0:
            """
            Update the slide table with the failure result.
            """
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'error'
                                  })
            return
                        
        """
        Update the image table with the success result.
        """
        url = '/attribute/Vocab:workflow_status/Name={}/ID'.format(urlquote('Record Ready'))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        ID = resp.json()[0]['ID']
        
        obj = {}
        obj['RID'] = rid
        obj['Workflow_Status'] = ID
        obj['Process_Status'] = 'success'
        obj['File_MD5'] = md5
        columns = ['Workflow_Status', 'Process_Status', 'File_MD5']
        self.updateAttributes(schema,
                         table,
                         rid,
                         columns,
                         obj)
   
        self.logger.debug('Ended PDB Processing for the %s:%s table.' % (schema, table)) 
        
    """
    Process the mmCIF file of the entry table
    """
    def process_mmCIF(self, schema, table, rid, status='in progress'):
        """
        Query for detecting the record to be processed
        """
        url = '/entity/%s:%s/RID=%s/Process_Status=%s' % (urlquote(schema), urlquote(table), urlquote(rid), urlquote(status))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row = resp.json()[0]
        filename = row['mmCIF_File_Name']
        file_url = row['mmCIF_File_URL']
        md5 = row['mmCIF_File_MD5']
        id = row['id']
            
        creation_time = row['RCT']
        
        """
        Extract the file from hatrac
        """
        f = self.getHatracFile(filename, file_url)
        
        if f == None:
            return
        
        """
        Get the md5 if necessary
        """
        if md5 == None:
            md5 = self.md5hex(f)
            self.logger.debug("The MD5 was computed and it is: %s" % md5)
            
        """
        Convert the file to JSON and load the data into the tables
        """
        returncode = self.convert2json(filename, id)
        
        if returncode != 0:
            """
            Update the slide table with the failure result.
            """
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'error'
                                  })
            return
                        
                            
        """
        Update the image table with the success result.
        """
        url = '/attribute/Vocab:workflow_status/Name={}/ID'.format(urlquote('Record Ready'))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        ID = resp.json()[0]['ID']
        
        obj = {}
        obj['RID'] = rid
        obj['Workflow_Status'] = ID
        obj['Process_Status'] = 'success'
        obj['mmCIF_File_MD5'] = md5
        columns = ['Workflow_Status', 'Process_Status', 'mmCIF_File_MD5']
        self.updateAttributes(schema,
                         table,
                         rid,
                         columns,
                         obj)
   
        self.logger.debug('Ended PDB Processing for the %s:%s table.' % (schema, table)) 
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, filename, file_url):
        try:
            hatracFile = '{}/{}'.format(self.make_mmCIF, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return hatracFile
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: get file from hatrac ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            return None
            
    """
    Convert the input file to JSON
    """
    def convert2json(self, filename, entry_id):
        try:
            """
            Apply make-mmcif.py
            """
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.make_mmCIF))
            args = [self.python_bin, 'make-mmcif.py', filename]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.make_mmCIF)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata)) 
                self.sendMail('FAILURE PDB', 'Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata))
                os.remove('{}/{}'.format(self.make_mmCIF, filename))
                return returncode
            
            os.remove('{}/{}'.format(self.make_mmCIF, filename))
            
            """
            Move the output.cif file to the rcsb/db/tests-validate/test-output/ihm-files directory and apply testSchemaDataPrepValidate-ihm.py
            """
            shutil.move('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))
            self.logger.debug('File {} was moved to the {} directory'.format('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))) 
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.py_rcsb_db))
            args = ['env', 'PYTHONPATH={}'.format(self.py_rcsb_db), self.python_bin, 'rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py']
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.py_rcsb_db)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not validate testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % ('output.cif', stdoutdata, stderrdata)) 
                self.sendMail('FAILURE PDB', 'Can not make testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % ('output.cif', stdoutdata, stderrdata))
                os.remove('{}/rcsb/db/tests-validate/test-output/ihm-files/output.cif'.format(self.py_rcsb_db))
                return returncode
            
            os.remove('{}/rcsb/db/tests-validate/test-output/ihm-files/output.cif'.format(self.py_rcsb_db))
            self.logger.debug('File {}/{} was removed'.format(self.py_rcsb_db, 'rcsb/db/tests-validate/test-output/ihm-files/output.cif')) 
            
            """
            Load now the data from JSON files which are in the rcsb/db/tests-validate/test-output directory into the tables 
            """
            fpath = '{}/rcsb/db/tests-validate/test-output'.format(self.py_rcsb_db)

            json_files = []
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        json_files.append(entry.name)
            self.logger.debug('The following JSON files were generated in the {}/rcsb/db/tests-validate/test-output directory:\n\t{}'.format(self.py_rcsb_db, '\n\t'.join(json_files))) 

            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        returncode = self.loadTablesFromJSON(entry.path, entry_id)
                        if returncode != 0:
                            break
            
            """
            Remove the JSON files that were created
            """
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        os.remove(entry.path)
                        self.logger.debug('Removed file {}'.format(entry.path))
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: convert to JSON ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            os.chdir(currentDirectory)
            self.logger.error('Can not convert to JSON for the file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata)) 
            self.sendMail('FAILURE PDB', 'Can not convert to JSON for the file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata))
            returncode = 1
            
        return returncode
            
        
    """
    Update the ermrest attributes
    """
    def updateAttributes (self, schema, table, rid, columns, row):
        """
        Update the ermrest attributes with the row values.
        """
        try:
            columns = ','.join([urlquote(col) for col in columns])
            url = '/attributegroup/%s:%s/RID;%s' % (urlquote(schema), urlquote(table), columns)
            resp = self.catalog.put(
                url,
                json=[row]
            )
            resp.raise_for_status()
            self.logger.debug('SUCCEEDED updated the table "%s" for the RID "%s"  with "%s".' % (url, rid, json.dumps(row, indent=4))) 
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: reportFailure ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            
            
    """
    Get the hexa md5 checksum of the file.
    """
    def md5hex(self, fpath):
        h = hashlib.md5()
        try:
            f = open(fpath, 'rb')
            try:
                b = f.read(4096)
                while b:
                    h.update(b)
                    b = f.read(4096)
                return h.hexdigest()
            finally:
                f.close()
        except:
            return None

    """
    Sort the tables to be loaded based on the FK dependencies.
    """
    def sortTable(self, fpath):
        mmCIF_tables = [
            "struct",
            "entity",
            "entity_poly",
            "entity_poly_seq",
            "pdbx_poly_seq_scheme",
            "chem_comp",
            "atom_type",
            "struct_asym",
            "ihm_entity_poly_segment",
            "ihm_struct_assembly",
            "ihm_struct_assembly_details",
            "ihm_model_representation",
            "ihm_model_representation_details",
            "ihm_modeling_protocol",
            "ihm_model_list",
            "ihm_model_group",
            "ihm_model_group_link"
        ]

        """
        Get the tables groups
        """
        with open(self.tables_groups, 'r') as f:
            table_groups = json.load(f)
        
        """
        Sort the tables from the JSON file based on the groups
        """
        tables = []
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]
            group_no = 0
            while group_no < len(table_groups):
                group_str = str(group_no)
                for k,v in pdb.items():
                    if k in table_groups[group_str] and k in mmCIF_tables:
                        tables.append(k)
                group_no +=1
        return tables
        
    """
    Load data into the tables from the JSON file.
    """
    def loadTablesFromJSON(self, fpath, entry_id):
        """
        with open(self.pickle_file, 'rb') as pickle_file:
            vocab_list = pickle.load(pickle_file)
            
        vocabulary_new_tables_name = {
                                      'ihm_geometric_object_distance_restraint_group_conditionality': 'geometric_object_distance_restraint_group_condition',
                                      'ihm_geometric_object_distance_restraint_object_characteristic': 'geometric_object_distance_restraint_object_character',
                                      'ihm_model_representation_details_model_object_primitive': 'model_representation_details_model_object_primitive',
                                      'ihm_starting_comparative_models_template_sequence_identity_denominator': 'starting_comparative_models_template_sequence_id_denom'
                                      }
        vocab_schema_name = 'Vocab'
        term_data_map = {}
        for vocab_dict in vocab_list:
            for k, v in vocab_dict.items():
                vocab_table_name = '{}_{}'.format(k[0], k[1])
                if vocab_table_name in vocabulary_new_tables_name.keys():
                    vocab_table_name = vocabulary_new_tables_name[vocab_table_name]
                vocab_table = pb.schemas[vocab_schema_name].tables[vocab_table_name]
                entities = vocab_table.path.entities()
                for entity in entities:
                    term_data_map[(k[0], k[1], entity['Name'])] = entity['ID']
        """
        
        """
        Get the mapping Name-->ID of the vocabulary tables
        """
        fr = open(self.vocab_data_map_file, mode='r')
        vocab_data_map = fr.read()
        fr.close()
        term_data_map = literal_eval(vocab_data_map)
                
        schema_name = 'PDB'
        pb = self.catalog.getPathBuilder()
        returncode = 0
        
        """
        Read the JSON file data
        """
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]

        """
        Sort the tables based on the FK dependencies
        """
        tables = self.sortTable(fpath)
        
        for tname in tables:
            records = pdb[tname]
            if type(records) is dict:
                records = [records]
    
            table = pb.schemas[schema_name].tables[tname]
            entities = []
            for r in records:
                """
                Replace the vocabulary Name values with their ID mappings
                """
                for k, v in r.items():
                    if (tname, k, v) in term_data_map.keys():
                        r[k] = term_data_map[(tname, k, v)]

                """
                Replace the FK references to the entry table
                """
                r = self.getUpdatedRecord(tname, r, entry_id)
                entities.append(r)
            
            """
            Insert the data
            """
            try:
                table.insert(entities)
                inserted_rows = len(entities)
                self.logger.debug('File {}: inserted {} rows into table {}'.format(fpath, inserted_rows, tname))
                #self.logger.debug('Inserted into table {} the {} rows:\n'.format(tname, entities))
            except HTTPError as e:
                self.logger.error(e)
                self.logger.error(e.response.text)
                self.sendMail('FAILURE PDB: loadTablesFromJSON:\n{}\n{}'.format(e.response.text, e))
                returncode = 1
                break
            except:
                et, ev, tb = sys.exc_info()
                self.logger.error('got exception "%s"' % str(ev))
                self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                self.sendMail('FAILURE PDB: loadTablesFromJSON ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                returncode = 1
                break
        
        return returncode

    """
    Load data into the tables from a csv/tsv file.
    """
    def loadTableFromCVS(self, fpath, delimiter, tname, entry_id):
        """
        Get the mapping Name-->ID of the vocabulary tables
        """
        fr = open(self.vocab_data_map_file, mode='r')
        vocab_data_map = fr.read()
        fr.close()
        term_data_map = literal_eval(vocab_data_map)
        
        """
        Read in chunks of 1000 rows
        Make a temporization of 10 seconds between chunks readings
        """
        returncode = 0
        chunk_size = 1000
        sleep_time = 10
        schema_name = 'PDB'
        pb = self.catalog.getPathBuilder()
        table = pb.schemas[schema_name].tables[tname]
        column_definitions = table.column_definitions
        counter = 0
        
        """
        Read the rows of the csv/tsv file as dictionaries
        """
        csvfile = open(fpath, 'r')
        reader = csv.DictReader(csvfile, delimiter=delimiter)
        j=0
        done = False
        missing_columns = []
        while not done:
            done = True
            i = 0
            entities = []
            for row in reader:
                j=j+1
                entity = dict(row)
                for column in list(entity.keys()):
                    try:
                        column_definitions[column]
                    except:
                        if column not in missing_columns:
                            missing_columns.append(column)
                            self.logger.debug('Table "%s" has not the column "%s".' % (tname, column))
                        entity[column] = ''
                        
                    if entity[column] == '':
                        """
                        Any empty value will be treated as NULL
                        """
                        del entity[column]
                    elif column_definitions[column].type['typename'] == 'jsonb':
                        entity[column] = json.loads(entity[column])
                    elif column_definitions[column].type['typename'].endswith('[]'):
                        entity[column] = entity[column][1:-1].split(',')
                
                """
                Replace the vocabulary Name values with their ID mappings
                """
                for k, v in entity.items():
                    if (tname, k, v) in term_data_map.keys():
                        entity[k] = term_data_map[(tname, k, v)]
                        
                """
                Replace the FK references to the entry table
                """
                entity = self.getRecordUpdatedWithFK(tname, entity, entry_id)
                
                entities.append(entity)
                i = i+1
                if i >= chunk_size:
                    """
                    Insert the chunk
                    """
                    done = False
                    break
            if len(entities) > 0:
                try:
                    table.insert(entities).fetch()
                    counter = counter + len(entities)
                    time.sleep(sleep_time)
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    self.sendMail('FAILURE PDB: loadTableFromCVS ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                    returncode = 1
                    break
        self.logger.debug('File {}: inserted {} rows into table {}'.format(fpath, counter, tname))
        return returncode
             
    """
    Get the record with the foreign key updated to the entry id.
    If the FK is missing, add it.
    """
    def getRecordUpdatedWithFK(self, tname, row, entry_id):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog 1']['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            row[col] = entry_id
                
        return row

    """
    Get the record with the foreign key updated to the entry id.
    """
    def getUpdatedRecord(self, tname, row, entry_id):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog 1']['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            if col in row.keys():
                row[col] = entry_id
        return row


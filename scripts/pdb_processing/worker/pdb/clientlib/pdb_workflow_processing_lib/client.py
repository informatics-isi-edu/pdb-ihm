#!/usr/bin/python3
# 
# Copyright 2020 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Client for PDB workflow processing.
"""

import os
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
from email.mime.text import MIMEText
import socket
from dateutil.parser import parse
from socket import gaierror, EAI_AGAIN
from requests import HTTPError
import pickle
import csv
import filecmp
import mimetypes

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote
from deriva.core.utils import hash_utils as hu

mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'
catalog_dev_number = 99

class PDBClient (object):
    """Network client for PDB workflow processing.
    """
    ## Derived from the ermrest iobox service client

    def __init__(self, **kwargs):
        self.baseuri = kwargs.get("baseuri")
        o = urlparse(self.baseuri)
        self.scheme = o[0]
        host_port = o[1].split(":")
        self.host = host_port[0]
        self.path = o.path
        self.port = None
        if len(host_port) > 1:
            self.port = host_port[1]
        self.catalog_number = int(self.path.split('/')[-1])
        self.is_catalog_dev = (int(self.path.split('/')[-1]) == catalog_dev_number)
        self.make_mmCIF = kwargs.get("make_mmCIF")
        self.mmCIF_Schema_Version = kwargs.get("mmCIF_Schema_Version")
        self.py_rcsb_db = kwargs.get("py_rcsb_db")
        self.python_bin = kwargs.get("python_bin")
        self.pickle_file = kwargs.get("pickle_file")
        self.tables_groups = kwargs.get("tables_groups")
        self.export_tables = kwargs.get("export_tables")
        self.optional_fk_file = kwargs.get("optional_fk_file")
        self.scratch = kwargs.get("scratch")
        self.cif_tables = kwargs.get("cif_tables")
        self.export_order_by = kwargs.get("export_order_by")
        self.entry = kwargs.get("entry")
        self.credentials = kwargs.get("credentials")
        self.store = HatracStore(
            self.scheme, 
            self.host,
            self.credentials
        )
        self.catalog = PollingErmrestCatalog(
            self.scheme, 
            self.host,
            self.path.split('/')[-1],
            self.credentials
        )
        self.catalog.dcctx['cid'] = 'pipeline/pdb'
        self.mail_server = kwargs.get("mail_server")
        self.mail_sender = kwargs.get("mail_sender")
        self.mail_receiver = kwargs.get("mail_receiver")
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')

    """
    Send email notification
    """
    def sendMail(self, subject, text):
        if self.mail_server and self.mail_sender and self.mail_receiver:
            retry = 0
            ready = False
            while not ready:
                try:
                    msg = MIMEText('%s\n\n%s' % (text, mail_footer), 'plain')
                    msg['Subject'] = subject
                    msg['From'] = self.mail_sender
                    msg['To'] = self.mail_receiver
                    s = smtplib.SMTP(self.mail_server)
                    s.sendmail(self.mail_sender, self.mail_receiver.split(','), msg.as_string())
                    s.quit()
                    self.logger.debug('Sent email notification.')
                    ready = True
                except socket.gaierror as e:
                    if e.errno == socket.EAI_AGAIN:
                        time.sleep(100)
                        retry = retry + 1
                        ready = retry > 10
                    else:
                        ready = True
                    if ready:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    ready = True

    """
    Start the process for generating pyramidal tiles
    """
    def start(self):
        try:
            rid = os.getenv('RID', None)
            if rid == None:
                self.logger.error('RID was not specified in the environment')
                return
            
            action = os.getenv('action', None)
            if action == None:
                self.logger.error('"action" was not specified in the environment')
                return
                
            if action == 'entry':
                self.process_mmCIF('PDB','entry', rid)
            elif action == 'Entry_Related_File':
                self.process_Entry_Related_File('PDB', 'Entry_Related_File', rid)
            elif action == 'export':
                self.export_mmCIF('PDB', 'entry', rid)
            else:
                self.logger.error('Unknown action: "%s".' % action)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: unexpected exception', '%s\nThe process might have been stopped\n' % ''.join(traceback.format_exception(et, ev, tb)))
            raise
        
    """
    Process the csv/tsv file of the Entry_Related_File table
    """
    def process_Entry_Related_File(self, schema, table, rid, status='in progress'):
        """
        Query for detecting the record to be processed
        """
        url = '/entity/%s:%s/RID=%s/Process_Status=%s' % (urlquote(schema), urlquote(table), urlquote(rid), urlquote(status))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row = resp.json()[0]
        filename = row['File_Name']
        file_url = row['File_URL']
        md5 = row['File_MD5']
        structure_id = row['structure_id']
        creation_time = row['RCT']
        
        """
        Extract the file from hatrac
        """
        f,error_message = self.getHatracFile(filename, file_url)
        
        if f == None:
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  })
            return
        
        if md5 == None:
            md5 = self.md5hex(f)
            self.logger.debug("The MD5 was computed and it is: %s" % md5)
                                        
        """
        Load data from the csv/tsv files
        """
        url = '/attribute/%s:%s/RID=%s/Vocab:File_Type/Table_Name' % (urlquote(schema), urlquote(table), urlquote(rid))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        tname = resp.json()[0]['Table_Name']

        url = '/attribute/%s:%s/RID=%s/Vocab:File_Format/Name' % (urlquote(schema), urlquote(table), urlquote(rid))
        resp = self.catalog.get(url)
        resp.raise_for_status()
        file_type = resp.json()[0]['Name']
        delimiter = '\t' if file_type=='TSV' else ','
        
        # see where the csv/tsv file is
        # suppose it is fpath
        returncode,error_message = self.loadTableFromCVS(f, delimiter, tname, structure_id, rid)

        if returncode != 0:
            """
            Update the slide table with the failure result.
            """
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  })
            return
                        
        obj = {}
        obj['RID'] = rid
        obj['Workflow_Status'] = 'RECORD READY'
        obj['Process_Status'] = 'success'
        obj['File_MD5'] = md5
        obj['Record_Status_Detail'] = None
        columns = ['Workflow_Status', 'Process_Status', 'File_MD5', 'Record_Status_Detail']
        self.updateAttributes(schema,
                         table,
                         rid,
                         columns,
                         obj)
   
        self.logger.debug('Ended PDB Processing for the %s:%s table.' % (schema, table)) 
        
    """
    Export the mmCIF file of the entry table
    """
    def export_mmCIF(self, schema_pdb, table_entry, rid, status='in progress'):
        deriva_tables = ['entry']
        mmCIF_tables = []
        mmCIF_ignored = []
        error_message = None

        """
        Query for detecting the record to be exported
        """
        pb = self.catalog.getPathBuilder()
        schema = pb.PDB
        entry = schema.entry
        RID = entry.RID
        Process_Status = entry.Process_Status
        path = entry.path
        path.filter((RID == rid) & (Process_Status == status))
        self.logger.debug('Query Export URL: {}'.format(path.uri))

        results = path.entities()
        results.fetch()
        file_url = results[0]['mmCIF_File_URL']
        filename = results[0]['mmCIF_File_Name']
        entry_id = results[0]['id']
        creation_time = results[0]['RCT']
        year = parse(creation_time).strftime("%Y")
        
        #hatracFile = '{}/_{}.cif'.format(self.scratch, entry_id)
        hatracFile = self.getOutputCIF(file_url, filename)
        tables = self.export_tables
        output = '{}/{}.cif'.format(self.scratch, entry_id)
        fw = open(output, 'w')
        
        mmCIF_export = self.cif_tables
        pks_map = self.export_order_by
        matrix_tables = ['ihm_2dem_class_average_fitting', 'ihm_geometric_object_transformation']

        def writeLine(line, loopLine=None):
            table_name = line[1:].split('.')[0]
            if table_name not in mmCIF_tables:
                mmCIF_tables.append(table_name)
            if table_name not in deriva_tables and table_name not in mmCIF_export and table_name not in mmCIF_ignored:
                mmCIF_ignored.append(table_name)
            if table_name not in deriva_tables and table_name in mmCIF_export:
                if loopLine != None:
                    fw.write('{}\n'.format(loopLine))
                if table_name in matrix_tables:
                    try:
                        columns = line.split()
                        new_columns = []
                        for column in columns:
                            try:
                                r = re.search('(.*)[_]matrix[_]([0-9]+)[_]([0-9]+)$', column)
                                new_columns.append('{}_matrix[{}][{}]'.format(r.group(1),r.group(2),r.group(3)))
                            except:
                                new_columns.append(column)
                        fw.write('{}\n'.format(' '.join(new_columns)))
                    except:
                        fw.write('{}'.format(line))
                else:
                    fw.write('{}'.format(line))
                return True
            else:
                return False
        
        def getColumnValue(table_name, column_name, column_type, column_value):
            if column_value == None:
                return '.'
            if column_type in ['int4', 'float4']:
                return '{}'.format(column_value)
            if column_type == 'text':
                if '\t' in column_value:
                    self.logger.debug('tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    error_message = 'tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                    return None
                if '\n' in column_value:
                    return '\n;{}\n;\n'.format(column_value)
                if '"' not in column_value and "'" not in column_value and ' ' not in column_value and not column_value.startswith('_'):
                    return column_value
                if '"' not in column_value:
                    return '"{}"'.format(column_value)
                if "'" not in column_value:
                    return "'{}'".format(column_value)
                else:
                    self.logger.debug('Both " and \' are in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    return '\n;{}\n;\n'.format(column_value)
                error_message = 'Unhandled value in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                return None
            else:
                self.logger.debug('unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name))
                error_message = 'unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name)
                return None

        def exportData():
            try:
                for table_name, table_body in tables.items():
                    pk = table_body['pkey_columns']
                    try:
                        pk.remove('structure_id')
                    except:
                        pass
                    
                    """
                    if len(pk) == 0:
                        self.logger.debug('No PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) == 2:
                        self.logger.debug('2 PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) == 3:
                        self.logger.debug('3 PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) != 1:
                        self.logger.debug('More than 3 PK Table {}, PK: {}'.format(table_name,pk))
                    """
                            
                    if table_name in ['entry', 'chem_comp_atom']:
                        continue
                    table = schema.tables[table_name]
                    path = table.path
                    if table_name == 'struct':
                        entry_id_column = table.column_definitions['entry_id']
                        path.filter(entry_id_column == entry_id)
                    else:
                        structure_id = table.column_definitions['structure_id']
                        path.filter(structure_id == entry_id)
                    #self.logger.debug('Query Export Data URL: {}'.format(path.uri))
                    results = path.entities()
                    if len(pk) == 1:
                        #self.logger.debug('Sorting results based on: {}'.format(table.column_definitions[pk[0]]))
                        results.sort(table.column_definitions[pk[0]])
                    else:
                        pk = pks_map[table_name]
                        #self.logger.debug('Sorting results based on: ({}, {})'.format(table.column_definitions[pk[0]], table.column_definitions[pk[1]]))
                        results.sort(table.column_definitions[pk[0]], table.column_definitions[pk[1]])
                    results.fetch()
                    if len(results) == 0:
                        continue
                    deriva_tables.append(table_name)
                    fw.write('loop_\n')
                    for column in table_body['columns']:
                        if column['name'] == 'structure_id':
                            continue
                        fw.write('_{}.{}\n'.format(table_name,column['name']))
                    for row in results:
                        line = []
                        for column in table_body['columns']:
                            column_name = column['name']
                            column_type = column['type']
                            if column_name == 'structure_id':
                                continue
                            column_value = row[column_name]
                            value = getColumnValue(table_name, column_name, column_type, column_value)
                            if value == None:
                                self.logger.debug('Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                self.sendMail('FAILURE PDB: Could not find column value', 'Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                return 1
                            line.append(value)
                        fw.write('{}\n'.format('\t'.join(line)))
                    fw.write('#\n')    
                
                return 0
            except:
                et, ev, tb = sys.exc_info()
                self.logger.debug('exportData got exception "%s"' % str(ev))
                self.logger.debug('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                self.sendMail('FAILURE PDB: exportData got exception', '%s\nThe process might have been stopped\n' % ''.join(traceback.format_exception(et, ev, tb)))
                return 1

        def exportCIF():
            #self.store.get_obj(mmCIF_File_URL, destfilename=hatracFile)
            
            fr = open(hatracFile, 'r')
            lines = fr.readlines()
            status = 'skip'

            for line in lines:
                if status == 'skip':
                    if line.strip() == 'loop_':
                        status = 'loop'
                    elif line.startswith('_'):
                        if writeLine(line):
                            status = 'columns'
                        
                elif status == 'loop':
                    if line.startswith('_'):
                        if writeLine(line, loopLine='loop_'):
                            status = 'columns'
                        else:
                            status = 'skip'
                    else:
                        self.logger.debug('Unexpected line after loop_:\n{}'.format(line))
                        fr.close()
                        self.sendMail('FAILURE PDB: Unknown status', 'status = {}\nThe process might have been stopped\n'.format(status))
                        return 1
            
                elif status == 'columns':
                    if line.startswith('_'):
                        if not writeLine(line):
                            status = 'skip'
                    elif line.strip() == 'loop_':
                        status = 'loop'
                    else:
                        fw.write('{}'.format(line))
                        status = 'rows'
            
                elif status == 'rows':
                    if line.strip() == 'loop_':
                        status = 'loop'
                    elif line.startswith('_'):
                        if writeLine(line):
                            status = 'columns'
                        else:
                            status = 'skip'
                    else:
                        fw.write('{}'.format(line))
                else:
                    self.logger.debug('Unknown status: {}'.format(status))
                    self.sendMail('FAILURE PDB: Unknown status', 'status = {}\nThe process might have been stopped\n'.format(status))
                    return 1
        
            fr.close()
            return 0
            
        fw.write('data_{}\n\n'.format(entry_id))
        value = getColumnValue('entry', 'id', 'text', '{}'.format(entry_id))
        fw.write('#\n_entry.id  {}\n#\n'.format(value))

        if exportData() != 0:
            self.logger.debug('Update error in exportData()')
            fw.close()
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status", "Generated_mmCIF_Processing_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': 'Update error in exportData():\n{}'.format(error_message),
                                  'Workflow_Status': 'ERROR',
                                  'Generated_mmCIF_Processing_Status': 'ERROR'
                                  })
            return
        if exportCIF() != 0:
            self.logger.debug('Update error in exportCIF()')
            fw.close()
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status", "Generated_mmCIF_Processing_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': 'Update error in exportCIF():\n{}'.format(error_message),
                                  'Workflow_Status': 'ERROR',
                                  'Generated_mmCIF_Processing_Status': 'ERROR'
                                  })
            return
        fw.close()
        os.remove(hatracFile)
        
        if len(mmCIF_ignored) > 0:
            self.logger.debug('Tables from the mmCIF file that were not included in the export:')
            for table_name in sorted(mmCIF_ignored):
                self.logger.debug('\t{}'.format(table_name))
                
        file_name = '{}.cif'.format(entry_id)
        newFile = output
        file_size = os.path.getsize(newFile)
        hashes = hu.compute_file_hashes(newFile, hashes=['md5', 'sha256'])
        new_md5 = hashes['md5'][1]
        new_sha256 = hashes['sha256'][1]
        hexa_md5 = hashes['md5'][0]
        new_uri = '/hatrac/pdb/entry_mmCIF/%s/%s' % (year, urlquote(hexa_md5))
        
        """
        Store the mmCIF in hatrac if they are not already
        """
        hatrac_URI = None
        try:
            outfile = hatracFile
            self.hatrac_store.get_obj(new_uri, destfilename=outfile)
            hatrac_URI = new_uri
        except:
            pass
            
        if hatrac_URI != None and filecmp.cmp(outfile, newFile) == True:
            self.logger.info('Skipping the upload of the file "%s" as it already exists hatrac.' % file_name)
            self.logger.debug('Removing file "%s"' % (outfile))
            os.remove(outfile)
        
        else:
            if mimetypes.inited == False:
                mimetypes.init()
            content_type,encoding = mimetypes.guess_type(newFile)
            if content_type == None:
                content_type = 'application/octet-stream'
            try:
                hatrac_URI = self.store.put_loc(new_uri,
                                                     newFile,
                                                     headers={'Content-Type': content_type},
                                                     content_disposition = "filename*=UTF-8''%s" % urlquote(file_name),
                                                     md5 = new_md5,
                                                     sha256 = new_sha256,
                                                     content_type = content_type,
                                                     chunked = True
                                                   )
            except:
                et, ev, tb = sys.exc_info()
                self.logger.error('Can not upload file "%s" in hatrac "%s". Error: "%s"' % (file_name, new_uri, str(ev)))
                self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                self.sendMail('FAILURE IMAGE PROCESSING: HATRAC PUT ERROR', 'RID: %s\nCan not upload file "%s" in hatrac "%s". Error: "%s"' % (rid, file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
                return 1
                
        self.logger.debug('Removing file "%s"' % (newFile))
        os.remove(newFile)
        
        """
        Insert or update the Entry_mmCIF_File table
        """
        url = '/entity/PDB:Entry_mmCIF_File/Structure_Id={}/mmCIF_Schema_Version={}'.format(urlquote(entry_id), urlquote(self.mmCIF_Schema_Version))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        if len(resp.json()) > 0:
            """
            Update Entry_mmCIF_File table
            """
            row = resp.json()[0]
            mmCIF_RID = row['RID']
            self.logger.debug('Update the Entry_mmCIF_File table')
            self.updateAttributes('PDB',
                                  'Entry_mmCIF_File',
                                  rid,
                                  ["File_URL", "File_Name", "File_MD5", "File_Bytes"],
                                  {'RID': mmCIF_RID,
                                  'File_URL': hatrac_URI,
                                  'File_Name': file_name,
                                  'File_MD5': hexa_md5,
                                  'File_Bytes': file_size
                                  })
        else:
            """
            Insert a row
            """
            self.logger.debug('Insert a row in the Entry_mmCIF_File table')
            row = {'File_URL' : hatrac_URI,
                   'File_Name': file_name,
                   'File_Bytes': file_size,
                   'File_MD5': hexa_md5,
                   'Structure_Id': entry_id,
                   'mmCIF_Schema_Version': self.mmCIF_Schema_Version
                   }
            if self.createEntity('PDB:Entry_mmCIF_File', row, rid) == None:
                self.updateAttributes(schema_pdb,
                                      table_entry,
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status", "Generated_mmCIF_Processing_Status"],
                                      {'RID': rid,
                                      'Process_Status': 'ERROR',
                                      'Record_Status_Detail': 'Update error in createEntity():\n{}'.format(error_message),
                                      'Workflow_Status': 'ERROR',
                                      'Generated_mmCIF_Processing_Status': 'ERROR'
                                      })
                return
                    
        self.logger.debug('Update success in export_mmCIF()')
        self.updateAttributes(schema_pdb,
                              table_entry,
                              rid,
                              ["Process_Status", "Workflow_Status", "Generated_mmCIF_Processing_Status"],
                              {'RID': rid,
                              'Process_Status': 'success',
                              'Workflow_Status': 'mmCIF CREATED',
                              'Generated_mmCIF_Processing_Status': 'success'
                              })
        return
            
    """
    Insert a row in a table
    """
    def createEntity (self, path, row, rid):
        """
        Insert the row in the table.
        """
        try:
            url = '/entity/%s' % (path)
            resp = self.catalog.post(
                url,
                json=[row]
            )
            resp.raise_for_status()
            
            self.logger.debug('SUCCEEDED created in the table "%s" the entry "%s".' % (url, json.dumps(row, indent=4))) 
            return url
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE IMAGE PROCESSING: CREATE ENTITY ERROR', 'RID: %s\n%s\n' % (rid, ''.join(traceback.format_exception(et, ev, tb))))
            return None

    """
    Process the mmCIF file of the entry table
    """
    def process_mmCIF(self, schema, table, rid, status='in progress'):
        """
        Query for detecting the record to be processed
        """
        url = '/entity/%s:%s/RID=%s/Process_Status=%s' % (urlquote(schema), urlquote(table), urlquote(rid), urlquote(status))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row = resp.json()[0]
        filename = row['mmCIF_File_Name']
        file_url = row['mmCIF_File_URL']
        md5 = row['mmCIF_File_MD5']
        last_md5 = row['Last_mmCIF_File_MD5']
        id = row['id']
        creation_time = row['RCT']
        
        """
        Check if we have a new mmCIF file
        """
        if md5 == last_md5:
            self.logger.debug('RID="{}", Skipping loading the table as the mmCIF file is unchanged'.format(rid))
            obj = {}
            obj['RID'] = rid
            obj['Workflow_Status'] = 'RECORD READY'
            obj['Process_Status'] = 'success'
            columns = ['Workflow_Status', 'Process_Status']
            self.updateAttributes(schema,
                             table,
                             rid,
                             columns,
                             obj)
       
            self.logger.debug('RID="{}", Ended PDB Processing for the {}:{} table.'.format(rid, schema, table)) 
            return
        
        """
        Extract the file from hatrac
        """
        f,error_message = self.getHatracFile(filename, file_url)
        
        if f == None:
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  })
            return
        
        """
        Get the md5 if necessary
        """
        if md5 == None:
            md5 = self.md5hex(f)
            self.logger.debug("The MD5 was computed and it is: %s" % md5)
            
        """
        Convert the file to JSON and load the data into the tables
        """
        returncode,error_message = self.convert2json(filename, id)
        
        if returncode != 0:
            """
            Update the slide table with the failure result.
            """
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': 'ERROR',
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  })
            return
                        
                            
        obj = {}
        obj['RID'] = rid
        obj['Workflow_Status'] = 'RECORD READY'
        obj['Process_Status'] = 'success'
        obj['mmCIF_File_MD5'] = md5
        obj['Record_Status_Detail'] = None
        columns = ['Workflow_Status', 'Process_Status', 'mmCIF_File_MD5', 'Record_Status_Detail', 'Last_mmCIF_File_MD5']
        obj['Last_mmCIF_File_MD5'] = md5
        self.updateAttributes(schema,
                         table,
                         rid,
                         columns,
                         obj)
   
        self.logger.debug('Ended PDB Processing for the %s:%s table.' % (schema, table)) 
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, filename, file_url):
        error_message = None
        try:
            hatracFile = '{}/{}'.format(self.make_mmCIF, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return (hatracFile,error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: get file from hatrac ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = ''.join(traceback.format_exception(et, ev, tb))
            return (None,error_message)
            
    """
    Convert the input file to JSON
    """
    def convert2json(self, filename, entry_id):
        try:
            """
            Apply make-mmcif.py
            """
            error_message = None
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.make_mmCIF))
            args = [self.python_bin, 'make-mmcif.py', filename]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.make_mmCIF)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata)) 
                self.sendMail('FAILURE PDB', 'Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata))
                os.remove('{}/{}'.format(self.make_mmCIF, filename))
                error_message = '{}'.format(stderrdata)
                return (returncode,error_message)
            
            os.remove('{}/{}'.format(self.make_mmCIF, filename))
            
            """
            Move the output.cif file to the rcsb/db/tests-validate/test-output/ihm-files directory and apply testSchemaDataPrepValidate-ihm.py
            """
            shutil.move('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))
            self.logger.debug('File {} was moved to the {} directory'.format('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))) 
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.py_rcsb_db))
            args = ['env', 'PYTHONPATH={}'.format(self.py_rcsb_db), self.python_bin, 'rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py']
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.py_rcsb_db)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not validate testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % ('output.cif', stdoutdata, stderrdata)) 
                self.sendMail('FAILURE PDB', 'Can not make testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % ('output.cif', stdoutdata, stderrdata))
                os.remove('{}/rcsb/db/tests-validate/test-output/ihm-files/output.cif'.format(self.py_rcsb_db))
                error_message = '{}'.format(stderrdata)
                return (returncode,error_message)
            
            os.remove('{}/rcsb/db/tests-validate/test-output/ihm-files/output.cif'.format(self.py_rcsb_db))
            self.logger.debug('File {}/{} was removed'.format(self.py_rcsb_db, 'rcsb/db/tests-validate/test-output/ihm-files/output.cif')) 
            
            """
            Load now the data from JSON files which are in the rcsb/db/tests-validate/test-output directory into the tables 
            """
            fpath = '{}/rcsb/db/tests-validate/test-output'.format(self.py_rcsb_db)

            json_files = []
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        json_files.append(entry.name)
            self.logger.debug('The following JSON files were generated in the {}/rcsb/db/tests-validate/test-output directory:\n\t{}'.format(self.py_rcsb_db, '\n\t'.join(json_files))) 

            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        returncode,error_message = self.loadTablesFromJSON(entry.path, entry_id)
                        if returncode != 0:
                            break
            
            """
            Remove the JSON files that were created
            """
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        os.remove(entry.path)
                        self.logger.debug('Removed file {}'.format(entry.path))
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: convert to JSON ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            os.chdir(currentDirectory)
            returncode = 1
            error_message = '{}'.format(''.join(traceback.format_exception(et, ev, tb)))
            
        return (returncode,error_message)
            
        
    """
    Update the ermrest attributes
    """
    def updateAttributes (self, schema, table, rid, columns, row):
        """
        Update the ermrest attributes with the row values.
        """
        try:
            columns = ','.join([urlquote(col) for col in columns])
            url = '/attributegroup/%s:%s/RID;%s' % (urlquote(schema), urlquote(table), columns)
            resp = self.catalog.put(
                url,
                json=[row]
            )
            resp.raise_for_status()
            self.logger.debug('SUCCEEDED updated the table "%s" for the RID "%s"  with "%s".' % (url, rid, json.dumps(row, indent=4))) 
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: reportFailure ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            
            
    """
    Get the hexa md5 checksum of the file.
    """
    def md5hex(self, fpath):
        h = hashlib.md5()
        try:
            f = open(fpath, 'rb')
            try:
                b = f.read(4096)
                while b:
                    h.update(b)
                    b = f.read(4096)
                return h.hexdigest()
            finally:
                f.close()
        except:
            return None

    """
    Sort the tables to be loaded based on the FK dependencies.
    """
    def sortTable(self, fpath):
        mmCIF_tables = [
            "struct",
            "entity",
            "entity_poly",
            "entity_poly_seq",
            "pdbx_poly_seq_scheme",
            "chem_comp",
            "atom_type",
            "struct_asym",
            "ihm_entity_poly_segment",
            "ihm_struct_assembly",
            "ihm_struct_assembly_details",
            "ihm_model_representation",
            "ihm_model_representation_details",
            "ihm_modeling_protocol",
            "ihm_model_list",
            "ihm_model_group",
            "ihm_model_group_link"
        ]

        """
        Get the tables groups
        """
        with open(self.tables_groups, 'r') as f:
            table_groups = json.load(f)
        
        """
        Sort the tables from the JSON file based on the groups
        """
        tables = []
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]
            group_no = 0
            while group_no < len(table_groups):
                group_str = str(group_no)
                for k,v in pdb.items():
                    if k in table_groups[group_str] and k in mmCIF_tables:
                        tables.append(k)
                group_no +=1
        return tables
        
    """
    Rollback JSON inserted rows
    """
    def rollbackInsertedRows(self, records):
        records.reverse()
        for record in records:
            tname = record['name']
            rows = record['rows']
            for row in rows:
                rid = row['RID']
                try:
                    path = '%s:%s/%s=%s' % (urlquote('PDB'), urlquote(tname), urlquote('RID'), urlquote(rid))
                    url = '/entity/%s' % (path)
                    resp = self.catalog.delete(
                        url
                    )
                    resp.raise_for_status()
                    self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
                except HTTPError as e:
                    if e.response.status_code == HTTPStatus.NOT_FOUND:
                        self.logger.debug('No rows found to delete from the URL "%s".' % (url))
                    else:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                        self.sendMail('FAILURE PDB: DELETE ERROR', 'URL: %s\n%s\n' % (url, ''.join(traceback.format_exception(et, ev, tb))))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    self.sendMail('FAILURE PDB: DELETE ERROR', 'URL: %s\n%s\n' % (url, ''.join(traceback.format_exception(et, ev, tb))))
                
    """
    Load data into the tables from the JSON file.
    """
    def loadTablesFromJSON(self, fpath, entry_id):
        schema_name = 'PDB'
        pb = self.catalog.getPathBuilder()
        returncode = 0
        error_message = None
        
        """
        Read the JSON file data
        """
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]

        """
        Read the JSON FK optional file
        """
        with open(self.optional_fk_file, 'r') as f:
            optional_fks = json.load(f)

        """
        Sort the tables based on the FK dependencies
        """
        tables = self.sortTable(fpath)
        
        """
        Keep track of the inserted rows in case of rollback
        """
        inserted_records = []
        
        for tname in tables:
            records = pdb[tname]
            if type(records) is dict:
                records = [records]
    
            table = pb.schemas[schema_name].tables[tname]
            entities = []
            for r in records:
                """
                Replace the FK references to the entry table
                """
                r = self.getUpdatedRecord(tname, r, entry_id)
                if self.is_catalog_dev == True:
                    if tname == 'ihm_entity_poly_segment':
                        r = self.getUpdatedEntityPolySegment(r)
                self.getUpdatedOptional(optional_fks, tname, r, entry_id)
                entities.append(r)
            
            """
            Insert the data
            """
            try:
                res = table.insert(entities).fetch()
                inserted_records.append({'name': tname, 'rows': res})
                inserted_rows = len(entities)
                self.logger.debug('File {}: inserted {} rows into table {}'.format(fpath, inserted_rows, tname))
                #self.logger.debug('Inserted into table {} the {} rows:\n'.format(tname, entities))
            except HTTPError as e:
                self.logger.error(e)
                self.logger.error(e.response.text)
                self.sendMail('FAILURE PDB: loadTablesFromJSON:\n{}\n{}'.format(e.response.text, e))
                returncode = 1
                error_message = '{}\n{}'.format(e.response.text, e)
                self.rollbackInsertedRows(inserted_records)
                break
            except:
                et, ev, tb = sys.exc_info()
                self.logger.error('got exception "%s"' % str(ev))
                self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                self.sendMail('FAILURE PDB: loadTablesFromJSON ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                returncode = 1
                error_message = ''.join(traceback.format_exception(et, ev, tb))
                self.rollbackInsertedRows(inserted_records)
                break
        
        return (returncode,error_message)

    """
    Load data into the tables from a csv/tsv file.
    """
    def loadTableFromCVS(self, fpath, delimiter, tname, entry_id, rid):
        
        """
        Read in chunks of 1000 rows
        Make a temporization of 10 seconds between chunks readings
        """
        returncode = 0
        error_message = None
        chunk_size = 1000
        sleep_time = 10
        schema_name = 'PDB'
        pb = self.catalog.getPathBuilder()
        table = pb.schemas[schema_name].tables[tname]
        column_definitions = table.column_definitions
        counter = 0
        
        """
        Read the JSON FK optional file
        """
        with open(self.optional_fk_file, 'r') as f:
            optional_fks = json.load(f)

        """
        Read the rows of the csv/tsv file as dictionaries
        """
        csvfile = open(fpath, 'r')
        reader = csv.DictReader(csvfile, delimiter=delimiter)
        j=0
        done = False
        missing_columns = []
        while not done:
            done = True
            i = 0
            entities = []
            for row in reader:
                j=j+1
                entity = dict(row)
                for column in list(entity.keys()):
                    try:
                        column_definitions[column]
                    except:
                        if column not in missing_columns:
                            missing_columns.append(column)
                            self.logger.debug('Table "%s" has not the column "%s".' % (tname, column))
                        entity[column] = ''
                        
                    """
                    Columns types:
                        ermrest_rid
                        ermrest_rct
                        ermrest_rmt
                        ermrest_rcb
                        ermrest_rmb
                        ermrest_curie
                        ermrest_uri
                        text
                        markdown
                        text[]
                        int4
                        float4
                        int8
                    """
                    if entity[column] == '':
                        """
                        Any empty value will be treated as NULL
                        """
                        del entity[column]
                    elif column_definitions[column].type.typename == 'jsonb':
                        entity[column] = json.loads(entity[column])
                    elif column_definitions[column].type.typename.endswith('[]'):
                        entity[column] = entity[column][1:-1].split(',')
                
                """
                Replace the FK references to the entry table
                """
                entity = self.getRecordUpdatedWithFK(tname, entity, entry_id)
                entity = self.getUpdatedOptional(optional_fks, tname, entity, entry_id)
                entity['Entry_Related_File'] = rid
                
                entities.append(entity)
                i = i+1
                if i >= chunk_size:
                    """
                    Insert the chunk
                    """
                    done = False
                    break
            if len(entities) > 0:
                try:
                    table.insert(entities).fetch()
                    counter = counter + len(entities)
                    time.sleep(sleep_time)
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    self.sendMail('FAILURE PDB: loadTableFromCVS ERROR', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                    error_message = ''.join(traceback.format_exception(et, ev, tb))
                    returncode = 1
                    break
        self.logger.debug('File {}: inserted {} rows into table {}'.format(fpath, counter, tname))
        return (returncode, error_message)
             
    """
    Get the record with the foreign key updated to the entry id.
    If the FK is missing, add it.
    """
    def getRecordUpdatedWithFK(self, tname, row, entry_id):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog {}'.format(self.catalog_number)]['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            if tname == 'struct' and col == 'structure_id':
                continue
            row[col] = entry_id
                
        return row

    """
    Get the record with the foreign key updated to the entry id.
    """
    def getUpdatedRecord(self, tname, row, entry_id):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog {}'.format(self.catalog_number)]['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            if tname == 'struct' and col == 'structure_id':
                continue
            if col in row.keys():
                row[col] = entry_id
        return row

    """
    Get the record for the ihm_entity_poly_segment table updated with values for the Entity_Poly_Seq_RID_Begin and Entity_Poly_Seq_RID_End columns.
    """
    def getUpdatedEntityPolySegment(self, row):
        structure_id = row['structure_id']
        entity_id = row['entity_id']
        comp_id_begin = row ['comp_id_begin']
        comp_id_end = row ['comp_id_end']
        seq_id_begin = row['seq_id_begin']
        seq_id_end = row['seq_id_end']
        
        url = '/attribute/PDB:entity_poly_seq/structure_id={}&entity_id={}&mon_id={}&num={}/RID'.format(urlquote(structure_id), urlquote(entity_id), urlquote(comp_id_begin), seq_id_begin)
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row['Entity_Poly_Seq_RID_Begin'] = resp.json()[0]['RID']

        url = '/attribute/PDB:entity_poly_seq/structure_id={}&entity_id={}&mon_id={}&num={}/RID'.format(urlquote(structure_id), urlquote(entity_id), urlquote(comp_id_end), seq_id_end)
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row['Entity_Poly_Seq_RID_End'] = resp.json()[0]['RID']

        return row

    """
    Update the record for the optional composite FK
    """
    def getUpdatedOptional(self, optional_fks, tname, row, entry_id):
        if tname in optional_fks:
            for fk in optional_fks[tname]:
                url_structure_pattern = fk['url_structure_pattern']
                url_pattern = fk['url_pattern']
                fk_RID_column_name = fk['fk_RID_column_name']
                fk_other_column_name = fk['fk_other_column_name']
                ref_table = fk['ref_table']
                ref_other_column_name = fk['ref_other_column_name']
                if ref_other_column_name not in row.keys():
                    continue
                if fk_other_column_name not in row.keys():
                    continue
                fk_other_value = row[fk_other_column_name]
                if type(fk_other_value).__name__ == 'str':
                    fk_other_value = urlquote(fk_other_value)
                url = url_structure_pattern.format(urlquote(ref_table), urlquote(ref_other_column_name), fk_other_value, entry_id)
                self.logger.debug('Query URL with structure_id for OPTIONAL FK: "%s"' % url) 
                resp = self.catalog.get(url)
                resp.raise_for_status()
                if len(resp.json()) > 0:
                    row[fk_RID_column_name] = resp.json()[0]['RID']
                    continue
                url = url_pattern.format(urlquote(ref_table), urlquote(ref_other_column_name), fk_other_value)
                self.logger.debug('Query URL for OPTIONAL FK: "%s"' % url) 
                resp = self.catalog.get(url)
                resp.raise_for_status()
                if len(resp.json()) > 0:
                    row[fk_RID_column_name] = resp.json()[0]['RID']

        return row

    """
    Get the output.cif file
    """
    def getOutputCIF(self, file_url, filename):
        try:
            """
            Apply make-mmcif.py
            """
            hatracFile = '{}/{}'.format(self.make_mmCIF, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.make_mmCIF))
            args = [self.python_bin, 'make-mmcif.py', filename]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.make_mmCIF)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata)) 
                self.sendMail('FAILURE PDB', 'Can not make mmCIF for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (filename, stdoutdata, stderrdata))
                os.remove('{}/{}'.format(self.make_mmCIF, filename))
                return None
            
            os.remove('{}/{}'.format(self.make_mmCIF, filename))
            
            """
            Move the output.cif file to the scratch directory
            """
            shutil.move('{}/output.cif'.format(self.make_mmCIF), '{}/'.format(self.scratch))
            self.logger.debug('File {} was moved to the {} directory'.format('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))) 
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.sendMail('FAILURE PDB: Export make-mmcif.py', '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            os.chdir(currentDirectory)
            
        return '{}/output.cif'.format(self.scratch)
            
        

#!/usr/bin/python3

import os
import json
from deriva.core import PollingErmrestCatalog, init_logging
import subprocess
import logging
import sys
import traceback

# Loglevel dictionary
__LOGLEVEL = {'error': logging.ERROR,
              'warning': logging.WARNING,
              'info': logging.INFO,
              'debug': logging.DEBUG}

FORMAT = '%(asctime)s: %(levelname)s <%(module)s>: %(message)s'

logger = logging.getLogger(__name__)
loglevel = os.getenv('LOGLEVEL', 'info')
loglevel =__LOGLEVEL.get(loglevel)
init_logging(level=loglevel, log_format=FORMAT, file_path='/home/serban/log/pdb.log')

class WorkerRuntimeError (RuntimeError):
    pass

class WorkerBadDataError (RuntimeError):
    pass

class WorkUnit (object):
    def __init__(
            self,
            get_claimable_url,
            put_claim_url,
            put_update_baseurl,
            run_row_job,
            claim_input_data=lambda row: {'RID': row['RID'], 'Process_Status': "in progress"},
            failure_input_data=lambda row, e: {'RID': row['RID'], 'Process_Status': "error"}
    ):
        self.get_claimable_url = get_claimable_url
        self.put_claim_url = put_claim_url
        self.put_update_baseurl = put_update_baseurl
        self.run_row_job = run_row_job
        self.claim_input_data = claim_input_data
        self.failure_input_data = failure_input_data
        self.idle_etag = None

_work_units = []

def entry_row_job(handler):
    """
    Process mmCIF the PDB:entry table.
    """
    pdb_row_job(handler, 'entry')
    
def ihm_starting_model_details_row_job(handler):
    """
    Process mmCIF the PDB:ihm_starting_model_details table.
    """
    pdb_row_job(handler, 'ihm_starting_model_details')
    
def Entry_Related_File_row_job(handler):
    """
    Load data from the csv/tsv file into a table.
    """
    pdb_row_job(handler, 'Entry_Related_File')
    
def pdb_row_job(handler, action):
    """
    Run the script for generating pyramidal tiles.
    """
    
    try:
        row = handler.row
        unit = handler.unit
        
        logger.info('Running job for generating pyramidal tiles for RID="%s" and Filename="%s".' % (row['RID'], row['Original_File_Name'])) 
        args = ['env', 'action=%s' % action,  'RID=%s' % row['RID'], 'URL=https://%s/ermrest/catalog/2' % Worker.servername, '/usr/local/bin/pdb_workflow_processing.py', '--config', '%s' % Worker.config_file]
        p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        stdoutdata, stderrdata = p.communicate()
        returncode = p.returncode
    except:
        et, ev, tb = sys.exc_info()
        logger.error('got unexpected exception "%s"' % str(ev))
        logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
        returncode = 1
        
    if returncode != 0:
        logger.error('Could not execute the script for generating pyramidal tiles.\nstdoutdata: %s\nstderrdata: %s\n' % (stdoutdata, stderrdata)) 
        raise WorkerRuntimeError('Could not execute the script for generating pyramidal tiles.\nstdoutdata: %s\nstderrdata: %s\n' % (stdoutdata, stderrdata))
    else:
        logger.info('Finished job for generating pyramidal tiles for RID="%s" and Filename="%s".' % (row['RID'], row['Original_File_Name'])) 
        

_work_units.append(
    WorkUnit(
        '/entity/A:=PDB:entry/Process_Status=New;Process_Status=Renew/Vocab:workflow_status/Name=DEPO/$A?limit=1',
        '/attributegroup/PDB:entry/RID;Process_Status',
        '/attributegroup/PDB:entry/RID',
        entry_row_job
    )
)

"""
_work_units.append(
    WorkUnit(
        '/entity/A:=PDB:ihm_starting_model_details/Process_Status=New;Process_Status=Renew/Vocab:workflow_status/Name=DEPO/$A?limit=1',
        '/attributegroup/PDB:ihm_starting_model_details/RID;Process_Status',
        '/attributegroup/PDB:ihm_starting_model_details/RID',
        ihm_starting_model_details_row_job
    )
)
"""

_work_units.append(
    WorkUnit(
        '/entity/A:=PDB:Entry_Related_File/Process_Status=New;Process_Status=Renew/Vocab:workflow_status/Name=DEPO/$A?limit=1',
        '/attributegroup/PDB:Entry_Related_File/RID;Process_Status',
        '/attributegroup/PDB:Entry_Related_File/RID',
        Entry_Related_File_row_job
    )
)


class Worker (object):
    # server to talk to... defaults to our own FQDN
    servername = os.getenv('PDB_SERVER', 'pdb.isrd.isi.edu')

    # secret session cookie
    credfile = os.getenv('PDB_CREDENTIALS', '/home/secrets/serban/credentials.json')
    credentials = json.load(open(credfile))

    poll_seconds = int(os.getenv('PDB_POLL_SECONDS', '300'))
    config_file = os.getenv('PDB_CONFIG', '/home/serban/config/pdb.conf')

    # these are peristent/logical connections so we create once and reuse
    # they can retain state and manage an actual HTTP connection-pool
    catalog = PollingErmrestCatalog(
        'https', 
        servername,
        '1',
        credentials
    )
    catalog.dcctx['cid'] = 'pipeline/pdb'

    def __init__(self, row, unit):
        logger.info('Claimed job %s.\n' % row.get('RID'))

        self.row = row
        self.unit = unit

    work_units = _work_units # these are defined above w/ their funcs and URLs...

    @classmethod
    def look_for_work(cls):
        """Find, claim, and process work for each work unit.

        Do find/claim with HTTP opportunistic concurrency control and
        caching for efficient polling and quiescencs.

        On error, set Process_Status="failed: reason"

        Result:
         true: there might be more work to claim
         false: we failed to find any work
        """
        found_work = False

        for unit in cls.work_units:
            # this handled concurrent update for us to safely and efficiently claim a record
            try:
                unit.idle_etag, batch = cls.catalog.state_change_once(
                    unit.get_claimable_url,
                    unit.put_claim_url,
                    unit.claim_input_data,
                    unit.idle_etag
                )
            except:
                # keep going if we have a broken WorkUnit
                continue
            # batch may be empty if no work was found...
            for row, claim in batch:
                found_work = True
                try:
                    handler = cls(row, unit)
                    unit.run_row_job(handler)
                except WorkerBadDataError as e:
                    logger.error("Aborting task %s on data error: %s\n" % (row["RID"], e))
                    cls.catalog.put(unit.put_claim_url, json=[unit.failure_input_data(row, e)])
                    # continue with next task...?
                except WorkerRuntimeError as e:
                    logger.error("Aborting task %s on data error: %s\n" % (row["RID"], e))
                    cls.catalog.put(unit.put_claim_url, json=[unit.failure_input_data(row, e)])
                    # continue with next task...?
                except Exception as e:
                    cls.catalog.put(unit.put_claim_url, json=[unit.failure_input_data(row, e)])
                    raise

        return found_work

    @classmethod
    def blocking_poll(cls):
        return cls.catalog.blocking_poll(cls.look_for_work, polling_seconds=cls.poll_seconds)

Worker.blocking_poll()


#!/usr/bin/python
# 
# Copyright 2020 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Client for PDB workflow processing.
"""

import os
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
from email.mime.text import MIMEText
import socket
from datetime import datetime
from dateutil.parser import parse
from socket import gaierror, EAI_AGAIN
from requests import HTTPError
from subprocess import TimeoutExpired
import csv
import filecmp
import mimetypes
import tempfile

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote
from deriva.core.utils import hash_utils as hu
from deriva.core.utils.core_utils import DEFAULT_CHUNK_SIZE
from deriva.core.datapath import DataPathException

from deriva.utils.extras.data import insert_if_not_exist, update_table_rows, delete_table_rows, get_ermrest_query
from .mmcif_utils import get_mmcif_rid_optional_fkeys, print_restraint_table_fkeys, get_mmcif_rid_mandatory_fkeys
#from ...utils.shared import PDBDEV_CLI, DCCTX
from pdb_dev.utils.shared import PDBDEV_CLI, DCCTX
from pdb_dev.processing.processor import PipelineProcessor, ProcessingError, ErmrestError, ErmrestUpdateError, FileError

#mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'
catalog_dev_number = [99]

mmCIF_hold_records="""_pdbx_database_status.status_code                     <status_code>
_pdbx_database_status.entry_id                        <entry_id>
_pdbx_database_status.deposit_site                    RCSB
_pdbx_database_status.process_site                    RCSB
_pdbx_database_status.recvd_initial_deposition_date   <deposition_date> 
#
loop_
_database_2.database_id 
_database_2.database_code 
_database_2.pdbx_database_accession 
_database_2.pdbx_DOI 
"""

mmCIF_release_records="""_pdbx_database_status.status_code                     <status_code>
_pdbx_database_status.entry_id                        <entry_id>
_pdbx_database_status.deposit_site                    RCSB
_pdbx_database_status.process_site                    RCSB
_pdbx_database_status.recvd_initial_deposition_date   <deposition_date> 
# 
loop_
_pdbx_audit_revision_history.ordinal
_pdbx_audit_revision_history.data_content_type
_pdbx_audit_revision_history.major_revision
_pdbx_audit_revision_history.minor_revision
_pdbx_audit_revision_history.revision_date
1 'Structure model' 1 0 <revision_date> 
# 
_pdbx_audit_revision_details.ordinal             1
_pdbx_audit_revision_details.revision_ordinal    1
_pdbx_audit_revision_details.data_content_type   'Structure model'
_pdbx_audit_revision_details.provider            repository
_pdbx_audit_revision_details.type                'Initial release'
_pdbx_audit_revision_details.description         ?
#
loop_
_database_2.database_id 
_database_2.database_code 
_database_2.pdbx_database_accession 
_database_2.pdbx_DOI 
"""

Process_Status_Terms = {
    'NEW': 'New (trigger backend process)',
    'REPROCESS': 'Reprocess (trigger backend process after Error)',
    'IN_PROGRESS_UPLOADING_mmCIF_FILE': 'In progress: processing uploaded mmCIF file',
    'IN_PROGRESS_GENERATING_mmCIF_FILE': 'In progress: generating mmCIF file',
    'IN_PROGRESS_GENERATING_SYSTEM_FILES': 'In progress: generating system files',
    'IN_PROGRESS_RELEASING_ENTRY': 'In progress: releasing entry',
    'SUCCESS': 'Success',
    'RESUME': 'Resume (trigger backend process)',
    'ERROR_PROCESSING_UPLOADED_mmCIF_FILE': 'Error: processing uploaded mmCIF file',
    'ERROR_GENERATING_mmCIF_FILE': 'Error: generating mmCIF file',
    'ERROR_GENERATING_SYSTEM_FILES': 'Error: generating system files',
    'ERROR_RELEASING_ENTRY': 'Error: releasing entry',
    'IN_PROGRESS_PROCESSING_UPLOADED_RESTRAINT_FILES': 'In progress: processing uploaded restraint files',
    'ERROR_PROCESSING_UPLOADED_RESTRAINT_FILES': 'Error: processing uploaded restraint files'
    }


class EntryProcessor(PipelineProcessor):
    """Network client for PDB entry processing.
    """
    primary_accession_code_mode = "PDB"
    alternative_accession_code_mode = "PDBDEV"
    singularity_sif = 'ihmv_20250205.sif'       # default singularity for validation report (previous:'ihmv_20231222.sif')
    CifCheck = "/home/pdbihm/bin/CifCheck"
    py_rcsb_db = "/home/pdbihm/pdb/py-rcsb_db"
    scratch = "/mnt/vdb1/entry_processing/scratch"
    log_dir = "/home/pdbihm/log/entry_processing"
    
    def __init__(self, **kwargs):
        self.action = kwargs.get("action")
        self.rid = kwargs.get("rid")
        self.mmCIF_defaults = kwargs.get("mmCIF_defaults")
        self.vocab_ucode = kwargs.get("vocab_ucode")
        self.make_mmCIF = kwargs.get("make_mmCIF")
        self.mmCIF_Schema_Version = kwargs.get("mmCIF_Schema_Version")
        self.tables_groups = kwargs.get("tables_groups")
        self.export_tables = kwargs.get("export_tables")
        self.optional_fk_file = kwargs.get("optional_fk_file")
        self.cif_tables = kwargs.get("cif_tables")
        self.export_order_by = kwargs.get("export_order_by")
        self.combo1_columns = kwargs.get("combo1_columns")
        self.dictSdb = kwargs.get("dictSdb")
        self.entry = kwargs.get("entry")
        self.hatrac_namespace = kwargs.get("hatrac_namespace")
        self.validation_dir = kwargs.get("validation_dir")
        self.reportValidation = True if kwargs.get("reportValidation")=='Yes' else False
        
        if kwargs.get("python_bin", None): self.python_bin = kwargs.get("python_bin")
        if kwargs.get("py_rcsb_db", None): self.py_rcsb_db = kwargs.get("py_rcsb_db")
        if kwargs.get("CifCheck", None): self.CifCheck = kwargs.get("CifCheck")
        if kwargs.get("timeout", None): self.timeout = kwargs.get("timeout")
        
        if kwargs.get("scratch", None): self.scratch = kwargs.get("scratch")
        if kwargs.get("primary_accession_code_mode", None): self.primary_accession_code_mode = kwargs.get("primary_accession_code_mode")
        if kwargs.get("alternative_accession_code_mode", None): self.alternative_accession_code_mode = kwargs.get("alternative_accession_code_mode")
        if kwargs.get("singularity_sif", None): self.singularity_sif=kwargs.get("singularity_sif")
        if kwargs.get("email", None): self.email_config = kwargs.get("email")
        
        super().__init__(hostname=kwargs.get("hostname"), catalog_id=kwargs.get("catalog_id"), credentials = kwargs.get("credentials"),
                         cfg=kwargs.get("cfg"))
        
        self.is_catalog_dev = True if self.catalog_id in catalog_dev_number else False
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')
        if kwargs.get("log_dir", None): self.log_dir = kwargs.get("log_dir")
        if kwargs.get("verbose", None): self.verbose = kwargs.get("verbose")
        if kwargs.get("notify", None): self.notify = kwargs.get("notify")

    """
    Trace into the log_dir e.g. /home/pdbihm/log/trace.log file 
    """
    def trace(self, condition, message):
        if condition==True:
            fa = open(f'{self.log_dir}/trace.log', 'a')
            fa.write(message)
            fa.write('\n')
            fa.close()
    """
    Get the user email or full name
    """
    def getUser(self, schema, table, rid):
        user = None
        try:
            """
            Query for detecting the user email
            """
            rows = get_ermrest_query(self.catalog, schema, table, constraints=f"RID={rid}/B:=(M:RCB)=(public:ERMrest_Client:ID)", attributes=["B:Email","B:Full_Name"])
            if len(rows) == 1:
                row = rows[0]                
                user = row['Email'] if row['Email'] else row['Full_Name']
            return user
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            return None

    """
    Get the user_id 
    """
    def getUserId(self, schema, table, rid):
        try:
            """
            Query for detecting the user email
            """
            url = '/attribute/{}:{}/RID={}/RCB'.format(urlquote(schema), urlquote(table), urlquote(rid))
            self.logger.debug('Query user_id URL: "{}"'.format(url)) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            if len(rows) == 1:
                row = resp.json()[0]
                return row['RCB'].split('/')[-1]
            else:
                return None
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            return None

    """
    Get the user_id 
    """
    def getUserRCB(self, schema, table, rid):
        try:
            """
            Query for detecting the user email
            """
            url = '/attribute/{}:{}/RID={}/RCB'.format(urlquote(schema), urlquote(table), urlquote(rid))
            self.logger.debug('Query user_id URL: "{}"'.format(url)) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            if len(rows) == 1:
                row = resp.json()[0]
                return row['RCB']
            else:
                return None
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            return None

    """
    Start the process for generating pyramidal tiles
    """
    def start(self):
        rid = self.rid
        action = self.action
        try:
            if action == 'entry':
                self.action = 'DEPO'
                self.process_mmCIF('PDB','entry', rid)
            elif action == 'Entry_Related_File':
                self.action = 'DEPO'
                self.process_Entry_Related_File('PDB', 'Entry_Related_File', rid)
            elif action == 'export':
                self.action = 'SUBMIT'
                self.export_mmCIF('PDB', 'entry', rid)
            elif action == 'accession_code':
                self.action = 'SUBMISSION COMPLETE'
                self.set_accession_code(rid)
            elif action == 'release_mmCIF':
                self.action = 'RELEASE READY'
                self.addReleaseRecords(rid)
            else:
                self.logger.error('Unknown action: "%s".' % action)
        except:
            if action == 'Entry_Related_File':
                table = 'Entry_Related_File'
            else:
                table = 'entry'
            user = self.getUser('PDB', table, rid)
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'unexpected exception', 'Error', user)
            self.sendMail(subject, '%s\nThe process might have been stopped\n' % ''.join(traceback.format_exception(et, ev, tb)))
            raise
        
    """
    Process the csv/tsv file of the Entry_Related_File table
    """
    def process_Entry_Related_File(self, schema, table, related_file_rid):
        """
        Get the RCB user
        """
        user = self.getUser(schema, table, related_file_rid)
        
        """
        Query for detecting the record to be processed
        """
        constraints='RID=%s/T:=(M:File_Type)=(Vocab:File_Type:Name)' % (related_file_rid)
        rows = get_ermrest_query(self.catalog, schema, table, constraints=constraints,
                                 attributes=['M:File_Name','M:File_URL','M:File_MD5','M:structure_id','M:RCT','M:File_Format','M:Restraint_Process_Status','M:Restraint_Workflow_Status','T:Table_Name'] )
        row = rows[0]
        if self.verbose: print(json.dumps(row, indent=4))
        structure_id = row['structure_id']
        filename = row['File_Name']
        file_url = row['File_URL']
        #md5 = row['File_MD5']
        #creation_time = row['RCT']
        file_format = row['File_Format']
        csv_tname = row['Table_Name']
        
        if self.cfg.is_dev:       #if self.is_catalog_dev == True:
            subject = '{}: START with status = {} ({})'.format(related_file_rid, row['Restraint_Process_Status'], user)
            self.sendMail(subject, 'The Restraint Process Status of the Entry Related File with RID={} was changed to "{}".'.format(row['RID'], row['Restraint_Process_Status']), receivers=self.email_config['curators'])

        """
        prepare update row (assuming success)
        """
        update_cnames = ["Restraint_Process_Status", "Record_Status_Detail", "Restraint_Workflow_Status"]
        update_file_row = {}
        update_file_row['RID'] = related_file_rid
        update_file_row['Restraint_Workflow_Status'] = 'RECORD READY'
        update_file_row['Restraint_Process_Status'] = Process_Status_Terms['SUCCESS']
        update_file_row['Record_Status_Detail'] = None        
        
        
        """
        Download file and Load data from the csv/tsv files
        """
        try:
            self.hatrac_file.download_file(file_url, self.make_mmCIF, filename)
            delimiter = '\t' if file_format=='TSV' else ','        
            self.loadTableFromCSV(self.hatrac_file.file_path, delimiter, csv_tname, structure_id, related_file_rid, user)
            
            self.updateAttributes(schema, table, related_file_rid, update_cnames, update_file_row, user)
            self.logger.debug('Ended PDB Processing for the %s:%s table with RID %s.' % (schema, table, related_file_rid))
            
            if self.verbose: print("Good processing")
            if self.verbose: print('Ended PDB Processing for the %s:%s:%s with status:%s' % (schema, table, related_file_rid, update_file_row))
            
        except Exception as e:
            if self.verbose: print("error processing: %s" % (subject))
            # -- update the slide table with the failure result.
            update_file_row['Restraint_Workflow_Status'] = 'ERROR'
            update_file_row['Restraint_Process_Status'] = Process_Status_Terms['ERROR_PROCESSING_UPLOADED_RESTRAINT_FILES'],
            update_file_row['Record_Status_Detail'] = error_message
            self.updateAttributes(schema, table, related_file_rid, update_cnames, update_file_row, user)
            error_message = e
            subject = '%s %s: %s (%s)' % (related_file_rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_RESTRAINT_FILES'], user)
            self.handle_error(e, notify=False, subject=subject, re_raise=True)
            
            
        
    """
    Export the mmCIF file of the entry table
    """
    def export_mmCIF(self, schema_pdb, table_entry, rid, release=False, user_id=None):
        """
        Get the RCB user
        """
        user = self.getUser(schema_pdb, table_entry, rid)
        if user_id == None:
            user_id = self.getUserId(schema_pdb, table_entry, rid)
        
        if release == True:
            process_status_error = Process_Status_Terms['ERROR_RELEASING_ENTRY']
        else:
            if self.action == 'SUBMIT':
                process_status_error = Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE']
            else:
                process_status_error = Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES']
        
        deriva_tables = ['entry']
        mmCIF_tables = []
        mmCIF_ignored = []
        self.export_error_message = None

        """
        Query for detecting the record to be exported
        """
        pb = self.catalog.getPathBuilder()
        schema = pb.PDB
        entry = schema.entry
        RID = entry.RID
        path = entry.path
        path.filter(RID == rid)
        self.logger.debug('Query Export URL: {}'.format(path.uri))

        results = path.entities()
        results.fetch()
        file_url = results[0]['mmCIF_File_URL']
        filename = results[0]['mmCIF_File_Name']
        entry_id = results[0]['id']
        creation_time = results[0]['RCT']
        year = parse(creation_time).strftime("%Y")
        
        if self.is_catalog_dev == True:
            subject = '{}: {} ({})'.format(rid, results[0]['Process_Status'], user)
            self.sendMail(subject, 'The Process Status of the Entry with RID={} was changed to "{}".'.format(rid, results[0]['Process_Status']), receivers=self.email_config['curators'])

        hatracFile = self.getOutputCIF(rid, file_url, filename, user)
        if hatracFile == None:
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': process_status_error,
                                  'Record_Status_Detail': 'Update error in exportData():\n{}'.format(self.export_error_message),
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return 1
            
        tables = self.export_tables
        output = '{}/{}.cif'.format(self.scratch, entry_id)
        fw = open(output, 'w')
        
        mmCIF_export = self.cif_tables
        pks_map = self.export_order_by
        matrix_tables = ['ihm_2dem_class_average_fitting', 'ihm_geometric_object_transformation']
        collections_tables = ['ihm_entry_collection', 'ihm_entry_collection_mapping']

        def writeLine(line, loopLine=None):
            table_name = line[1:].split('.')[0]
            if table_name not in mmCIF_tables:
                mmCIF_tables.append(table_name)
            if table_name not in deriva_tables and table_name not in mmCIF_export and table_name not in mmCIF_ignored and not table_name.startswith('flr_'):
                mmCIF_ignored.append(table_name)
            if table_name not in deriva_tables and (table_name in mmCIF_export or table_name.startswith('flr_')):
                if table_name.startswith('flr_'):
                    return False
                if table_name in collections_tables:
                    return False
                if loopLine != None:
                    fw.write('{}\n'.format(loopLine))
                if table_name in matrix_tables:
                    try:
                        columns = line.split()
                        new_columns = []
                        for column in columns:
                            try:
                                r = re.search('(.*)[_]matrix[_]([0-9]+)[_]([0-9]+)$', column)
                                new_columns.append('{}_matrix[{}][{}]'.format(r.group(1),r.group(2),r.group(3)))
                            except:
                                new_columns.append(column)
                        fw.write('{}\n'.format(' '.join(new_columns)))
                    except:
                        fw.write('{}'.format(line))
                else:
                    fw.write('{}'.format(line))
                return True
            else:
                return False
        
        def getColumnValue(table_name, column_name, column_type, column_value):
            if column_value == None:
                return '.'
            if column_type in ['int4', 'float4']:
                return '{}'.format(column_value)
            if column_type == 'text':
                if '\t' in column_value:
                    self.logger.debug('tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    self.export_error_message = 'ERROR getColumnValue: tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                    return None
                if '\n' in column_value:
                    return '\n;{}\n;\n'.format(column_value)
                if '"' not in column_value and "“" not in column_value and "”" not in column_value and "'" not in column_value and ' ' not in column_value and not column_value.startswith('_'):
                    return column_value
                if '"' not in column_value and "“" not in column_value and "”" not in column_value:
                    return '"{}"'.format(column_value)
                if "'" not in column_value:
                    return "'{}'".format(column_value)
                else:
                    self.logger.debug('Both " and \' are in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    return '\n;{}\n;\n'.format(column_value)
                self.export_error_message = 'ERROR getColumnValue: Unhandled value in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                return None
            else:
                self.logger.debug('unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name))
                self.export_error_message = 'ERROR getColumnValue: unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name)
                return None

        def exportData(rid, user):
            try:
                for table_name, table_body in tables.items():
                    pk = table_body['pkey_columns']
                    try:
                        pk.remove('structure_id')
                    except:
                        pass
                    
                    """
                    if len(pk) == 0:
                        self.logger.debug('No PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) == 2:
                        self.logger.debug('2 PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) == 3:
                        self.logger.debug('3 PK Table {}, PK: {}'.format(table_name,pk))
                    elif len(pk) != 1:
                        self.logger.debug('More than 3 PK Table {}, PK: {}'.format(table_name,pk))
                    """
                            
                    if table_name in ['entry', 'chem_comp_atom', 'database_2', 'ihm_entry_collection', 'ihm_entry_collection_mapping', 'pdbx_audit_revision_details',
                                      'pdbx_audit_revision_history', 'pdbx_database_status']:
                        continue
                    table = schema.tables[table_name]
                    self.logger.debug('Exporting table: {}'.format(table_name))
                    path = table.path
                    if table_name in ['struct', 'pdbx_entry_details']:
                        entry_id_column = table.column_definitions['entry_id']
                        path.filter(entry_id_column == entry_id)
                    else:
                        structure_id = table.column_definitions['structure_id']
                        path.filter(structure_id == entry_id)
                    #self.logger.debug('Query Export Data URL: {}'.format(path.uri))
                    if table_name != 'audit_conform':
                        results = path.entities()
                        if len(pk) == 1:
                            #self.logger.debug('Sorting results based on: {}'.format(table.column_definitions[pk[0]]))
                            results.sort(table.column_definitions[pk[0]])
                        else:
                            pk = pks_map[table_name]
                            #self.logger.debug('Sorting results based on: ({}, {})'.format(table.column_definitions[pk[0]], table.column_definitions[pk[1]]))
                            results.sort(table.column_definitions[pk[0]], table.column_definitions[pk[1]])
                        results.fetch()
                        if len(results) == 0:
                            continue
                    else:
                        url = '/attribute/PDB:Supported_Dictionary/A:=PDB:Data_Dictionary/B:=Vocab:Data_Dictionary_Name/dict_location:=B:Location,dict_name:=B:Name,dict_version:=A:Version@sort(dict_name,dict_version)'
                        self.logger.debug('Query URL: "%s"' % url) 
                        resp = self.catalog.get(url)
                        resp.raise_for_status()
                        results = resp.json()
                        if len(results) == 0:
                            continue
                    deriva_tables.append(table_name)
                    if len(results) > 1:
                        fw.write('loop_\n')
                        for column in table_body['columns']:
                            if column['name'] == 'structure_id':
                                continue
                            if column['name'] not in table.column_definitions.keys():
                                continue
                            fw.write('_{}.{}\n'.format(table_name,column['name']))
                        for row in results:
                            line = []
                            for column in table_body['columns']:
                                column_name = column['name']
                                column_type = column['type']
                                if column_name == 'structure_id':
                                    continue
                                if column_name not in table.column_definitions.keys():
                                    continue
                                column_value = row[column_name]
                                value = getColumnValue(table_name, column_name, column_type, column_value)
                                if value == None:
                                    self.logger.debug('Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                    subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                                    self.sendMail(subject, 'Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                    self.export_error_message = 'ERROR exportData: Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value)
                                    return 1
                                line.append(value)
                            fw.write('{}\n'.format('\t'.join(line)))
                    elif len(results) == 1:
                        row = results[0]
                        for column in table_body['columns']:
                            column_name = column['name']
                            column_type = column['type']
                            if column_name == 'structure_id':
                                continue
                            if column_name not in table.column_definitions.keys():
                                continue
                            column_value = row[column_name]
                            value = getColumnValue(table_name, column_name, column_type, column_value)
                            if value == None:
                                self.logger.debug('Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                                self.sendMail(subject, 'Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                                self.export_error_message = 'ERROR exportData: Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value)
                                return 1
                            fw.write('_{}.{}\t{}\n'.format(table_name, column['name'], value))
                        
                    fw.write('#\n')    
                
                return 0
            except:
                et, ev, tb = sys.exc_info()
                self.logger.debug('exportData got exception "%s"' % str(ev))
                self.logger.debug('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                self.sendMail(subject, '%s\nThe process might have been stopped\n' % ''.join(traceback.format_exception(et, ev, tb)))
                self.export_error_message = 'ERROR exportData: "%s"' % str(ev)
                return 1

        def exportCIF(rid, user):
            fr = open(hatracFile, 'r')
            lines = fr.readlines()
            status = 'skip'

            for line in lines:
                if status == 'skip':
                    if line.strip() == 'loop_':
                        status = 'loop'
                    elif line.startswith('_'):
                        if writeLine(line):
                            status = 'columns'
                        
                elif status == 'loop':
                    if line.startswith('_'):
                        if writeLine(line, loopLine='loop_'):
                            status = 'columns'
                        else:
                            status = 'skip'
                    else:
                        self.logger.debug('Unexpected line after loop_:\n{}'.format(line))
                        fr.close()
                        subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                        self.sendMail(subject, 'status = {}\nThe process might have been stopped\n'.format(status))
                        self.export_error_message = 'ERROR exportCIF: Unknown status', 'status = {}'.format(status)
                        return 1
            
                elif status == 'columns':
                    if line.startswith('_'):
                        if not writeLine(line):
                            status = 'skip'
                    elif line.strip() == 'loop_':
                        status = 'loop'
                    else:
                        fw.write('{}'.format(line))
                        status = 'rows'
            
                elif status == 'rows':
                    if line.strip() == 'loop_':
                        status = 'loop'
                    elif line.startswith('_'):
                        if writeLine(line):
                            status = 'columns'
                        else:
                            status = 'skip'
                    else:
                        fw.write('{}'.format(line))
                else:
                    self.logger.debug('Unknown status: {}'.format(status))
                    subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                    self.sendMail(subject, 'status = {}\nThe process might have been stopped\n'.format(status))
                    self.export_error_message = 'ERROR exportCIF: Unknown status', 'status = {}'.format(status)
                    return 1
        
            fr.close()
            return 0
            
        fw.write('data_{}\n\n'.format(entry_id))
        value = getColumnValue('entry', 'id', 'text', '{}'.format(entry_id))
        fw.write('#\n_entry.id  {}\n#\n'.format(value))

        if exportData(rid, user) != 0:
            self.logger.debug('Update error in exportData()')
            fw.close()
            os.remove(hatracFile)
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': process_status_error,
                                  'Record_Status_Detail': 'Update error in exportData():\n{}'.format(self.export_error_message),
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return 1
        if exportCIF(rid, user) != 0:
            self.logger.debug('Update error in exportCIF()')
            fw.close()
            os.remove(hatracFile)
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': process_status_error,
                                  'Record_Status_Detail': 'Update error in exportCIF():\n{}'.format(self.export_error_message),
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return 1
        fw.close()
        os.remove(hatracFile)
        
        if len(mmCIF_ignored) > 0:
            self.logger.debug('Tables from the mmCIF file that were not included in the export:')
            for table_name in sorted(mmCIF_ignored):
                self.logger.debug('\t{}'.format(table_name))
                
        file_name = '{}.cif'.format(entry_id)
        returncode,error_message = self.validateExportmmCIF(self.scratch, file_name, year, entry_id, rid, user, process_status_error, user_id)

        if returncode == 0:
            """
            Generate the Conform_Dictionary entries
            """
            returncode = self.generateConformDictionary ('PDB', 'entry', entry_id, process_status_error, rid, user)
            
            if returncode == 0:
                self.logger.debug('Update success in export_mmCIF()')
                if release == False and self.action == 'SUBMIT':
                    self.updateAttributes(schema_pdb,
                                          table_entry,
                                          rid,
                                          ["Process_Status", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': Process_Status_Terms['SUCCESS'],
                                          'Workflow_Status': 'mmCIF CREATED'
                                          },
                                          user)
                    subject = '{} {}: {} ({})'.format(rid, 'mmCIF CREATED', Process_Status_Terms['SUCCESS'], user)
                    self.sendMail(subject, 'The workflow status of the entry with RID={} was changed to mmCIF CREATED.'.format(rid), receivers=self.email_config['curators'])
            else:
                self.updateAttributes(schema_pdb,
                                      table_entry,
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': process_status_error,
                                      'Record_Status_Detail': 'ERROR in validateExportmmCIF:\n{}'.format(error_message),
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                return 1
            return 0
            
        else:
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
            self.sendMail(subject, error_message)
            return 1
            
    """
    Generate the Conform_Dictionary
    """
    def generateConformDictionary (self, schema_pdb, table_entry, entry_id, process_status_error, rid, user):
        """
        Generate the Conform_Dictionary entries
        """
        
        """
        Get the RID of Entry_Generated_File
        """
        url = '/attribute/PDB:Entry_Generated_File/Structure_Id={}&File_Type=mmCIF/RID'.format(urlquote(entry_id))
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        mmCIF_rows = resp.json()
        if len(mmCIF_rows) != 1:
            self.logger.debug('Entry_Generated_File is not unique')
            self.updateAttributes(schema_pdb,
                                  table_entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': process_status_error,
                                  'Record_Status_Detail': 'Entry_Generated_File is not unique',
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return 1

        mmCIF_row = mmCIF_rows[0]
        
        """
        Delete entries from Conform_Dictionary if any
        """
        url = '/entity/PDB:Conform_Dictionary/Exported_mmCIF_RID={}'.format(urlquote(mmCIF_row['RID']))
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        conform_rows = resp.json()
        if len(conform_rows) > 0:
            url = '/entity/PDB:Conform_Dictionary/Exported_mmCIF_RID={}'.format(urlquote(mmCIF_row['RID']))
            resp = self.catalog.delete(
                url
            )
            resp.raise_for_status()
            self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 

        """
        Get the supported entries
        """
        url = '/attribute/PDB:Supported_Dictionary/Data_Dictionary_RID'
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        supported_rows = resp.json()
        
        """
        Insert rows into Conform_Dictionary
        """
        for supported_row in supported_rows:
            row = {'Data_Dictionary_RID': supported_row['Data_Dictionary_RID'], 'Exported_mmCIF_RID': mmCIF_row['RID']}
        
            if self.createEntity('PDB:Conform_Dictionary', row, rid, user) == None:
                self.updateAttributes(schema_pdb,
                                      table_entry,
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': process_status_error,
                                      'Record_Status_Detail': 'Update error in createEntity():\n{}'.format(self.export_error_message),
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                return 1

        return 0
    
    """
    Insert a row in a table
    """
    def createEntity (self, path, row, rid, user):
        """
        Insert the row in the table.
        """
        try:
            url = '/entity/%s' % (path)
            resp = self.catalog.post(
                url,
                json=[row]
            )
            resp.raise_for_status()
            
            self.logger.debug('SUCCEEDED created in the table "%s" the entry "%s".' % (url, json.dumps(row, indent=4))) 
            return url
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.export_error_message = 'ERROR createEntity: "%s"' % str(ev)
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
            self.sendMail(subject, 'RID: %s\n%s\n' % (rid, ''.join(traceback.format_exception(et, ev, tb))))
            return None

    """
    Process the mmCIF file of the entry table
    """
    def process_mmCIF(self, schema, table, rid):
        """
        Get the RCB user
        """
        user = self.getUser(schema, table, rid)
        
        """
        Query for detecting the record to be processed
        """
        url = '/entity/%s:%s/RID=%s' % (urlquote(schema), urlquote(table), urlquote(rid))
        self.logger.debug('Query URL: "%s"' % url) 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row = resp.json()[0]
        filename = row['mmCIF_File_Name']
        file_url = row['mmCIF_File_URL']
        md5 = row['mmCIF_File_MD5']
        last_md5 = row['Last_mmCIF_File_MD5']
        id = row['id']
        creation_time = row['RCT']
        
        if self.is_catalog_dev == True:
            subject = '{}: {} ({})'.format(rid, row['Process_Status'], user)
            self.sendMail(subject, 'The Process Status of the Entry with RID={} was changed to "{}".'.format(rid, row['Process_Status']), receivers=self.email_config['curators'])

        """
        Check if we have a new mmCIF file
        """
        if md5 == last_md5:
            self.logger.debug('RID="{}", Skipping loading the table as the mmCIF file is unchanged'.format(rid))
            obj = {}
            obj['RID'] = rid
            obj['Workflow_Status'] = 'RECORD READY'
            obj['Process_Status'] = Process_Status_Terms['SUCCESS']
            columns = ['Workflow_Status', 'Process_Status']
            self.updateAttributes(schema,
                             table,
                             rid,
                             columns,
                             obj,
                             user)
       
            subject = '{} {}: {} ({})'.format(rid, 'RECORD READY', Process_Status_Terms['SUCCESS'], user)
            self.sendMail(subject, 'The workflow status of the entry with RID={} was changed to RECORD READY. Same mmCIF file content.'.format(rid), receivers=self.email_config['curators'])
            self.logger.debug('RID="{}", mmCIF file is the same. Ended PDB Processing for the {}:{} table.'.format(rid, schema, table)) 
            return
        
        """
        Cleanup the self.make_mmCIF directory 
        """
        for entry in os.scandir(self.make_mmCIF):
            if entry.is_file() and entry.path.endswith('.cif'):
                os.remove(entry.path)
                self.logger.debug('Removed file {}'.format(entry.path))
            
        """
        Extract the file from hatrac
        """
        f,error_message = self.getHatracFile(filename, file_url, self.make_mmCIF, rid, user)
        
        if f == None:
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return
        
        """
        Get the md5 if necessary
        """
        if md5 == None:
            md5 = self.md5hex(f)
            self.logger.debug("The MD5 was computed and it is: %s" % md5)
            
        """
        Convert the file to JSON and load the data into the tables
        """
        returncode,error_message = self.convert2json(filename, id, rid, user)
        
        if returncode != 0:
            """
            Update the slide table with the failure result.
            """
            self.updateAttributes(schema,
                                  table,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return
                        
                            
        obj = {}
        obj['RID'] = rid
        obj['Workflow_Status'] = 'RECORD READY'
        obj['Process_Status'] = Process_Status_Terms['SUCCESS']
        obj['mmCIF_File_MD5'] = md5
        obj['Record_Status_Detail'] = None
        obj['Last_mmCIF_File_MD5'] = md5
        columns = ['Workflow_Status', 'Process_Status', 'mmCIF_File_MD5', 'Record_Status_Detail', 'Last_mmCIF_File_MD5']
        self.updateAttributes(schema,
                         table,
                         rid,
                         columns,
                         obj,
                         user)
   
        subject = '{} {}: {} ({})'.format(rid, 'RECORD READY', Process_Status_Terms['SUCCESS'], user)
        self.sendMail(subject, 'The workflow status of the entry with RID={} was changed to RECORD READY.'.format(rid), receivers=self.email_config['curators'])
        self.logger.debug('Ended PDB Processing for the %s:%s table.' % (schema, table)) 
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, filename, file_url, input_dir, rid, user):
        error_message = None
        try:
            hatracFile = '{}/{}'.format(input_dir, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return (hatracFile,error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR getHatracFile: "%s"' % str(ev)
            return (None,error_message)
            
    """
    Convert the input file to JSON.
    1. generate mmcif file using make_mmcif
    2. Move output.cif file to the rcsb/db/tests-validate/test-output/ihm-files
    3. from py_rcsb_db dir, cp rcsb/db/config/exdb-config-example-ihm-DEPO.yml to rcsb/db/config/exdb-config-example-ihm.yml
    4. run 'rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py'
       > env PYTHONPATH=~/pdb/py-rcsb_db python3 testSchemaDataPrepValidate-ihm.py
    5. copy output.cif to  /home/pdbihm/temp
    HT TODO: refactor code
    """
    def convert2json(self, filename, entry_id, rid, user):
        try:
            """
            Prepend the RID to the input file
            """
            shutil.move('{}/{}'.format(self.make_mmCIF, filename), '{}/{}_{}'.format(self.make_mmCIF, rid, filename))
            filename = '{}_{}'.format(rid, filename)
            output_cif = '%s_output.cif' % (rid)

            """
            Apply make_mmcif.py
            """
            error_message = None
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.make_mmCIF))
            args = [self.python_bin, '-m', 'ihm.util.make_mmcif', filename, output_cif]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.make_mmCIF)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not make mmCIF for entry id = "%s" and file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (entry_id, filename, stdoutdata, stderrdata)) 
                subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, 'Can not make mmCIF for entry id = "%s" and file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (entry_id, filename, stdoutdata, stderrdata))
                os.remove('{}/{}'.format(self.make_mmCIF, filename))
                error_message = 'ERROR convert2json: {}'.format(stderrdata)
                return (returncode,error_message)
            
            os.remove('{}/{}'.format(self.make_mmCIF, filename))

            output_cif_fpath = '%s/%s' % (self.make_mmCIF, output_cif)
            py_rcsb_db_input_cif_dir = '%s/rcsb/db/tests-validate/test-output/ihm-files' % (self.py_rcsb_db)
            py_rcsb_db_input_cif_fpath = '%s/%s' % (py_rcsb_db_input_cif_dir, output_cif)
            py_rcsb_db_output_json_dir = '%s/rcsb/db/tests-validate/test-output' % (self.py_rcsb_db)            
            
            """
            Cleanup the rcsb/db/tests-validate/test-output/ihm-files (*.cif) and rcsb/db/tests-validate/test-output directories (*.json)
            Since we will use the .cif and .json in those dirs for processing (instead of specifying as arguments). 
            """
            fpath = py_rcsb_db_input_cif_dir
            for entry in os.scandir(fpath):
                if entry.is_file() and entry.path.endswith('.cif'):
                    os.remove(entry.path)
                    self.logger.debug('Removed file {}'.format(entry.path))
            
            fpath = py_rcsb_db_output_json_dir
            for entry in os.scandir(fpath):
                if entry.is_file() and entry.path.endswith('.json'):
                    os.remove(entry.path)
                    self.logger.debug('Removed file {}'.format(entry.path))

            """
            Move the output.cif file to the rcsb/db/tests-validate/test-output/ihm-files directory and apply testSchemaDataPrepValidate-ihm.py
            """
            '''HT TODO: CLEANUP
            shutil.move('{}/output.cif'.format(self.make_mmCIF), '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db))
            self.logger.debug('convert2jason: File {} was moved to the {} directory'.format(output_cif_fpath, '{}/rcsb/db/tests-validate/test-output/ihm-files/'.format(self.py_rcsb_db)))
            '''
            shutil.move(output_cif_fpath, py_rcsb_db_input_cif_dir)
            self.logger.debug('convert2jason: File %s was moved to the %s directory' % (output_cif, py_rcsb_db_input_cif_dir))
            
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.py_rcsb_db))
            shutil.copy2(f'{self.py_rcsb_db}/rcsb/db/config/exdb-config-example-ihm-DEPO.yml', f'{self.py_rcsb_db}/rcsb/db/config/exdb-config-example-ihm.yml')
            args = ['env', 'PYTHONPATH={}'.format(self.py_rcsb_db), self.python_bin, 'rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py']
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.py_rcsb_db)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)

            if returncode != 0:
                self.logger.error('Can not validate testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (output_cif, stdoutdata, stderrdata)) 
                subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, 'Can not make testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (output_cif, stdoutdata, stderrdata))
                #os.remove('%s/rcsb/db/tests-validate/test-output/ihm-files/%s' % (self.py_rcsb_db, output_cif))
                os.remove(py_rcsb_db_input_cif_fpath)
                error_message = 'ERROR convert2json: {}'.format(stderrdata)
                return (returncode,error_message)

            '''TODO: CLEANUP
            shutil.copy2('%s/rcsb/db/tests-validate/test-output/ihm-files/%s' % (self.py_rcsb_db, output_cif), '/home/pdbihm/temp')
            os.remove('%s/rcsb/db/tests-validate/test-output/ihm-files/%s' % (self.py_rcsb_db, output_cif))
            self.logger.debug('convert2josn: File {}/{} was removed'.format(self.py_rcsb_db, 'rcsb/db/tests-validate/test-output/ihm-files/%s' % (output_cif)))
            '''
            
            shutil.copy2(py_rcsb_db_input_cif_fpath, '/home/pdbihm/temp')
            os.remove(py_rcsb_db_input_cif_fpath)
            self.logger.debug('convert2josn: File %s was removed' % (py_rcsb_db_input_cif_fpath))
            
            """
            Load now the data from JSON files which are in the rcsb/db/tests-validate/test-output directory into the tables 
            """
            fpath = py_rcsb_db_output_json_dir
            json_files = []
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        json_files.append(entry.name)
            self.logger.debug('The following JSON files were generated in the {}/rcsb/db/tests-validate/test-output directory:\n\t{}'.format(self.py_rcsb_db, '\n\t'.join(json_files))) 

            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        returncode,error_message = self.loadTablesFromJSON(entry.path, entry_id, rid, user)
                        if returncode != 0:
                            break
            
            """
            Remove the JSON files that were created
            """
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        os.remove(entry.path)
                        self.logger.debug('Removed file {}'.format(entry.path))
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            os.chdir(currentDirectory)
            returncode = 1
            error_message = 'ERROR convert2json: "%s"' % str(ev) 
            
        return (returncode,error_message)
            
        
    """
    Update the ermrest attributes
    """
    def updateAttributes (self, schema, table, rid, columns, row, user):
        """
        Update the ermrest attributes with the row values.
        """
        try:
            columns = ','.join([urlquote(col) for col in columns])
            url = '/attributegroup/%s:%s/RID;%s' % (urlquote(schema), urlquote(table), columns)
            resp = self.catalog.put(url, json=[row])
            resp.raise_for_status()
            self.logger.debug('SUCCEEDED updated the table "%s" for the RID "%s"  with "%s".' % (url, rid, json.dumps(row, indent=4))) 
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            if 'Process_Status' in row.keys():
                status = row['Process_Status']
            elif 'Restraint_Process_Status' in row.keys():
                status = row['Restraint_Process_Status']
            else:
                status = None
            subject = '{} {}: {} ({})'.format(rid, 'ERROR', status, user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            

    """
    Get the hexa md5 checksum of the file.
    """
    def md5hex(self, fpath):
        h = hashlib.md5()
        try:
            f = open(fpath, 'rb')
            try:
                b = f.read(4096)
                while b:
                    h.update(b)
                    b = f.read(4096)
                return h.hexdigest()
            finally:
                f.close()
        except:
            return None

    """
    Sort the tables to be loaded based on the FK dependencies.
    """
    def sortTable(self, fpath):
        excluded_mmCIF_tables = [
            'entry', 'database_2', 'pdbx_audit_revision_details', 'pdbx_audit_revision_history', 'pdbx_database_status'
        ]

        """
        Get the tables groups
        """
        with open(self.tables_groups, 'r') as f:
            table_groups = json.load(f)
        
        """
        Sort the tables from the JSON file based on the groups
        """
        tables = []
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]
            group_no = 0
            while group_no < len(table_groups):
                group_str = str(group_no)
                for k,v in pdb.items():
                    if k in table_groups[group_str] and k not in excluded_mmCIF_tables:
                        tables.append(k)
                group_no +=1
        """
        Check that all the tables are in the database
        """
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]
            for k,v in pdb.items():
                if k not in (tables + excluded_mmCIF_tables):
                    raise RuntimeError('Table "{}" from mmCIF is not present in the DERIVA database. Possible mismatch versions.'.format(k))
        
        return tables
        
    """
    Rollback JSON inserted rows
    """
    def rollbackInsertedRows(self, records, entry_id, user):
        records.reverse()
        for record in records:
            tname = record['name']
            rows = record['rows']
            for row in rows:
                rid = row['RID']
                try:
                    if 'structure_id' in row.keys():
                        path = '%s:%s/%s=%s' % (urlquote('PDB'), urlquote(tname), urlquote('structure_id'), urlquote(entry_id))
                    else:
                        path = '%s:%s/%s=%s' % (urlquote('PDB'), urlquote(tname), urlquote('RID'), urlquote(rid))
                    url = '/entity/%s' % (path)
                    resp = self.catalog.delete(
                        url
                    )
                    resp.raise_for_status()
                    self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
                    if 'structure_id' in row.keys():
                        break
                except HTTPError as e:
                    if e.response.status_code == HTTPStatus.NOT_FOUND:
                        self.logger.debug('No rows found to delete from the URL "%s".' % (url))
                    else:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                        subject = '{} {}: {} ({})'.format(entry_id, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_RESTRAINT_FILES'], user)
                        self.sendMail(subject, 'URL: %s\n%s\n' % (url, ''.join(traceback.format_exception(et, ev, tb))))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    subject = '{} {}: {} ({})'.format(entry_id, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_RESTRAINT_FILES'], user)
                    self.sendMail(subject, 'URL: %s\n%s\n' % (url, ''.join(traceback.format_exception(et, ev, tb))))
                
    """
    Load data into the tables from the JSON file.
    """
    def loadTablesFromJSON(self, fpath, entry_id, rid, user):
        shutil.copy2(fpath, '/home/pdbihm/temp')
        
        """
        Tables that have a NOT NULL *_RID column
        """
        fk_tables = []
        for fk_table in self.combo1_columns.keys():
            if fk_table not in fk_tables:
                fk_tables.append(fk_table)
        
        schema_name = 'PDB'
        pb = self.catalog.getPathBuilder()
        returncode = 0
        error_message = None
        
        """
        Read the JSON file data
        """
        with open(fpath, 'r') as f:
            pdb = json.load(f)
            pdb = pdb[0]

        """
        Read the JSON FK optional file
        """
        with open(self.optional_fk_file, 'r') as f:
            optional_fks = json.load(f)

        """
        Sort the tables based on the FK dependencies
        """
        try:
            tables = self.sortTable(fpath)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            returncode = 1
            error_message = str(ev)
            return (returncode,error_message)
        
        """
        Keep track of the inserted rows in case of rollback
        """
        inserted_records = []
        
        model_root = self.catalog.getCatalogModel()
        for tname in tables:
            records = pdb[tname]
            if type(records) is dict:
                records = [records]
    
            table = pb.schemas[schema_name].tables[tname]
            entities = []
            for r in records:
                """
                Replace the FK references to the entry table
                """
                r = self.getUpdatedRecord(tname, r, entry_id, model_root.schemas['PDB'].tables[tname], inserted_records, fk_tables)
                if self.is_catalog_dev == True:
                    if tname == 'ihm_entity_poly_segment':
                        r = self.getUpdatedEntityPolySegment(r)
                self.getUpdatedOptional(optional_fks, tname, r, entry_id)
                entities.append(r)
            
            self.logger.debug('Table {}, inserting {} rows'.format(tname, len(entities)))
            """
            Insert the data
            """
            try:
                res = table.insert(entities).fetch()
                inserted_records.append({'name': tname, 'rows': res})
                inserted_rows = len(entities)
                self.logger.debug('File {}: inserted {} rows into table {}'.format(fpath, inserted_rows, tname))
                #self.logger.debug('Inserted into table {} the {} rows:\n'.format(tname, entities))
            except DataPathException as e:
                et, ev, tb = sys.exc_info()
                self.logger.error(e)
                self.logger.error(e.message)
                self.logger.error(e.reason)
                subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                returncode = 1
                error_message = 'Error in inserting values into the table {}\n{}'.format(tname, e.message)
                self.rollbackInsertedRows(inserted_records, entry_id, user)
                break
            except HTTPError as e:
                et, ev, tb = sys.exc_info()
                self.logger.error(e)
                self.logger.error(e.response.text)
                subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                returncode = 1
                error_message = 'Error in inserting values into the table {}\n{}'.format(tname, e.response.text)
                self.rollbackInsertedRows(inserted_records, entry_id, user)
                break
            except:
                et, ev, tb = sys.exc_info()
                self.logger.error('got exception "%s"' % str(ev))
                self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
                returncode = 1
                error_message = 'Error in inserting values into the table {}\n{}'.format(tname, str(ev))
                self.rollbackInsertedRows(inserted_records, entry_id, user)
                break
        
        return (returncode,error_message)

    # note: if the structure in print statement is a tuple, need to convert to string first
    def print_pk_tables_dict(self, pk_table_dict, data_dict=False, limit=50):
        """
        limit puts a cap on the number of data rows to print
        """
        for pk_tname in pk_table_dict.keys():
            print("   - tname : %s" % pk_tname)
            for constraint_name in pk_table_dict[pk_tname].keys():
                if not data_dict:
                    print("      key name : %s" % (str(constraint_name)))
                    print("         %s" % (str(pk_table_dict[pk_tname][constraint_name])))
                    #print("         %s" % (json.dumps(pk_table_dict[pk_tname][constraint_name])))
                    continue
                count=0
                print("      key name : %s [%d]" % (str(constraint_name), len(pk_table_dict[pk_tname][constraint_name].keys())))
                for k,v in pk_table_dict[pk_tname][constraint_name].items():
                    print("        k: %s -> %s" % (k, json.dumps(v, indent=2)))
                    count+=1
                    if count >= limit: break
 
    def handle_error(self, e, re_raise=False, notify=True, subject=None):
        et, ev, tb = sys.exc_info()
        tb_message = ''.join(traceback.format_exception(et, ev, tb))
        self.logger.error('-- Got exception "%s: %s"' % (et.__name__, str(ev)))
        self.logger.error('%s' % (tb_message))
        self.sendMail(subject, tb_message)
        if self.verbose: print(tb_message)
        if re_raise: raise
    
    def loadTableFromCSV(self, fpath, delimiter, tname, entry_id, related_file_rid, user):
        catalog = self.catalog
        model = self.catalog.getCatalogModel()
        table = model.schemas["PDB"].tables[tname]
        structure_id = entry_id
        entry_rid = entry_id.replace('D_', '')  # This might not aways be true

        """
        Get entry RID
        """
        entry_row = get_ermrest_query(catalog, "PDB", "entry", constraints="id=%s" % (urlquote(structure_id)))[0]
        entry_rid = entry_row["RID"]
        
        """
        Read csv file
        """
        try :
            csvfile = open(fpath, 'r')
            reader = csv.DictReader(csvfile, delimiter=delimiter)
            headers = reader.fieldnames
        
            # check missing headers or unrecognized headers
            bad_headers = []
            for header in headers:
                if header not in table.columns.elements:
                    print("header: %s is not a column" % (header))
                    bad_headers.append(header)
            if bad_headers:
                raise FileError("HEADER ERROR: table %s contains undefined columns: %s" % (tname, bad_headers))
            
            """
            Create initial payload based on the file. "" is treated as NULL.
            Note: ermrest deals with text of int/float. No need to convert.
            """
            payload = []
            for row in reader:
                for k,v in row.items():
                    #print("k: %s, type: %s, name: %s" % (k, table.columns[k].type, table.columns[k].type.typename))
                    if k not in table.columns.elements: continue
                    #type = table.columns[k].type.typename
                    if v == "":
                        row[k] = None
                if "Structure_RID" in table.columns.elements:
                    row["Structure_RID"] = entry_rid
                if "structure_id" in table.columns.elements:
                    row["structure_id"] = structure_id
                if "Entry_Related_File" in table.columns.elements:
                    row["Entry_Related_File"] = related_file_rid
                payload.append(row)
            csvfile.close()
            if self.verbose: print("%s raw payload[%d][0:2]: %s" % (tname, len(payload), json.dumps(payload[0:2], indent=4)))
        except FileError as e:
            raise FileError(e)
        except Exception as e:
            raise FileError("CSV ERROR: unable to read file %s ()" % (fpath, e))
        
        """
        Read content from needed parent tables, so RID can be obtained in bulk.
        pk_tables_raw: dict of corresponding payload based on table name
        """
        try: 
            # - Prepare raw data from parent tables
            if self.verbose: print_restraint_table_fkeys(table)
            combo_fkeys = get_mmcif_rid_optional_fkeys(table) + get_mmcif_rid_mandatory_fkeys(table)
            
            #  - query parent tables once
            pk_tables_raw = {}  # raw data based on tname
            for fkey in combo_fkeys:
                pk_tname = fkey.pk_table.name
                if pk_tname in pk_tables_raw.keys(): continue
                pk_tables_raw[pk_tname] = get_ermrest_query(catalog, "PDB", pk_tname, "structure_id=%s" % (structure_id))
        except Exception as e:
            raise ErmrestError("Unable to read data from parent tables of %s (%s)" % (tname, e))

        # - create lookup tables based on keys used in fkey definition. The keys are sorted to get canonical key. 
        pk_tables_constraint2keys={}
        pk_tables_key2constraints={}
        pk_tables_to_cname2from_cnames={}
        # key value to row based on tname and key_cnames e.g. { tname : { ("group_id", "feature_id") : { [1, 2] : row } } }
        # RID is removed from key_cnames because we want to look up based on natural key
        pk_tables_key2rows = {}     
        for fkey in combo_fkeys:
            pk_tname = fkey.pk_table.name
            from_cnames = [ col.name for col in fkey.column_map.keys() ]        
            to_cnames = [ col.name for col in fkey.column_map.values() ]
            to_cname2from_cnames = { to_col.name : from_col.name for from_col, to_col in fkey.column_map.items() }
            if self.verbose: print("fkey: name:%s %s -> %s : %s" % (fkey.constraint_name, from_cnames, pk_tname, to_cnames))
            rid_index = to_cnames.index("RID")
            to_cnames.remove("RID")
            key_cnames = tuple(sorted(to_cnames))
            pk_tables_to_cname2from_cnames[pk_tname] = pk_tables_to_cname2from_cnames.get(pk_tname, {})
            pk_tables_to_cname2from_cnames[pk_tname][fkey.constraint_name] = to_cname2from_cnames
            pk_tables_constraint2keys[pk_tname] = pk_tables_constraint2keys.get(pk_tname, {})
            pk_tables_constraint2keys[pk_tname][fkey.constraint_name] = key_cnames        
            pk_tables_key2constraints[pk_tname] = pk_tables_key2constraints.get(pk_tname, {})
            pk_tables_key2constraints[pk_tname][key_cnames] = pk_tables_key2constraints[pk_tname].get(key_cnames, [])
            pk_tables_key2constraints[pk_tname][key_cnames].append(fkey.constraint_name)
            # - initialize 
            pk_tables_key2rows[pk_tname] = pk_tables_key2rows.get(pk_tname, {}) 
            if key_cnames in pk_tables_key2rows[pk_tname].keys(): continue # pk_table_dict already generated
            # -- create a dict based on cannonical key
            pk_table_dict = {}
            for row in pk_tables_raw[pk_tname]:
                k = tuple([ str(row[cname]) for cname in key_cnames ])  # convert to text
                pk_table_dict[k] = row
            pk_tables_key2rows[pk_tname][key_cnames] = pk_table_dict
            

        """
        print structures
        """
        if self.verbose:
            print("\npk_tables_constraint2keys: %s ==>" % (pk_tables_constraint2keys))
            self.print_pk_tables_dict(pk_tables_constraint2keys, data_dict=False)
    
            print("\npk_tables_key2constraints: ==> %s" % (pk_tables_key2constraints))
            self.print_pk_tables_dict(pk_tables_key2constraints, data_dict=False)
    
            print("\npk_tables_to_cname2from_cnames: ==>")
            self.print_pk_tables_dict(pk_tables_to_cname2from_cnames, data_dict=True)
        
            print("\npk_tables_key2rows: ==> ")
            self.print_pk_tables_dict(pk_tables_key2rows, data_dict=True, limit=5)
            print("------------")
            
        """
        update payload with RID columns
        """
        # -for each fkey, fill in corresponding RID column
        for fkey in combo_fkeys:
            pk_tname = fkey.pk_table.name
            rid_cname = pk_tables_to_cname2from_cnames[pk_tname][fkey.constraint_name]["RID"]
            key_cnames = pk_tables_constraint2keys[pk_tname][fkey.constraint_name]
            to_cname2from_cnames = pk_tables_to_cname2from_cnames[pk_tname][fkey.constraint_name]        
            key_from_cnames = [ to_cname2from_cnames[cname] for cname in key_cnames ]
            if self.verbose: print("filling fkey: %s : %s -> %s : %s" % (fkey.constraint_name, key_from_cnames, pk_tname, key_cnames))
            for row in payload:
                pk_table_rows = pk_tables_key2rows[pk_tname][key_cnames]
                #print("pk_table_rows: %s" % (pk_table_rows))
                key = tuple([ row[cname] for cname in key_from_cnames ])
                #print("key : %s <- %s" % (key, key_from_cnames))
                # In case of optional fkey, the key column could have null value. In this case, don't fill in RID value
                if None in key: continue
                if key not in pk_table_rows.keys():
                    raise Exception("DATA ERROR: reference table: %s, do not contain columns: %s with reference values: %s" % (pk_tname, key_cnames, str(key)))
                else:
                    row[rid_cname] = pk_table_rows[key]["RID"]
            
        if self.verbose: print("sample payload: %s" % (json.dumps(payload[0:2], indent=4)))

        """
        Empty the tname table of those with structure_id
        """
        try:
            constraints="structure_id=%s&Entry_Related_File=%s" % (structure_id, related_file_rid)
            existing_rows = get_ermrest_query(self.catalog, "PDB", tname, constraints=constraints)
            #print("%s existing_rows[%d]: %s" % (tname, len(existing_rows), existing_rows))
            
            content_change = True
            if content_change:
                if self.verbose: print("!! %s content has changed: will delete existing data with constraint: %s" % (tname, constraints))
                delete_table_rows(self.catalog, "PDB", tname, constraints=constraints)
                self.logger.debug('Deleted rows from PDB:%s with constraints=%s' % (tname, constraints))
            else:
                if self.verbose: print("%s content does not change" % (tname))
        except Exception as e:
            raise ErmrestError("Unable to read or cleanup table %s" % (tname))
        
        """
        Insert updated payload to table
        """
        try:
            inserted = insert_if_not_exist(catalog, "PDB", tname, payload)
            if self.verbose: print("%s inserted(%d)[0:2]: %s" % (tname, len(inserted), inserted[0:2]))
            pass
        except Exception as e:
            raise ErmrestUpdateError("Fail to insert CSV file to table %s (%s)" % (tname, e))
        
             
    """
    Get the the entry child table foreign keys and update the entry_id/structure_id column with the entry id.
    If the FK is missing, add it.
    """
    def x_getRecordUpdatedWithFK(self, tname, row, entry_id):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog {}'.format(self.catalog_id)]['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            if tname == 'struct' and col == 'structure_id':
                continue
            row[col] = entry_id
                
        return row

    """
    Get the record with the foreign key updated to the entry id.
    """
    def getUpdatedRecord(self, tname, row, entry_id, table, inserted_records, fk_tables):
        with open('{}'.format(self.entry), 'r') as f:
            pdb = json.load(f)
        referenced_by = pdb['Catalog {}'.format(self.catalog_id)]['schemas']['PDB']['tables']['entry']['referenced_by']
        columns = []
        for k,v in referenced_by.items():
            if v['table'] == tname:
                col = v['columns'][1:-1]
                columns.append(col)
        for col in columns:
            if tname == 'struct' and col == 'structure_id':
                continue
            if col in row.keys():
                row[col] = entry_id
        
        """
        Set the missing defaults
        """
        if tname in self.mmCIF_defaults.keys():
            for col in self.mmCIF_defaults[tname]:
                if col not in row.keys():
                    row[col] = '.'
        """
        Set the ucode values
        """
        if tname in self.vocab_ucode.keys():
            for col in self.vocab_ucode[tname]:
                if col in row.keys():
                    row[col] = row[col].upper()
        
        """
        Set the values for the *_RID columns
        """
        if tname in fk_tables:
            entry = self.combo1_columns[tname]
            for col,value in entry.items():
                for pk_table,mappings in value.items():
                    rid_found = False
                    for inserted_record in inserted_records:
                        if inserted_record['name'] == pk_table:
                            for pk_row in inserted_record['rows']:
                                found = True
                                for fk_col, pk_col in mappings.items():
                                    if row[fk_col] != pk_row[pk_col]:
                                        found = False
                                        break
                                if found == True:
                                    row[col] = pk_row['RID']
                                    rid_found = True
                                    break
                            if rid_found == False:
                                self.logger.debug('Could not find a RID value for {} column in the {} table'.format(col, tname))
                                break
                    if rid_found == False:
                        break
        return row

    """
    Get the record for the ihm_entity_poly_segment table updated with values for the Entity_Poly_Seq_RID_Begin and Entity_Poly_Seq_RID_End columns.
    """
    def getUpdatedEntityPolySegment(self, row):
        structure_id = row['structure_id']
        entity_id = row['entity_id']
        comp_id_begin = row ['comp_id_begin']
        comp_id_end = row ['comp_id_end']
        seq_id_begin = row['seq_id_begin']
        seq_id_end = row['seq_id_end']
        
        url = '/attribute/PDB:entity_poly_seq/structure_id={}&entity_id={}&mon_id={}&num={}/RID'.format(urlquote(structure_id), urlquote(entity_id), urlquote(comp_id_begin), seq_id_begin)
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row['Entity_Poly_Seq_RID_Begin'] = resp.json()[0]['RID']

        url = '/attribute/PDB:entity_poly_seq/structure_id={}&entity_id={}&mon_id={}&num={}/RID'.format(urlquote(structure_id), urlquote(entity_id), urlquote(comp_id_end), seq_id_end)
        self.logger.debug('Query URL: "%s"' % url) 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        row['Entity_Poly_Seq_RID_End'] = resp.json()[0]['RID']

        return row

    """
    Update the record for the optional composite FK
    """
    def getUpdatedOptional(self, optional_fks, tname, row, entry_id):
        if tname in optional_fks:
            for fk in optional_fks[tname]:
                url_structure_pattern = fk['url_structure_pattern']
                url_pattern = fk['url_pattern']
                fk_RID_column_name = fk['fk_RID_column_name']
                fk_other_column_name = fk['fk_other_column_name']
                ref_table = fk['ref_table']
                ref_other_column_name = fk['ref_other_column_name']
                """
                if fk_RID_column_name not in row.keys():
                    continue
                """
                if fk_other_column_name not in row.keys():
                    continue
                fk_other_value = row[fk_other_column_name]
                if type(fk_other_value).__name__ == 'str':
                    fk_other_value = urlquote(fk_other_value)
                url = url_structure_pattern.format(urlquote(ref_table), urlquote(ref_other_column_name), fk_other_value, entry_id)
                self.logger.debug('Query URL with structure_id for OPTIONAL FK: "%s"' % url) 
                resp = self.catalog.get(url)
                resp.raise_for_status()
                if len(resp.json()) > 0:
                    row[fk_RID_column_name] = resp.json()[0]['RID']
                    continue
                url = url_pattern.format(urlquote(ref_table), urlquote(ref_other_column_name), fk_other_value)
                self.logger.debug('Query URL for OPTIONAL FK: "%s"' % url) 
                resp = self.catalog.get(url)
                resp.raise_for_status()
                if len(resp.json()) > 0:
                    row[fk_RID_column_name] = resp.json()[0]['RID']

        return row

    """
    Get the output.cif file from make_mmcif and put it in self.scratch directory
     > python3 -m ihm.util.make_mmcif <input> <output>
    HT TODO: replace output.cif with <rid>_output.cif
    """
    def getOutputCIF(self, rid, file_url, filename, user):
        try:
            """
            Cleanup the self.make_mmCIF directory 
            """
            for entry in os.scandir(self.make_mmCIF):
                if entry.is_file() and entry.path.endswith('.cif'):
                    os.remove(entry.path)
                    self.logger.debug('Removed file {}'.format(entry.path))
                    
            """
            Get the file from hatrac
            """
            hatracFile = '{}/{}'.format(self.make_mmCIF, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(self.make_mmCIF))
            
            """
            Prepend the RID to the input file
            """
            shutil.move('{}/{}'.format(self.make_mmCIF, filename), '{}/{}_{}'.format(self.make_mmCIF, rid, filename))
            filename = '{}_{}'.format(rid, filename)
            output_cif = '%s_output.cif' % (rid)

            """
            Apply make_mmcif.py
            """
            args = [self.python_bin, '-m', 'ihm.util.make_mmcif', filename, output_cif]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.make_mmCIF)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not make mmCIF for entry RID = "%s" and file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (rid, filename, stdoutdata, stderrdata)) 
                subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
                self.sendMail(subject, 'Can not make mmCIF for entry RID = "%s" and file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (rid, filename, stdoutdata, stderrdata))
                os.remove('{}/{}'.format(self.make_mmCIF, filename))
                self.export_error_message = 'ERROR getOutputCIF: Can not make mmCIF.\nstdoutdata: %s\nstderrdata: %s\n' % (stdoutdata, stderrdata)
                return None
            
            os.remove('{}/{}'.format(self.make_mmCIF, filename))

            """
            Move the output.cif file to the scratch directory
            """
            shutil.move('%s/%s' % (self.make_mmCIF, output_cif), '%s/' % (self.scratch))
            self.logger.debug('getOutputCIF: File %s/%s was moved to the %s directory' % (self.make_mmCIF, output_cif, self.scratch))
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            os.chdir(currentDirectory)
            self.export_error_message = 'ERROR getOutputCIF: "%s"' % str(ev)
            return None
            
        return '%s/%s' % (self.scratch, output_cif)
            
    """
    Get the accession serial value
    """
    def getNextAccessionSerial(self, rid, user):
        try:
            url = '/entity/PDB:Accession_Code/Entry::null::@sort(Accession_Serial)?limit=1'
            resp = self.catalog.get(url)
            resp.raise_for_status()
            if len(resp.json()) == 1:
                row = resp.json()[0]
                row['Entry'] = rid
                self.updateAttributes('PDB', 'Accession_Code', row['RID'], ['Entry'], row, user)
                self.logger.debug('SUCCEEDED updated the table Accession_Code with Entry "%s".' % (rid))
                accession_serial_value = row['Accession_Code']
                return (accession_serial_value, None)
            else:
                error_message = f'No accession codes available'
                self.logger.error(error_message)
                subject = '{} {}: {} ({})'.format(row['RID'], 'SUBMISSION COMPLETE', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'], user)
                self.sendMail(subject, error_message)
                return (None, error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.export_error_message = 'ERROR getNextAccessionSerial: "%s"' % str(ev)
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'], user)
            self.sendMail(subject, 'RID: %s\n%s\n' % (rid, ''.join(traceback.format_exception(et, ev, tb))))
            return (None, str(ev))
        
    """
    Get the accession code value
    """
    def getAccessionCode(self, row, user):
        try:
            #value = 'PDBDEV_' + ('00000000' + str(row['Accession_Serial']))[-8:]
            """
            Check if we have already an Accession Code
            """
            url = '/entity/PDB:Accession_Code/Entry={}'.format(row['RID'])
            resp = self.catalog.get(
                url
            )
            resp.raise_for_status()
            rows = resp.json()
            if len(rows) == 1:
                row = rows[0]
                return (row['Accession_Code'], None)

            value, error_message = self.getNextAccessionSerial(row['RID'], user)
            self.logger.debug('Accession Code = {}'.format(value))
            return (value, error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(row['RID'], 'SUBMISSION COMPLETE', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            return (None, str(ev))
        
    """
    Store the validation error file into hatrac
    """
    def storeFileInHatrac(self, hatrac_namespace, file_name, file_path, rid, user):
        try:
            newFile = '{}/{}'.format(file_path, file_name)
            file_size = os.path.getsize(newFile)
            hashes = hu.compute_file_hashes(newFile, hashes=['md5', 'sha256'])
            new_md5 = hashes['md5'][1]
            new_sha256 = hashes['sha256'][1]
            hexa_md5 = hashes['md5'][0]
            new_uri = '{}/{}'.format(hatrac_namespace, urlquote(file_name))
            chunked = True if file_size > DEFAULT_CHUNK_SIZE else False
            
            """
            Store the log file in hatrac if they are not already
            """
            hatrac_URI = None
            try:
                outfile = '{}.hatrac'.format(newFile)
                r = self.store.get_obj(new_uri, destfilename=outfile)
                hatrac_URI = r.headers['Content-Location']
                hashes = hu.compute_file_hashes(outfile, hashes=['md5', 'sha256'])
                old_hexa_md5 = hashes['md5'][0]
                os.remove(outfile)
            except:
                old_hexa_md5 = None
            
            if hatrac_URI != None and hexa_md5 == old_hexa_md5:
                self.logger.info('Skipping the upload of the file "%s" as it already exists hatrac.' % file_name)
            else:
                if mimetypes.inited == False:
                    mimetypes.init()
                content_type,encoding = mimetypes.guess_type(newFile)
                if content_type == None:
                    content_type = 'application/octet-stream'
                try:
                    hatrac_URI = self.store.put_loc(new_uri,
                                                         newFile,
                                                         headers={'Content-Type': content_type},
                                                         content_disposition = "filename*=UTF-8''%s" % urlquote(file_name),
                                                         md5 = new_md5,
                                                         sha256 = new_sha256,
                                                         content_type = content_type,
                                                         chunked = chunked
                                                       )
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('Can not upload file "%s" in hatrac "%s". Error: "%s"' % (file_name, new_uri, str(ev)))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
                    self.sendMail(subject, 'RID={}, Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(rid, file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
                    return (None, None, None, None)
            return (hatrac_URI, file_name, file_size, hexa_md5)

        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
            self.sendMail(subject, 'RID={}, Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(rid, file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
            return (None, None, None, None)

    """
    Cleanup the entry file tables
    """
    def cleanupEntryFileTables(self, entry_id, rid, user):
        try:
            url = '/entity/PDB:Entry_Generated_File/Structure_Id={}'.format(urlquote(entry_id))
            self.logger.debug('Query URL: "%s"' % url) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            if len(resp.json()) > 0:
                resp = self.catalog.delete(
                    url
                )
                resp.raise_for_status()
                self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
                
            url = '/entity/PDB:Entry_Error_File/Entry_RID={}'.format(urlquote(rid))
            self.logger.debug('Query URL: "%s"' % url) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            if len(resp.json()) > 0:
                resp = self.catalog.delete(
                    url
                )
                resp.raise_for_status()
                self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
            return 0
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'], user)
            self.sendMail(subject, 'RID={}, Can not cleanup the entry file tables:\n{}'.format(rid, ''.join(traceback.format_exception(et, ev, tb))))
            return 1
            
    """
    Validate the exported mmCIF file
    """
    def validateExportmmCIF(self, input_dir, filename, year, entry_id, rid, user, process_status_error, user_id):
        try:
            if self.cleanupEntryFileTables(entry_id, rid, user) != 0:
                self.cleanupDataScratch()
                return (1, 'Can not cleanup the entry file tables')

            entry_RCB = self.getUserRCB('PDB', 'entry', rid)
            currentDirectory=os.getcwd()
            os.chdir('{}'.format(input_dir))
            args = [self.CifCheck, '-f', '{}/{}'.format(input_dir, filename), '-dictSdb', self.dictSdb]
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), input_dir)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.debug('Can not CifCheck for file "{}".\nstdoutdata: {}\nstderrdata: {}\n'.format(filename, stdoutdata.decode('utf-8'), stderrdata.decode('utf-8')))
                subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                self.sendMail(subject, 'RID={}, Can not execute CifCheck for file "{}".\nstdoutdata: {}\nstderrdata: {}\n'.format(rid, filename, stdoutdata.decode('utf-8'), stderrdata.decode('utf-8')))
                self.cleanupDataScratch()
                return (1, stderrdata.decode('utf-8'))
            has_errors = False
            try:
                hatrac_namespace = '/{}/generated/uid/{}/entry/id/{}/validation_error'.format(self.hatrac_namespace, user_id, entry_id)
                log_file_name = '{}-diag.log'.format(filename)
                log_file_path = '{}/{}-diag.log'.format(input_dir, filename)
                fr = open(log_file_path, 'r')
                has_errors = True
                fr.close()
                hatrac_URI, file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, log_file_name, input_dir, rid, user)
                if hatrac_URI == None:
                    self.cleanupDataScratch()
                    return (1, 'Can not store file {} in hatrac'.format(log_file_name))
                self.logger.debug('Insert a row in the Entry_Error_File table')
                row = {'File_URL' : hatrac_URI,
                       'File_Name': file_name,
                       'File_Bytes': file_size,
                       'File_MD5': hexa_md5,
                       'File_Type': 'Log: CifCheck diagnostic error file',
                       'Entry_RID': rid,
                       }
                if self.createEntity('PDB:Entry_Error_File', row, rid, user) == None:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': process_status_error,
                                          'Record_Status_Detail': 'Error in createEntity(Entry_Error_File)',
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    self.cleanupDataScratch()
                    return (1, 'Error in createEntity(Entry_Error_File)')
            except:
                pass
            try:
                log_file_name = '{}-parser.log'.format(filename)
                log_file_path = '{}/{}-parser.log'.format(input_dir, filename)
                fr = open(log_file_path, 'r')
                has_errors = True
                fr.close()
                hatrac_URI, file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, log_file_name, input_dir, rid, user)
                if hatrac_URI == None:
                    self.cleanupDataScratch()
                    return (1, 'Can not store file {} in hatrac'.format(log_file_name))
                self.logger.debug('Insert a row in the Entry_Error_File table')
                row = {'File_URL' : hatrac_URI,
                       'File_Name': file_name,
                       'File_Bytes': file_size,
                       'File_MD5': hexa_md5,
                       'File_Type': 'Log: CifCheck parser error file',
                       'Entry_RID': rid,
                       }
                if self.createEntity('PDB:Entry_Error_File', row, rid, user) == None:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': process_status_error,
                                          'Record_Status_Detail': 'Error in createEntity(Entry_Error_File)',
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    self.cleanupDataScratch()
                    return (1, 'Error in createEntity(Entry_Error_File)')
            except:
                pass
            
            
            if has_errors == False:
                hatrac_namespace = '/{}/generated/uid/{}/entry/id/{}/final_mmCIF'.format(self.hatrac_namespace, user_id, entry_id)
            else:
                shutil.move('{}/{}'.format(self.scratch, filename), '{}/{}_error.cif'.format(self.scratch, filename))
                filename = '{}_error.cif'.format(filename)

            hatrac_URI, file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, filename, input_dir, rid, user)
            if hatrac_URI == None:
                raise RuntimeError(f'Can not store file {filename} in hatrac')
            if has_errors == False:
                self.logger.debug('Insert a row in the Entry_Generated_File table')
                row = {'File_URL' : hatrac_URI,
                       'File_Name': file_name,
                       'File_Bytes': file_size,
                       'File_MD5': hexa_md5,
                       'Structure_Id': entry_id,
                       'Entry_RCB': entry_RCB,
                       'File_Type': 'mmCIF',
                       'mmCIF_Schema_Version': urlquote(self.mmCIF_Schema_Version)
                       }
                if self.createEntity('PDB:Entry_Generated_File', row, rid, user) == None:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': process_status_error,
                                          'Record_Status_Detail': 'Error in createEntity(Entry_Generated_File)',
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    self.cleanupDataScratch()
                    return (1, 'Error in createEntity(Entry_Generated_File)')
                self.cleanupDataScratch()
                return (0, None)
            else:
                self.logger.debug('Insert a row in the Entry_Error_File table')
                row = {'File_URL' : hatrac_URI,
                       'File_Name': file_name,
                       'File_Bytes': file_size,
                       'File_MD5': hexa_md5,
                       'File_Type': 'mmCIF',
                       'Entry_RID': rid,
                       }
                if self.createEntity('PDB:Entry_Error_File', row, rid, user) == None:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': process_status_error,
                                          'Record_Status_Detail': 'Error in createEntity(Entry_Error_File)',
                                          'Workflow_Status': 'ERROR',
                                          },
                                          user)
                    self.cleanupDataScratch()
                    return (1, 'Error in createEntity(Entry_Error_File)')
                self.logger.debug('Update error in export_mmCIF()')
                self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': process_status_error,
                                      'Workflow_Status': 'ERROR',
                                      'Record_Status_Detail': 'mmCIF Validation Failure. For details, see the files:'
                                      },
                                      user)
                self.cleanupDataScratch()
                return (1, 'mmCIF Validation Failure. For details, see the files at: https://{}/chaise/recordset/#{}/PDB:Entry_Error_File/Entry_RID={}'.format(self.host, self.catalog_id, rid))
                
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            self.cleanupDataScratch()
            return (1, 'ERROR mmCIF Validation: "%s"' % str(ev))
        
    """
    Cleanup the scratch directory.
    """
    def cleanupDataScratch(self):
        for file_name in os.listdir(self.scratch):
            file_path = '{}/{}'.format(self.scratch, file_name)
            if os.path.isfile(file_path):
                self.logger.debug('Removing file "{}"\n'.format(file_path))
                os.remove(file_path)
            elif os.path.isdir(file_path):
                self.logger.debug('Removing directory "{}"\n'.format(file_path))
                shutil.rmtree(file_path)
        
    """
    Cleanup the singularity directory.
    """
    def cleanupSingularityDir(self, dir_path):
        for file_name in os.listdir(dir_path):
            file_path = f'{dir_path}/{file_name}'
            if os.path.isfile(file_path):
                self.logger.debug(f'Removing file "{file_path}"\n')
                os.remove(file_path)
            elif os.path.isdir(file_path):
                self.logger.debug(f'Removing directory "{file_path}"\n')
                shutil.rmtree(file_path)
        
    """
    Add a table that is not specified in the initial json schema (json-full-db-ihm_dev_full-col-ihm_dev_fulljson file).
    """
    def addTable(self, rid, results, table_name, columns, fw):
        def getColumnValue(table_name, column_name, column_type, column_value):
            if column_value == None:
                return '.'
            if column_type in ['int4', 'float4']:
                return '{}'.format(column_value)
            if column_type == 'text':
                if '\t' in column_value:
                    self.logger.debug('tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    self.export_error_message = 'ERROR getColumnValue: tab character in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                    return None
                if '\n' in column_value:
                    return '\n;{}\n;\n'.format(column_value)
                if '"' not in column_value and "“" not in column_value and "”" not in column_value and "'" not in column_value and ' ' not in column_value and not column_value.startswith('_'):
                    return column_value
                if '"' not in column_value and "“" not in column_value and "”" not in column_value:
                    return '"{}"'.format(column_value)
                if "'" not in column_value:
                    return "'{}'".format(column_value)
                else:
                    self.logger.debug('Both " and \' are in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value))
                    return '\n;{}\n;\n'.format(column_value)
                self.export_error_message = 'ERROR getColumnValue: Unhandled value in table: {}, column: {}, value: {}'.format(table_name, column_name, column_value)
                return None
            else:
                self.logger.debug('unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name))
                self.export_error_message = 'ERROR getColumnValue: unknown type: {}, table: {}, column: {}'.format(column_type, table_name, column_name)
                return None

        """
        Get the RCB user
        """
        user = self.getUser('PDB', 'entry', rid)

        if len(results) > 1:
            fw.write('loop_\n')
            for column in columns:
                fw.write('_{}.{}\n'.format(table_name,column['name']))
            for row in results:
                line = []
                for column in columns:
                    column_name = column['name']
                    column_type = column['type']
                    column_value = row[column_name]
                    value = getColumnValue(table_name, column_name, column_type, column_value)
                    if value == None:
                        self.logger.debug('Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                        subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                        self.sendMail(subject, 'Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                        self.export_error_message = 'ERROR exportData: Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value)
                        return 1
                    line.append(value)
                fw.write('{}\n'.format('\t'.join(line)))
        elif len(results) == 1:
            row = results[0]
            for column in columns:
                column_name = column['name']
                column_type = column['type']
                column_value = row[column_name]
                value = getColumnValue(table_name, column_name, column_type, column_value)
                if value == None:
                    self.logger.debug('Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                    subject = '{} {}: {} ({})'.format(rid, 'SUBMIT', process_status_error, user)
                    self.sendMail(subject, 'Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value))
                    self.export_error_message = 'ERROR exportData: Could not find column value for ({}, {}, {}, {})'.format(table_name, column_name, column_type, column_value)
                    return 1
                fw.write('_{}.{}\t{}\n'.format(table_name, column['name'], value))
            
        fw.write('#\n')    
        return 0

    def addCollectionRecords(self, rid, entry_id, fw, hold):
        """
        Get the RCB user
        """
        user = self.getUser('PDB', 'entry', rid)

        try:
            url = '/attribute/A:=PDB:entry/id={}/B:=PDB:ihm_entry_collection_mapping/C:=PDB:ihm_entry_collection/C:id,C:name,C:details'.format(urlquote(entry_id))
            self.logger.debug('ihm_entry_collection Query URL: "%s"' % url) 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            results = resp.json()
            if len(results) > 0:
                table_name = 'ihm_entry_collection'
                columns = [
                    {'name': 'id', 'type': 'text'},
                    {'name': 'name', 'type': 'text'},
                    {'name': 'details', 'type': 'text'}
                    ]
                if self.addTable(rid, results, table_name, columns, fw) != 0:
                    error_message = 'ERROR addCollectionRecords: "%s"' % self.export_error_message
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                          'Record_Status_Detail': error_message,
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    return False

            url = '/attribute/A:=PDB:entry/id={}/B:=PDB:ihm_entry_collection_mapping/C:=PDB:ihm_entry_collection/B:collection_id,entry_id:=A:Accession_Code'.format(urlquote(entry_id))
            self.logger.debug('ihm_entry_collection_mapping Query URL: "%s"' % url) 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            results = resp.json()
            if len(results) > 0:
                table_name = 'ihm_entry_collection_mapping'
                columns = [
                    {'name': 'collection_id', 'type': 'text'},
                    {'name': 'entry_id', 'type': 'text'}
                    ]
                if self.addTable(rid, results, table_name, columns, fw) != 0:
                    error_message = 'ERROR addCollectionRecords: "%s"' % self.export_error_message
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                          'Record_Status_Detail': error_message,
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    return False
            
            return True
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR addCollectionRecords: "%s"' % str(ev)
            self.updateAttributes('PDB',
                                  'entry',
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return False

    """
    Execute report validation.
    """
    def report_validation(self, rid, entry_id, user, user_id, hold):
        try:
            """
            Get the System Generated mmCIF File
            """
            entry_RCB = self.getUserRCB('PDB', 'entry', rid)
            url = f'/entity/PDB:entry/RID={rid}/PDB:Entry_Generated_File/File_Type=mmCIF'
            self.logger.debug(f'Query URL: {url}') 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            row = resp.json()[0]
            filename = row['File_Name']
            file_url = row['File_URL']
            file_path,error_message = self.getHatracFile(filename, file_url, self.scratch, rid, user)
            if file_path == None:
                self.logger.error(f'Can not get the mmCIF Entry_Generated_File RID={row["File_Name"]} for entry RID={rid}')
                subject = f'{rid} Scientific Validation Error ({user})'
                self.sendMail(subject, f'Can not get the mmCIF Entry_Generated_File RID={row["File_Name"]} for entry RID={rid}')
                return(None, None, None)
    
            self.cleanupSingularityDir(f'{self.validation_dir}/input')
            self.cleanupSingularityDir(f'{self.validation_dir}/output')
            self.cleanupSingularityDir(f'{self.validation_dir}/cache')
            shutil.copy2(file_path, f'{self.validation_dir}/input')
            
            filename = os.path.basename(file_path)
            currentDirectory=os.getcwd()
            os.chdir(f'{self.validation_dir}')

            args = ['singularity', 
                    'exec', '--pid',
                    '--bind', 'IHMValidation/:/opt/IHMValidation,/usr/local/lib/python3.8/dist-packages/ihm/:/opt/conda/lib/python3.10/site-packages/ihm/,input:/ihmv/input,cache:/ihmv/cache,output:/ihmv/output', 
                    self.singularity_sif, #'ihmv_20231222.sif', 
                    '/opt/IHMValidation/ihm_validation/ihm_validator.py',
                    '-f', f'/ihmv/input/{filename}', 
                    '--force',
                    '--output-root', '/ihmv/output', 
                    '--cache-root', '/ihmv/cache'
                    ]
            self.logger.debug(f'Running "{" ".join(args)}" from the {self.validation_dir} directory') 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            try:
                stdoutdata, stderrdata = p.communicate(timeout=self.timeout*60)
                returncode = p.returncode
                os.chdir(currentDirectory)
                if returncode != 0:
                    self.logger.debug(f'ERROR.\nstdoutdata: {stdoutdata}\nstderrdata: {stderrdata}\n') 
                    subject = '{} {}: {} ({})'.format(rid, 'REPORT VALIDATION', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
                    error_message = f'ERROR IN REPORT VALIDATION.\nstdoutdata: {stdoutdata}\nstderrdata: {stderrdata}\n'
                    self.sendMail(subject, error_message)
                    self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                      'Record_Status_Detail': error_message,
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                    return(None, None, None)
                self.logger.debug('SUCCESS in executing the scientific validation')
                output_files = []
                filename, _ext = os.path.splitext(os.path.basename(file_path))
                output_path = f'{self.validation_dir}/output/{filename}'
                hatrac_namespace = f'/{self.hatrac_namespace}/generated/uid/{user_id}/entry/id/{entry_id}/validation_report/'
                for file_name in os.listdir(output_path):
                    if file_name.endswith('_full_validation.pdf'):
                        file_type = 'Validation: Full PDF'
                    elif file_name.endswith('_summary_validation.pdf'):
                        file_type = 'Validation: Summary PDF'
                    elif file_name.endswith('_html.tar.gz'):
                        file_type = 'Validation: HTML tar.gz'
                    else:
                        self.logger.debug(f'Unknown file type got from the scientific validation: {file_name}')
                        subject = f'{rid} Scientific Validation Error ({user})'
                        self.sendMail(subject, f'Unknown file type got from the scientific validation: {file_name}')
                        return(None, None, None)
                    output_file_path = f'{output_path}/{file_name}'
                    if os.path.isfile(output_file_path):
                        output_files.append(output_file_path)
                        hatrac_URI, singular_file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, file_name, output_path, rid, user)
                        if hatrac_URI != None:
                            self.logger.debug('Insert a row in the Entry_Generated_File table')
                            row = {'File_URL' : hatrac_URI,
                                   'File_Name': singular_file_name,
                                   'File_Bytes': file_size,
                                   'File_MD5': hexa_md5,
                                   'Structure_Id': entry_id,
                                   'Entry_RCB': entry_RCB,
                                   'File_Type': file_type
                                   }
                            if self.createEntity('PDB:Entry_Generated_File', row, rid, user) == None:
                                self.updateAttributes('PDB',
                                                      'entry',
                                                      rid,
                                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                                      {'RID': rid,
                                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                                      'Record_Status_Detail': 'Error in createEntity(Entry_Generated_File)',
                                                      'Workflow_Status': 'ERROR'
                                                      },
                                                      user)
                                return(None, None, None) 
                        else:
                            return(None, None, None) 
                            
                return tuple(output_files)
            except TimeoutExpired:
                et, ev, tb = sys.exc_info()
                self.logger.error('got TimeoutExpired exception "%s"' % str(ev))
                self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                subject = '{} {}: {} ({})'.format(rid, 'REPORT VALIDATION TimeoutExpired', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
                error_message = '%s\n' % ''.join(traceback.format_exception(et, ev, tb))
                self.sendMail(subject, error_message)
                self.updateAttributes('PDB',
                                  'entry',
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
                p.kill()
                os.chdir(currentDirectory)
                return(None, None, None)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            return(None, None, None)

    """
    Execute JSON mmCIF content.
    1. download .cif file from hatrac
    2. prepare py_rcsb_db for processing (remove .cif, .json, and prepare yml config)
    3. run rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py
    4. renaming .json file
    5. upload to hatrac and update ermrest
    """
    def generate_JSON_mmCIF_content(self, rid, entry_id, user, user_id, hold):
        try:
            """
            Get the System Generated mmCIF File
            """
            entry_RCB = self.getUserRCB('PDB', 'entry', rid)
            url = f'/entity/PDB:entry/RID={rid}/PDB:Entry_Generated_File/File_Type=mmCIF'
            self.logger.debug(f'Query URL: {url}') 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            row = resp.json()[0]
            filename = row['File_Name']
            file_url = row['File_URL']
            file_path,error_message = self.getHatracFile(filename, file_url, self.scratch, rid, user)
            if file_path == None:
                self.logger.error(f'Can not get the mmCIF Entry_Generated_File RID={row["File_Name"]} for entry RID={rid}')
                subject = f'{rid} JSON mmCIF Content Error ({user})'
                self.sendMail(subject, f'Can not get the mmCIF Entry_Generated_File RID={row["File_Name"]} for entry RID={rid}')
                return None
    
            """
            Cleanup the rcsb/db/tests-validate/test-output/ihm-files and rcsb/db/tests-validate/test-output directories 
            """
            fpath = '{}/rcsb/db/tests-validate/test-output/ihm-files'.format(self.py_rcsb_db)
            for entry in os.scandir(fpath):
                if entry.is_file() and entry.path.endswith('.cif'):
                    os.remove(entry.path)
                    self.logger.debug('Removed file {}'.format(entry.path))
            
            fpath = '{}/rcsb/db/tests-validate/test-output'.format(self.py_rcsb_db)
            for entry in os.scandir(fpath):
                if entry.is_file() and entry.path.endswith('.json'):
                    os.remove(entry.path)
                    self.logger.debug('Removed file {}'.format(entry.path))

            """
            Move the System Generated mmCIF File file to the rcsb/db/tests-validate/test-output/ihm-files directory and apply testSchemaDataPrepValidate-ihm.py
            """
            shutil.move(file_path, f'{self.py_rcsb_db}/rcsb/db/tests-validate/test-output/ihm-files/')
            self.logger.debug(f'File {file_path} was moved to the {self.py_rcsb_db}/rcsb/db/tests-validate/test-output/ihm-files directory') 
            currentDirectory=os.getcwd()
            os.chdir(f'{self.py_rcsb_db}')
            shutil.copy2(f'{self.py_rcsb_db}/rcsb/db/config/exdb-config-example-ihm-HOLD-REL.yml', f'{self.py_rcsb_db}/rcsb/db/config/exdb-config-example-ihm.yml')
            args = ['env', f'PYTHONPATH={self.py_rcsb_db}', self.python_bin, 'rcsb/db/tests-validate/testSchemaDataPrepValidate-ihm.py']
            self.logger.debug('Running "{}" from the {} directory'.format(' '.join(args), self.py_rcsb_db)) 
            p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            stdoutdata, stderrdata = p.communicate()
            returncode = p.returncode
            os.chdir(currentDirectory)
            
            if returncode != 0:
                self.logger.error('Can not validate testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (file_path, stdoutdata, stderrdata)) 
                subject = '{} {}: {} ({})'.format(rid, 'HOLD/REL', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
                self.sendMail(subject, 'Can not make testSchemaDataPrepValidate-ihm for file "%s".\nstdoutdata: %s\nstderrdata: %s\n' % (file_path, stdoutdata, stderrdata))
                os.remove('{}/rcsb/db/tests-validate/test-output/ihm-files/%s'.format(self.py_rcsb_db, filename))
                error_message = 'ERROR convert2json: {}'.format(stderrdata)
                return (returncode,error_message)
            
            os.remove(f'{self.py_rcsb_db}/rcsb/db/tests-validate/test-output/ihm-files/{filename}')
            self.logger.debug(f'File {self.py_rcsb_db}/rcsb/db/tests-validate/test-output/ihm-files/{filename} was removed') 
            
            """
            Get the JSON file of the mmCIF file 
            """
            fpath = f'{self.py_rcsb_db}/rcsb/db/tests-validate/test-output'
            cif_filename,_ext = os.path.splitext(os.path.basename(filename))

            for entry in os.scandir(fpath):
                if entry.is_file() and entry.path.endswith('.json'):
                        shutil.move(entry.path, f'{fpath}/{cif_filename}.json')
                        break
            
            hatrac_namespace = f'/{self.hatrac_namespace}/generated/uid/{user_id}/entry/id/{entry_id}/final_mmCIF'
            hatrac_URI, json_file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, f'{cif_filename}.json', f'{fpath}', rid, user)
            if hatrac_URI != None:
                self.logger.debug('Insert a row in the Entry_Generated_File table')
                row = {'File_URL' : hatrac_URI,
                       'File_Name': json_file_name,
                       'File_Bytes': file_size,
                       'File_MD5': hexa_md5,
                       'Structure_Id': entry_id,
                       'Entry_RCB': entry_RCB,
                       'File_Type': 'JSON: mmCIF content'
                       }
                if self.createEntity('PDB:Entry_Generated_File', row, rid, user) == None:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                          {'RID': rid,
                                          'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                          'Record_Status_Detail': 'Error in createEntity(Entry_Generated_File)',
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    return None 
            else:
                return None 
            """
            Remove the JSON files that were created
            """
            for entry in os.scandir(fpath):
                    if entry.is_file() and entry.path.endswith('.json'):
                        os.remove(entry.path)
                        self.logger.debug('Removed file {}'.format(entry.path))

            os.chdir(currentDirectory)
            return f'{cif_filename}.json'
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            return None 

    def get_primary_accession_code(self, mode, accesion_code_row):
        """
        Get the primary accession code
        """
        return accesion_code_row['PDBDEV_Accession_Code'] if mode == 'PDBDEV' else accesion_code_row['PDB_Accession_Code']

    def get_lower_accession_code(self, value):
        """
        Change to lower case
        If prefix is TEST- do not change the prefix
        """
        if value.startswith('TEST-'):
            value = f'TEST-{value[len("TEST-"):].lower()}'
        else:
           value = value.lower()
        return value

    def database_2_accession_code(self, record, database_2_primary_accession_code, database_2_alternative_accession_code):
        if database_2_primary_accession_code != None:
            record = f'{record}{database_2_primary_accession_code}\n'
        if database_2_alternative_accession_code != None:
            record = f'{record}{database_2_alternative_accession_code}\n'
        record = f'{record}#\n'
        return record

    def get_database_2_string(self, mode, accesion_code_row):
        """
        Get the primary accession code
        """
        
        return '#' if mode == 'None' \
            else None if mode == 'PDBDEV' and accesion_code_row["PDBDEV_Accession_Code"] == None \
            else f'PDB-Dev {accesion_code_row["PDBDEV_Accession_Code"]} {self.get_primary_accession_code("PDBDEV", accesion_code_row)} ?' if mode == 'PDBDEV' \
            else f'PDB {accesion_code_row["PDB_Accession_Code"]} {self.get_lower_accession_code(accesion_code_row["PDB_Extended_Code"])} 10.2210/pdb{self.get_lower_accession_code(accesion_code_row["PDB_Code"])}/pdb'

    def addReleaseRecords(self, rid, hold=False, user_id=None):
        """
        Get the RCB user
        """
        user = self.getUser('PDB', 'entry', rid)
        
        user_id = self.getUserId('PDB', 'entry', rid)

        try:
            if self.export_mmCIF('PDB', 'entry', rid, release=not hold, user_id=user_id) != 0:
                """
                We can not recreate the mmCIF exported file
                """
                return

            """
            Query for detecting the mmCIF file
            """
            url = '/entity/PDB:entry/RID={}/PDB:Entry_Generated_File/File_Type=mmCIF'.format(urlquote(rid))
            self.logger.debug('Query URL: "%s"' % url) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            if len(rows) != 1:
                self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Workflow_Status"],
                                      {'RID': rid,
                                      'Record_Status_Detail': 'ERROR addReleaseRecords: Invalid number of mmCIF files: {}'.format(len(rows)),
                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                self.logger.debug('Invalid number of mmCIF files: {}'.format(len(rows))) 
                subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
                self.sendMail(subject, 'Invalid number of mmCIF files: {}'.format(len(rows)))
                return
            row = rows[0]
            file_url = row['File_URL']
            filename = row['File_Name']
            mmCIF_File_rid = row['RID']
            f,error_message = self.getHatracFile(filename, file_url, self.scratch, rid, user)
            if f == None:
                self.updateAttributes(schema,
                                      table,
                                      rid,
                                      ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                      'Record_Status_Detail': error_message,
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                return
            """
            Query for detecting the entry record
            """
            url = '/entity/PDB:entry/RID={}'.format(urlquote(rid))
            self.logger.debug('Query URL: "%s"' % url) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            row = resp.json()[0]
            creation_time = row['RCT']
            year = parse(creation_time).strftime("%Y")
            input_dir = self.scratch
            deposition_date = row['Deposit_Date']
            #deposition_date = parse(row['RCT']).strftime("%Y-%m-%d")
                
            entry_id = row['id']
            
            """
            Get the Accession_Code record
            """
            accesion_code_row = None
            url = '/entity/PDB:Accession_Code/Entry={}'.format(rid)
            resp = self.catalog.get(
                url
            )
            resp.raise_for_status()
            accesion_code_rows = resp.json()
            if len(accesion_code_rows) == 1:
                accesion_code_row = accesion_code_rows[0]
                if accesion_code_row['Accession_Code'] not in [self.get_primary_accession_code(self.primary_accession_code_mode, accesion_code_row)]:
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Process_Status", "Workflow_Status", "Record_Status_Detail"],
                                          {'RID': rid,
                                          'Record_Status_Detail': f'ERROR addReleaseRecords: primary_accession_code {self.get_primary_accession_code(self.primary_accession_code_mode, accesion_code_row)} different from Accession_Code column: {accesion_code_row["Accession_Code"]}',
                                          'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                          'Workflow_Status': 'ERROR'
                                          },
                                          user)
                    self.logger.debug(f'Entry RID={rid} ERROR addReleaseRecords: primary_accession_code {self.get_primary_accession_code(self.primary_accession_code_mode, accesion_code_row)} different from Accession_Code column: {accesion_code_row["Accession_Code"]}') 
                    subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
                    self.sendMail(subject, f'Entry RID={rid} ERROR addReleaseRecords: primary_accession_code {self.get_primary_accession_code(self.primary_accession_code_mode, accesion_code_row)} different from Accession_Code column: {accesion_code_row["Accession_Code"]}')
                    return
            else:
                self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Workflow_Status", "Record_Status_Detail"],
                                      {'RID': rid,
                                      'Record_Status_Detail': 'ERROR addReleaseRecords: Invalid number of mmCIF files: {}'.format(len(rows)),
                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                self.logger.debug(f'Entry RID={rid} has no accession code') 
                subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
                self.sendMail(subject, f'Entry RID={rid} has no accession code')
                return
            database_2_primary_accession_code = self.get_database_2_string(self.primary_accession_code_mode, accesion_code_row)
            database_2_alternative_accession_code = self.get_database_2_string(self.alternative_accession_code_mode, accesion_code_row)
            if hold==True:
                record_status = 'HOLD'
                records_release = mmCIF_hold_records.replace('<status_code>', record_status) \
                    .replace('<entry_id>', row['Accession_Code']) \
                    .replace('<deposition_date>', deposition_date)
                records_release = self.database_2_accession_code(records_release, database_2_primary_accession_code, database_2_alternative_accession_code)
            else:
                record_status = 'REL'
                revision_date = row['Release_Date']
                if revision_date == None:
                    # revision_date = parse(str(datetime.now())).strftime("%Y-%m-%d")  
                    revision_date = self.get_release_datetime_utc(isoformat=False).strftime("%Y-%m-%d")  
                    self.updateAttributes('PDB',
                                          'entry',
                                          rid,
                                          ["Release_Date"],
                                          {'RID': rid,
                                          'Release_Date': revision_date
                                          },
                                          user)
                records_release = mmCIF_release_records.replace('<status_code>', record_status) \
                    .replace('<entry_id>', row['Accession_Code']) \
                    .replace('<deposition_date>', deposition_date) \
                    .replace('<revision_date>', revision_date)
                records_release = self.database_2_accession_code(records_release, database_2_primary_accession_code, database_2_alternative_accession_code)
            file_name = '{}.cif'.format(row['Accession_Code'])
            fr = open('{}/{}'.format(input_dir, filename), 'r')
            fw = open('{}/{}'.format(input_dir, file_name), 'w')
            audit_conform = False
            
            while True:
                line = fr.readline()
                if not line:
                    break
                if line.startswith('data_'):
                    fw.write('data_{}\n'.format(row['Accession_Code']))
                elif line.startswith('_entry.id'):
                    fw.write('_entry.id  {}\n'.format(row['Accession_Code']))
                elif line.startswith('_struct.'):
                    if line.startswith('_struct.entry_id'):
                        line = '_struct.entry_id\t{}\n'.format(row['Accession_Code'])
                    fw.write(line)
                elif line.startswith('_audit_conform'):
                    audit_conform = True
                    fw.write(line)
                elif audit_conform == True:
                    fw.write(line)
                    if line == '#\n':
                        audit_conform = False
                        fw.write(records_release)
                        if self.addCollectionRecords(rid, entry_id, fw, hold) == False:
                            fr.close()
                            fw.close()
                            return
                else:
                    fw.write(line)
            fr.close()
            fw.close()
            hatrac_namespace = '/{}/generated/uid/{}/entry/id/{}/final_mmCIF'.format(self.hatrac_namespace, user_id, entry_id)
            hatrac_URI, file_name, file_size, hexa_md5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir, rid, user)
            if hatrac_URI == None:
                self.cleanupDataScratch()
                raise RuntimeError(f'Can not store file {file_name} in hatrac')
            self.updateAttributes('PDB',
                                  'Entry_Generated_File',
                                  mmCIF_File_rid,
                                  ["File_URL", "File_Name", "File_MD5", "File_Bytes"],
                                  {'RID': mmCIF_File_rid,
                                  'File_URL': hatrac_URI,
                                  'File_Name': file_name,
                                  'File_MD5': hexa_md5,
                                  'File_Bytes': file_size
                                  },
                                  user)
            process_status_error = Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY']
            returncode,error_message = self.validateExportmmCIF(input_dir, file_name, year, entry_id, rid, user, process_status_error, user_id)
            if returncode != 0:
                subject = '{} {}: {} ({})'.format(rid, 'HOLD' if hold else 'REL', process_status_error, user)
                self.sendMail(subject, error_message)
                return
            else:
                """
                Generate the Conform_Dictionary entries
                """
                returncode = self.generateConformDictionary ('PDB', 'entry', entry_id, process_status_error, rid, user)
                
                if returncode != 0:
                    return
            
            if self.reportValidation:
                if self.report_validation(rid, entry_id, user, user_id, hold) != (None, None, None):
                    if self.generate_JSON_mmCIF_content(rid, entry_id, user, user_id, hold) != None:
                        self.updateAttributes('PDB',
                                              'entry',
                                              rid,
                                              ["Process_Status", "Workflow_Status"],
                                              {'RID': rid,
                                              'Process_Status': Process_Status_Terms['SUCCESS'],
                                              'Workflow_Status': 'REL' if hold==False else 'HOLD'
                                              },
                                              user)
            else:
                self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Workflow_Status"],
                                      {'RID': rid,
                                      'Process_Status': Process_Status_Terms['SUCCESS'],
                                      'Workflow_Status': 'REL' if hold==False else 'HOLD'
                                      },
                                      user)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE' if hold else 'RELEASE READY', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR addReleaseRecords: "%s"' % str(ev)
            self.updateAttributes('PDB',
                                  'entry',
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'] if hold else Process_Status_Terms['ERROR_RELEASING_ENTRY'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            
    def set_accession_code(self, rid):
        """
        Get the RCB user
        """
        user = self.getUser('PDB', 'entry', rid)
        
        try:
            """
            Query for detecting the record to be processed
            """
            url = '/entity/PDB:entry/RID={}'.format(urlquote(rid))
            self.logger.debug('Query URL: "%s"' % url) 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            row = resp.json()[0]
            accession_code, error_message = self.getAccessionCode(row, user)
            if accession_code == None:
                self.updateAttributes('PDB',
                                      'entry',
                                      rid,
                                      ["Process_Status", "Accession_Code", "Workflow_Status", "Record_Status_Detail"],
                                      {'RID': rid,
                                      'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'],
                                      'Accession_Code': None,
                                      'Record_Status_Detail': error_message,
                                      'Workflow_Status': 'ERROR'
                                      },
                                      user)
                return

            """
            if self.is_catalog_dev == True:
                subject = '{}: {} ({})'.format(rid, row['Process_Status'], user)
                self.sendMail(subject, 'The Process Status of the entry with RID={} was changed to "{}".'.format(rid, row['Process_Status']), receivers=self.email_config['curators'])
            """

            self.updateAttributes('PDB',
                                  'entry',
                                  rid,
                                  ["Accession_Code"],
                                  {'RID': rid,
                                  'Accession_Code': accession_code
                                  },
                                  user)
            self.addReleaseRecords(rid, hold=True, user_id=None)
            self.logger.debug('Ended PDB Processing to set the accession code for the PDB:entry table with RID="{}".'.format(rid)) 
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'SUBMISSION COMPLETE', Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR set_accession_code: "%s"' % str(ev)
            self.updateAttributes('PDB',
                                  entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_GENERATING_SYSTEM_FILES'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
        
    def clear_entry(self, rid, id, user):
        try:
            """
            Get the references of the "entry" table 
            """
            references = []
            delete_tables = []
            cols = []
            model_root = self.catalog.getCatalogModel()
            schema = model_root.schemas['PDB']
            table = schema.tables['entry']
            for referenced_by in table.referenced_by:
                pk_table_name = referenced_by.table.name
                for foreign_column in referenced_by.foreign_key_columns:
                    col = foreign_column.name
                    references.append({pk_table_name: col})
                    if col not in cols:
                        cols.append(col)
            
            self.logger.debug('References columns of the PDB:entry table:\n{}"'.format(json.dumps(cols, indent=4))) 
            
            """
            Get the referenced that need to be deleted
            """
            for reference in references:
                for k,v in reference.items():
                    if v == 'Entry_RID':
                        val = rid
                    else:
                        val = id
                    url = '/entity/PDB:{}/{}={}'.format(k, v, val)
                    resp = catalog_ermrest.get(url)
                    resp.raise_for_status()
                    if len(resp.json()) > 0:
                        delete_tables.append(url)
            
            """
            Delete the records referenced by the entry table
            """
            for url in delete_tables:
                resp = catalog.get(url)
                resp.raise_for_status()
                if len(resp.json()) > 0:
                    resp = self.catalog.delete(
                        url
                    )
                    resp.raise_for_status()
                    self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
            return 0
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = '{} {}: {} ({})'.format(rid, 'DEPO', Process_Status_Terms['ERROR_PROCESSING_UPLOADED_mmCIF_FILE'], user)
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR clear_entry: "%s"' % str(ev)
            self.updateAttributes('PDB',
                                  entry,
                                  rid,
                                  ["Process_Status", "Record_Status_Detail", "Workflow_Status"],
                                  {'RID': rid,
                                  'Process_Status': Process_Status_Terms['ERROR_GENERATING_mmCIF_FILE'],
                                  'Record_Status_Detail': error_message,
                                  'Workflow_Status': 'ERROR'
                                  },
                                  user)
            return 1
        

#!/usr/bin/python3
# 
# Copyright 2017 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Worker for archiving mmCIF files.
"""

import os
import stat
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
import gzip
from email.mime.text import MIMEText
import socket
from socket import gaierror, EAI_AGAIN

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote
from deriva.core.utils import hash_utils as hu
from deriva.core.utils.core_utils import DEFAULT_CHUNK_SIZE, NotModified
from deriva.utils.extras.data import get_key2rows, get_ermrest_query

import re
import math
import mimetypes
from datetime import datetime as dt, timedelta, timezone
import pytz

mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'

"""
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/Vocab:System_Generated_File_Type/!Archive_Category::null::/Archive_Category,Name
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/A:=PDB:entry/Workflow_Status=REL/B:=PDB:Entry_Generated_File/File_Type=mmCIF/C:=left(A:RID)=(PDB:Entry_Latest_Archive:Entry)/Entry::null::;!B:File_URL=mmCIF_URL/$A/RID
"""

class ArchiveClient (object):
    # HT: initialize with ermrest query
    submission_date = None
    previous_submission_date = None

    system_generated_file_types = {}
    entry_latest_archive = {}
    new_releases = {}
    re_releases = {}
    entry_generated_files = {}
    released_entries = {}
    hold_entries = {}    
    entry_id2rid = {}
    entry_archive_insert_rids = []
    entry_archive_update_rids = []
    
    """
    Network client for archiving mmCIF files.
    """
    def __init__(self, kwargs):
        self.scheme = 'https'
        self.host = kwargs.get("hostname")
        self.catalog_id = kwargs.get("catalog_id")
        self.archive_parent = kwargs.get("archive_parent")
        self.released_entry_dir = kwargs.get("released_entry_dir")
        self.holding_dir = kwargs.get("holding_dir")
        self.data_scratch = kwargs.get("data_scratch")
        self.credentials = kwargs.get("credentials")
        self.hatrac_namespace = kwargs.get("hatrac_namespace")
        self.holding_namespace = kwargs.get("holding_namespace")
        self.released_structures = {}
        self.current_holdings = {}
        self.archive_category_dir_names = {}
        self.PDB_Archive_RID = None
        self.store = HatracStore(
            self.scheme, 
            self.host,
            self.credentials
        )
        self.catalog = PollingErmrestCatalog(
            self.scheme, 
            self.host,
            self.catalog_id,
            self.credentials
        )
        self.catalog.dcctx['cid'] = 'pipeline/archive'
        self.email = kwargs.get("email")
        self.cutoff_time_pacific = kwargs.get('cutoff_time_pacific')
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')
        
        self.submission_date = self.getArchiveDate()
        self.previous_submission_date = self.getPreviousArchiveDate(self.submission_date) 
        
        self.set_entry_latest_archive()
        self.set_new_releases()
        self.set_re_releases()
        entry_rids = list(self.new_releases.keys()) + list(self.re_releases.keys()) 
        self.set_released_entries(entry_rids)
        self.set_system_generated_file_types()
        self.set_entry_generated_files(entry_rids)
        self.set_entry_archive_lists()

    def set_entry_latest_archive(self):
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Latest_Archive", None)
        for row in rows:
            self.entry_latest_archive[row["RID"]] = row
        
    def set_new_releases(self):
        url1 = f'/attribute/' + \
            f'E:=PDB:entry/Workflow_Status=REL/' + \
            f'F:=(id)=(PDB:Entry_Generated_File:Structure_Id)/File_Type=mmCIF/' + \
            f'A:=left(E:RID)=(PDB:Entry_Latest_Archive:Entry)/A:RID::null::;(A:RCT::gt::{urlquote(self.previous_submission_date)}&A:Submission_Time={urlquote(self.submission_date)})/' + \
            f'$E/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_URL,A:Entry,A:Submission_Time,A:mmCIF_URL'
            
        self.logger.debug(f"Query for entries that haven't been archived: {url1}") 
        resp = self.catalog.get(url1)
        resp.raise_for_status()
        rows = resp.json()
        for row in rows:
            self.new_releases[row["RID"]] = row

    def set_re_releases(self):
        url2 = f'/attribute/' + \
            f'A:=PDB:Entry_Latest_Archive/A:RCT::leq::{urlquote(self.previous_submission_date)}/' + \
            f'E:=(A:Entry)=(PDB:entry:RID)/Workflow_Status=REL/' + \
            f'F:=left(A:mmCIF_URL)=(PDB:Entry_Generated_File:File_URL)/F:RID::null::;(F:File_Type=mmCIF&A:Submission_Time={urlquote(self.submission_date)})/' + \
            f'$A/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_URL,A:Entry,A:Submission_Time,A:mmCIF_URL'
        
        self.logger.debug(f"Query for entries that entries that mmCIF contents have changed: {url2}") 
        resp = self.catalog.get(url2)
        resp.raise_for_status()
        rows = resp.json()
        for row in rows:
            self.re_releases[row["RID"]] = row

    """
      REL entries
    """
    def set_released_entries(self, entry_rids):
        """
        constraints = "RID=ANY(%s)" %  ",".join([ urlquote(v) for v in entry_rids ])
        rows = get_ermrest_query(self.catalog, "PDB", "entry", constraints, attributes=["id","Accession_Code"])
        for row in rows:
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
        """
        for row in list(self.new_releases.values()) + list(self.re_releases.values()):
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
            
    """
    Information about system generated file types
    """
    def set_system_generated_file_types(self):
        constraints = "A:=Vocab:Archive_Category/$M"
        attributes = ["File_Type:=M:Name","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "Vocab", "System_Generated_File_Type", constraints, attributes=attributes)
        for row in rows:
            self.system_generated_file_types[row["File_Type"]] = row

    """
    query based on released RIDs
      self.entry_generated_files : {
        <RID>: {"mmCIF": [....], "Validation: Full PDF": [...], ...   }
      }
    """
    def set_entry_generated_files(self, entry_rids):
        constraints = "Structure_Id=ANY(%s)/T:=Vocab:System_Generated_File_Type/A:=Vocab:Archive_Category/$M" % ",".join([ urlquote(self.released_entries[rid]["id"]) for rid  in entry_rids ])
        attributes = ["M:Structure_Id","M:File_Type","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Generated_File", constraints)        
        for row in rows:
            rid = self.entry_id2rid[row["Structure_Id"]]
            file_type = row["File_Type"]
            #row["Archive_Category"] = self.system_generated_file_types[file_type]["Archive_Category"]
            #row["Directory_Name"] = self.system_generated_file_types[file_type]["Directory_Name"]
            self.entry_generated_files.setdefault(rid, {})
            self.entry_generated_files[rid]["File_Type"] = row

    def set_entry_archive_lists(self):
        for rid in self.new_releases.keys():
            if rid not in self.entry_latest_archive.keys():
                self.entry_archive_insert_rids.append(rid)
            else:
                self.entry_archive_update_rids.append(rid)
        self.entry_archive_update_rids.extend(self.re_releases.keys())

    
    def set_current_holdings(self):
        curent_holdings = self.entry_latest_archive.copy()
        for row in self.new_releases.values() + self.re_releases.values():
            rid = row["RID"]
            submitted_files = {}
            for file_type, generated_file in self.entry_generated_files[rid]:
                submitted_files.update(generated_file["Directory_Name"], [])
                submitted_files[generated_file["Directory_Name"]].append(generated_file["File_URL"])
            current_holdings[rid] = {
                "Entry" : rid,
                "Submission_Time" : self.submission_date,
                "mmCIF_URL" : self.entry_generated_files[rid]["mmCIF"]["File_URL"],
                "Submitted_Files": submitted_files,
                "Archive" : self.Archive_RID,
                "Submission_History": None,
            }
            if rid in self.re_releases.keys():
                current_holdings[rid]["Submission_History"].update({
                    self.submission_date : {
                        "mmCIF_URL": current_holdings["mmCIF_URL"],
                        "Submission_Files": current_holdings["Submission_Files"]
                    }
                })
        # generate manifest from current holdings
        #...
        
    """
    Print the configuration
    """
    def printConfiguration(self):
        config = {
            'host': self.host,
            'catalog_id': self.catalog_id,
            'archive_parent': self.archive_parent,
            'released_entry_dir': self.released_entry_dir,
            'holding_dir': self.holding_dir,
            'data_scratch': self.data_scratch,
            'credentials': self.credentials,
            'hatrac_namespace': self.hatrac_namespace,
            'holding_namespace': self.holding_namespace,
            'email': self.email
            }
        print(json.dumps(config, indent=4))
        return 0

    """
    Send email notification
    """
    def sendMail(self, subject, text):
        if self.email['server'] and self.email['sender']:
            if self.host in ['dev.pdb-dev.org', 'dev-aws.pdb-dev.org']:
                subject = 'DEV {}'.format(subject)
            retry = 0
            ready = False
            receivers = self.email['receivers']
            while not ready:
                try:
                    msg = MIMEText('%s\n\n%s' % (text, self.email['footer']), 'plain')
                    msg['Subject'] = subject
                    msg['From'] = self.email['sender']
                    msg['To'] = receivers
                    s = smtplib.SMTP_SSL(self.email['server'], self.email['port'])
                    s.login(self.email['user'], self.email['password'])
                    s.sendmail(self.email['sender'], receivers.split(','), msg.as_string())
                    s.quit()
                    self.logger.debug(f'Sent email notification to {receivers}.')
                    ready = True
                except socket.gaierror as e:
                    if e.errno == socket.EAI_AGAIN:
                        time.sleep(100)
                        retry = retry + 1
                        ready = retry > 10
                    else:
                        ready = True
                    if ready:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    ready = True

    """
    Rollback the database depending on exceptions or book keeping?: HT: TODO
    1. Delete PDB.Archive row if the row was inserted
    2. Delete Entry_Latest_Archive with RIDs from self.entry_archive_insert 
    3. Update modified Entry_Latest_Archive with the original version from self.entry_latest_archive
    """
    def rollbackArchive(self):
        try:
            """
            Delete the self.PDB_Archive_RID record
            """
            if self.PDB_Archive_RID != None:
                url = f'/entity/PDB:PDB_Archive/RID={urlquote(self.PDB_Archive_RID)}'
                resp = self.catalog.delete(
                    url
                )
                resp.raise_for_status()
                self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 

                # Need another flag to indicate that insertion has been performed. Then delete using RID from self.entry_archive_insert
                # HT: The filter should be "Archive"
                url = f'/entity/PDB:Entry_Latest_Archive/Archive={urlquote(self.PDB_Archive_RID)}'
                try:
                    resp = self.catalog.delete(
                        url
                    )
                    resp.raise_for_status()
                    self.logger.debug('SUCCEEDED deleted the rows for the URL "%s".' % (url)) 
                except:
                    pass

                # Need another flag to indicate that update has been performed. Then perform an update to restore the previous state.
                # I am not sure whether this will ever happen if update is the last thing we do.
                # ...

        except:
            # HT: TODO: If failed to roll back, we should output the self.entry_archive_insert and self.entry_archive_update into a file.
            # 
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))

    """
    Process Archiving Files 
    """
    def processArchive(self):
        try:
            """
            Get the the file types with Archive_Category
            HT: Move to init
            """
            url = f'/attribute/Vocab:System_Generated_File_Type/!Archive_Category::null::/Archive_Category,Name'
            self.logger.debug(f'Query for the the file types with Archive_Category: "{url}"') 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            self.archive_category = {}
            for row in rows:
                self.archive_category[row['Name']] = row['Archive_Category']
                
            """
            Get the archive directories and the associated file types
            HT: Move to init            
            """
            self.archive_file_types = []
            for k,v in self.archive_category.items():
                if k not in self.archive_file_types:
                   self.archive_file_types.append(k) 

            """
            Get the archive directories of the Archive_Category
            HT: Move to init            
            """
            url = f'/attribute/Vocab:Archive_Category/Directory_Name,Name'
            self.logger.debug(f'Query for the directories names of the Archive_Category: "{url}"') 
            
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            self.archive_category_dir_names = {}
            for row in rows:
                self.archive_category_dir_names[row['Name']] = row['Directory_Name']
                
            """
            Get the archive directories and the associated file types
            """
            self.archive_dirs = []
            self.holding_map = {}
            for k,v in self.archive_category_dir_names.items():
                self.archive_dirs.append(v)
                self.holding_map[v] = k

            """
            Archive files
            """
            return_code = self.archiveFiles()
            if return_code != 0:
                self.rollbackArchive()
            return return_code
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))
            self.rollbackArchive()
           
            return 1

    """
    Archive files
    HT:
      - Prepare directory of files
      - Create manifest files
      - insert/Update PDB:PDB_Archive
      - Update PDB:Entry_Latest_Archive -> update "Archive" field with the PDB:Archive RID before performing updates
    """
    def archiveFiles(self):
        """
        Get the new entries to be released:
         - not released
         - or modified
        """
        rel_warnings = []
        self.hold_warnings = []
        
        """
        Clean up the release and holding directories
        """
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.released_entry_dir}')
        except:
            pass
        
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.holding_dir}')
        except:
            pass

        """
        Create the parent directories
        """
        os.makedirs(f'{self.archive_parent}/{self.released_entry_dir}', exist_ok=True)
        os.makedirs(f'{self.archive_parent}/{self.holding_dir}', exist_ok=True)

        """
        Get the current and previous submission date
        HT: Move to its own function and init
        """
        self.logger.debug(f'Submission Dates: {self.submission_date}, {self.previous_submission_date}') 
        url = f'/aggregate/A:=PDB:PDB_Archive/Submission_Time::lt::{urlquote(self.submission_date)}/previous_submission_time:=max(Submission_Time)'
        self.logger.debug(f"Query to find the maximum submission time:\n\nhttps://{self.host}/ermrest/catalog/{self.catalog_id}{url}\n") 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        self.logger.debug(f'Previous submission time\n\n{json.dumps(rows, indent=4)}\n')
        if len(rows) > 0 and rows[0]['previous_submission_time'] != None:
            self.previous_submission_date = rows[0]['previous_submission_time']
        
        """
        Create the new entry in the PDB_Archive table if it does not exist
        """
        url = f'/attribute/A:=PDB:PDB_Archive/Submission_Time={urlquote(self.submission_date)}/A:RID'
        self.logger.debug(f"Query to see if the archive has been run this week: {url}") 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        if len(rows) > 0:
            self.PDB_Archive_RID = rows[0]['RID']
        else:
            insert_row = {'Submission_Time': self.submission_date
                          }
            default_columns = [
                'Submitted_Entries',
                'Current_File_Holdings_Name',
                'Current_File_Holdings_URL',
                'Current_File_Holdings_Bytes',
                'Current_File_Holdings_MD5',
                'Released_Structures_LMD_Name',
                'Released_Structures_LMD_URL',
                'Released_Structures_LMD_Bytes',
                'Released_Structures_LMD_MD5',
                'Unreleased_Entries_Name',
                'Unreleased_Entries_URL',
                'Unreleased_Entries_Bytes',
                'Unreleased_Entries_MD5'
            ]
            res = self.insert_rows([insert_row], 'PDB_Archive',defaults=','.join(default_columns))
            self.PDB_Archive_RID = res['RID']
        
        """
        Get the entries that haven't been archived
        """
        url1 = f'/attribute/' + \
            f'E:=PDB:entry/Workflow_Status=REL/' + \
            f'F:=(id)=(PDB:Entry_Generated_File:Structure_Id)/File_Type=mmCIF/' + \
            f'A:=left(E:RID)=(PDB:Entry_Latest_Archive:Entry)/A:RID::null::;(A:RCT::gt::{urlquote(self.previous_submission_date)}&A:Submission_Time={urlquote(self.submission_date)})/' + \
            f'$E/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_URL,A:Entry,A:Submission_Time,A:mmCIF_URL'
            
        self.logger.debug(f"Query for entries that haven't been archived: {url1}") 
        resp = self.catalog.get(url1)
        resp.raise_for_status()
        rows = resp.json()
        unarchived_entries = []
        unarchived_entries_rids = []
        for row in rows:
            if row['RID'] not in unarchived_entries_rids:
                unarchived_entries_rids.append(row['RID'])
                unarchived_entries.append(row)
            else:
                subject = f'PDB-Dev {row["RID"]} Duplicate record to archive'
                self.sendMail(subject, f'Duplicate record to archive: {row["RID"]} for URL:\n{url1}')
                return 1
        
        self.new_released_entries = len(rows)
        
        """
        Get the entries that mmCIF contents have changed
        """
        url2 = f'/attribute/' + \
        f'A:=PDB:Entry_Latest_Archive/A:RCT::leq::{urlquote(self.previous_submission_date)}/' + \
        f'E:=(A:Entry)=(PDB:entry:RID)/Workflow_Status=REL/' + \
        f'F:=left(A:mmCIF_URL)=(PDB:Entry_Generated_File:File_URL)/F:RID::null::;(F:File_Type=mmCIF&A:Submission_Time={urlquote(self.submission_date)})/' + \
        f'$A/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_URL,A:Entry,A:Submission_Time,A:mmCIF_URL'
    
        self.logger.debug(f"Query for entries that entries that mmCIF contents have changed: {url2}") 
        resp = self.catalog.get(url2)
        resp.raise_for_status()
        rows = resp.json()
        re_released_entries_rids = []
        for row in rows:
            if row['RID'] not in unarchived_entries_rids:
                unarchived_entries_rids.append(row['RID'])
                unarchived_entries.append(row)
                re_released_entries_rids.append(row['RID'])
                self.logger.debug(f'appended {row["RID"]}')
            else:
                subject = f'PDB-Dev {row["RID"]} Duplicate record to archive'
                self.sendMail(subject, f'Duplicate record to archive: {row["RID"]} for URL(s):\n{url1}\nand/or:\n{url2}')
                return 1
        
        self.re_released_entries = len(rows)
        self.logger.debug(f'unarchived_entries: {json.dumps(unarchived_entries, indent=4)}')

        inserted_rows = []
        updated_rows = []
        self.no_validation_rids = []
        self.released_records = []
        
        rows = unarchived_entries

        """
        Get the new released files
        """
        for row in rows:
            structure_id = row['id']
            rid = row['RID']
            """
            Get the associated Accession_Code record
            """
            url = f'/entity/PDB:entry/RID={urlquote(rid)}/PDB:Accession_Code'
            self.logger.debug(f'Query for the associated Accession_Code: "{url}"') 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            accesion_code_rows = resp.json()
            if len(accesion_code_rows) != 1:
                subject = f'PDB-Dev {rid} Can not get the accession code record.'
                self.sendMail(subject, f'Found {len(accession_codes)} records.')
                return 1
            
            accesion_code_row = accesion_code_rows[0]
            hash = self.get_hash(accesion_code_row)
            entry_id = self.get_entry_id(accesion_code_row)
            
            self.released_structures[entry_id] = {}
                
            """
            Get the Entry_Generated_File
            """
            url = f'/attribute/PDB:entry/RID={urlquote(rid)}/PDB:Entry_Generated_File/File_Name,File_URL,File_MD5,File_Type,RMT'
            self.logger.debug(f'Query for the associated Entry_Generated_File: "{url}"') 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            files_generated = resp.json()
            if len(files_generated) < 3:
                """
                We are in the REL status but the report validation was not run
                """
                self.no_validation_rids.append({"Accession_Code": row['Accession_Code'], "Deposit_Date": row['Deposit_Date']})
                #continue
            
            submitted_files = {}

            for file_generated in files_generated:
                file_type = file_generated['File_Type']
                if file_type not in self.archive_file_types:
                    continue
                filename = file_generated['File_Name']
                file_url = file_generated['File_URL']
                
                if file_type == 'mmCIF':
                    if filename != f'{accesion_code_row["Accession_Code"]}.cif':
                        rel_warnings.append(rid)
                    else:
                        self.released_structures[entry_id]['File_URL'] = file_generated['File_URL']
                        self.released_records.append(accesion_code_row)
                
                if rid not in rel_warnings:
                    """
                    Extract the file from hatrac
                    """
                    file_path,error_message = self.getHatracFile(filename, file_url, self.data_scratch)
                    
                    manifest_key = self.get_manifest_key(accesion_code_row)
                    if manifest_key not in self.current_holdings.keys():
                        self.current_holdings[manifest_key] = {}
                        for archive_dir in self.archive_dirs:
                            submitted_files[archive_dir] = []
                            self.current_holdings[manifest_key][self.holding_map[archive_dir]] = []
    
                    renamed_file = filename.lower()
                    #submitted_files[self.archive_category_dir_names[self.archive_category[file_type]]].append(f'{renamed_file}.gz')
                    submitted_files[self.archive_category_dir_names[self.archive_category[file_type]]].append(file_url)
                    
                    """
                    Zip the file
                    """
                    self.generateReleasedZip(filename, hash, entry_id, file_type, self.get_manifest_key(accesion_code_row))

            url = f'/attribute/PDB:entry/RID={urlquote(rid)}/PDB:Entry_Latest_Archive/RID,mmCIF_URL,Submission_Time,Submitted_Files,Submission_History'
            self.logger.debug(f'Query for detecting if the record exists or not in the Entry_Latest_Archive table: "{url}"') 
            resp = self.catalog.get(url)
            resp.raise_for_status()
            latest_archive_record = resp.json()
            
            """
            Insert or update the record in the Entry_Latest_Archive table
            """    
            if len(latest_archive_record) == 0:
                """
                New entry
                """
                if rid not in rel_warnings:
                    inserted_rows.append(
                        {
                            'Entry': rid,
                            'mmCIF_URL': self.released_structures[entry_id]['File_URL'],
                            'Submission_Time': self.submission_date,
                            'Archive': self.PDB_Archive_RID,
                            'Submitted_Files': submitted_files
                        }
                )
            elif len(latest_archive_record) == 1:
                """
                Entry that was updated
                """
                if rid not in rel_warnings:
                    if latest_archive_record[0]['mmCIF_URL'] != self.released_structures[entry_id]['File_URL']:
                        submission_history = latest_archive_record[0]['Submission_History']
                        if submission_history == None:
                            submission_history = {}
                        submission_history.update({
                          latest_archive_record[0]['Submission_Time']: {
                            "mmCIF_URL": latest_archive_record[0]['mmCIF_URL'], 
                            "Submitted_Files": latest_archive_record[0]['Submitted_Files']
                          }
                        })
                        updated_rows.append(
                            {
                                'mmCIF_URL': self.released_structures[entry_id]['File_URL'],
                                'Submission_Time': self.submission_date,
                                'Archive': self.PDB_Archive_RID,
                                'Submitted_Files': submitted_files,
                                'Submission_History': submission_history,
                                'RID': latest_archive_record[0]['RID']
                            }
                )
        
        columns = [
            'mmCIF_URL',
            'Submission_Time',
            'Archive',
            'Submitted_Files',
            'Submission_History'
            ]
        self.insert_or_update_rows(inserted_rows, updated_rows, 'Entry_Latest_Archive', columns)
        
        """
        Generate the manifest files
        """
        self.writeManifestFiles()
        self.generate_unreleased_entries()
         
        """
        Store in hatrac the manifest files
        """
        submission_datetime = dt.strptime(self.submission_date, '%Y-%m-%d %H:%M:%S%z')
        year = submission_datetime.year
        submission_date = f'{submission_datetime.date()}'
        hatrac_namespace = f'/{self.hatrac_namespace}/{self.holding_namespace}/{year}/{submission_date}'
        input_dir = f'{self.archive_parent}/{self.getHoldingSubDirectory()}'
        file_name = 'current_file_holdings.json.gz'
        
        Current_File_Holdings_URL, Current_File_Holdings_Name, Current_File_Holdings_Bytes, Current_File_Holdings_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Current_File_Holdings_URL == None:
            return 1
 
        file_name = 'released_structures_last_modified_dates.json.gz'
        
        Released_Structures_LMD_URL, Released_Structures_LMD_Name, Released_Structures_LMD_Bytes, Released_Structures_LMD_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Released_Structures_LMD_URL == None:
            return 1
 
        file_name = 'unreleased_entries.json.gz'
        
        Unreleased_Entries_URL, Unreleased_Entries_Name, Unreleased_Entries_Bytes, Unreleased_Entries_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Unreleased_Entries_URL == None:
            return 1
 
        """
        Update the PDB_Archive table
        """
        columns = [
            'Submitted_Entries',
            'New_Released_Entries',
            'Re_Released_Entries',
            'Current_File_Holdings_Name',
            'Current_File_Holdings_URL',
            'Current_File_Holdings_Bytes',
            'Current_File_Holdings_MD5',
            'Released_Structures_LMD_Name',
            'Released_Structures_LMD_URL',
            'Released_Structures_LMD_Bytes',
            'Released_Structures_LMD_MD5',
            'Unreleased_Entries_Name',
            'Unreleased_Entries_URL',
            'Unreleased_Entries_Bytes',
            'Unreleased_Entries_MD5'
        ]
        rows = [
            {
            'RID': self.PDB_Archive_RID,
            'Submitted_Entries': self.new_released_entries + self.re_released_entries,
            'New_Released_Entries': self.new_released_entries,
            'Re_Released_Entries': self.re_released_entries,
            'Current_File_Holdings_Name': Current_File_Holdings_Name,
            'Current_File_Holdings_URL': Current_File_Holdings_URL,
            'Current_File_Holdings_Bytes': Current_File_Holdings_Bytes,
            'Current_File_Holdings_MD5': Current_File_Holdings_MD5,
            'Released_Structures_LMD_Name': Released_Structures_LMD_Name,
            'Released_Structures_LMD_URL': Released_Structures_LMD_URL,
            'Released_Structures_LMD_Bytes': Released_Structures_LMD_Bytes,
            'Released_Structures_LMD_MD5': Released_Structures_LMD_MD5,
            'Unreleased_Entries_Name': Unreleased_Entries_Name,
            'Unreleased_Entries_URL': Unreleased_Entries_URL,
            'Unreleased_Entries_Bytes': Unreleased_Entries_Bytes,
            'Unreleased_Entries_MD5': Unreleased_Entries_MD5
            }
        ]
        self.update_rows(columns, rows, 'PDB_Archive')
        
        url = '/attribute/A:=PDB:entry/Workflow_Status=HOLD/B:=left(A:id)=(PDB:Entry_Generated_File:Structure_Id)/Structure_Id::null::/$A/RID'
        self.logger.debug(f'Query for unreleased entries w/o a system mmCIF generated file: "{url}"') 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        for row in rows:
            if row['RID'] not in self.hold_warnings:
                self.hold_warnings.append(row['RID'])
        
        total_warning = len(self.hold_warnings) + len(rel_warnings)
        if total_warning > 0:
            subject_rids = []
            for rel_warning in rel_warnings:
                subject_rids.append(rel_warning)
                if len(subject_rids) >= 3:
                    break
            if len(subject_rids) < 3:
                for hold_warning in self.hold_warnings:
                    subject_rids.append(hold_warning)
                    if len(subject_rids) >= 3:
                        break
                    
            subject = f'PDB-Dev weekly archive warnings: {", ".join(subject_rids)}'
            if total_warning > 3:
                subject= f'{subject}, ...'
                rids = []
                for rel_warning in rel_warnings:
                    rids.append(f'{rel_warning}: REL') 
                for hold_warning in self.hold_warnings:
                    rids.append(f'{hold_warning}: HOLD') 
            newline_char = "\n"
            rids = f'{newline_char.join(rids)}'
            message_body = f'The following entries were not archived due to inconsistent filenames or missing system generated files:\n\n{rids}'
            self.sendMail(subject, message_body)

        return 0

    """
    Get UTC Submission Time 
    """
    def getSubmissionTimeUTC(self, submission_str):
        submission_obj = dt.fromisoformat(submission_str)
        #submission_utc = submission_obj.replace(hour=21,tzinfo=timezone.utc).isoformat()
        submission_utc = submission_obj.astimezone(timezone.utc).isoformat()
        return f'{submission_utc}'

    """
    Get the corresponding Zip file of the hatrac filename  
    """
    def getSubmittedZipFile(self, filename):
        if 'hatrac' not in filename:
            return filename
        parts = filename.split('/')
        name = f'{parts[-1].split(":")[0].lower()}.gz'
        ret='/'.join(parts[0:7]+[name])
        return ret

    """
    Write manifest 
    """
    def writeManifestFiles(self):
        url = '/attribute/A:=PDB:Entry_Latest_Archive/PDB:entry/B:=PDB:Accession_Code/$A/Entry,Submitted_Files,Submission_Time,B:Accession_Code'
        self.logger.debug(f'Query for writing the manifest files: "{url}"') 
        
        released_structures_LMD = {}
        holding_entries = {}
        resp = self.catalog.get(url)
        resp.raise_for_status()
        entries = resp.json()
        for entry in entries:
            released_structures_LMD[entry['Accession_Code']] = self.getSubmissionTimeUTC(f'{entry["Submission_Time"]}')
            submitted_files = entry['Submitted_Files']
            rid = urlquote(entry['Entry'])
            url = f'/entity/PDB:entry/RID={rid}/Entry_Generated_File'
            resp = self.catalog.get(url)
            resp.raise_for_status()
            rows = resp.json()
            for row in rows:
                #header = '/pdb_ihm/data/entries/es/test-pdbdev_00000389'
                file_type = row['File_Type']
                if file_type not in self.archive_file_types:
                    continue
                manifest_key = entry['Accession_Code']
                h = entry['Accession_Code'][1:3].lower()
                header = f'/{self.released_entry_dir}/{h}/{manifest_key.lower()}'
                if manifest_key not in holding_entries.keys():
                    holding_entries[manifest_key] = {}
                    for archive_dir in self.archive_dirs:
                        holding_entries[manifest_key][self.holding_map[archive_dir]] = []
                    holding_entries[manifest_key] = {}
                holding_key = self.holding_map[self.archive_category_dir_names[self.archive_category[file_type]]]
                if holding_key not in holding_entries[manifest_key].keys():
                    holding_entries[manifest_key][holding_key] = []
                if file_type == 'mmCIF':
                    holding_entries[manifest_key][holding_key].append(self.getSubmittedZipFile(f'{header}/structures/{submitted_files["structures"][0]}'))
                elif len(holding_entries[manifest_key][holding_key]) == 0:
                    for submitted_file in submitted_files["validation_reports"]:
                        holding_entries[manifest_key][holding_key].append(self.getSubmittedZipFile(f'{header}/validation_reports/{submitted_file}'))
                   
            
        self.generate_current_holdings(holdings=holding_entries)
        self.generate_released_structures_LMD(releases=released_structures_LMD)

    """
    Sort JSON object 
    """
    def order_dict(self, dictionary):
        result = {}
        for k, v in sorted(dictionary.items()):
            if isinstance(v, dict):
                result[k] = self.order_dict(v)
            elif isinstance(v, list):
                result[k] = sorted(v)
            else:
                result[k] = v
        return result
    
    """
    Get the archive directory 
    """
    def getArchiveDirectory(self):
        return self.archive_parent
        
    """
    Get the file archive subdirectory 
    """
    def getFileArchiveSubDirectory(self, hash, entry_id, archive_category):
        entry_dir = f'{self.released_entry_dir}/{hash}/{entry_id}/'
        return f'{entry_dir}{self.archive_category_dir_names[archive_category]}'
        
    """
    Get the holding subdirectory 
    """
    def getHoldingSubDirectory(self):
        return self.holding_dir
        
    """
    Get the archive date
    HT: Turn to class method?
    """
    def getArchiveDate(self):
        """
        Get the closed Thursday
        If the current date is before Thursday 2PM America/Los_Angeles:
            then return the Thursday of the current week
            else return the Thursday of the next week
        """
        #now = dt.now(timezone.utc)
        self.cutoff_day_of_week = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_wday
        now = dt.now(pytz.timezone('America/Los_Angeles'))
        closed_day_of_week = now + timedelta(days=(self.cutoff_day_of_week - now.weekday())%7)
        weekday = now.weekday()
        cutoff_hour_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_hour
        cutoff_minute_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_min
        if weekday == self.cutoff_day_of_week:
            """
            It is Thursday. Check the time.
            """
            hour = now.hour
            minute = now.minute
            if hour > cutoff_hour_pacific or hour == cutoff_hour_pacific and minute > cutoff_minute_pacific:
                """
                The UTC time has passed 9PM. Get the Thursday of next week
                The America/Los_Angeles time has passed 3PM. Get the Thursday of next week
                """
                closed_day_of_week += timedelta(days=7)
        closed_day_of_week=closed_day_of_week.replace(hour=cutoff_hour_pacific,minute=cutoff_minute_pacific,second=0,microsecond=0)
        return f'{closed_day_of_week}'

    """
    HT: TODO
    """
    def getPreviousArchiveDate(self, current_submission_time):
        """
        Get the latest Submission_Date that is less than the current submission Date. Otherwise, use Current_Submission_Date - 7 days
        """
        previous_submission_time = f'{dt.fromisoformat(current_submission_time) - timedelta(days=7)}'
        self.logger.debug(f'Submission Dates: {current_submission_time}, {previous_submission_time}') 
        url = f'/aggregate/A:=PDB:PDB_Archive/Submission_Time::lt::{urlquote(current_submission_time)}/previous_submission_time:=max(Submission_Time)'
        self.logger.debug(f"Query to find the maximum submission time:\n\nhttps://{self.host}/ermrest/catalog/{self.catalog_id}{url}\n") 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        self.logger.debug(f'Previous submission time\n\n{json.dumps(rows, indent=4)}\n')
        if len(rows) > 0 and rows[0]['previous_submission_time'] != None:
            previous_submission_time = rows[0]['previous_submission_time']
        return previous_submission_time
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, filename, file_url, input_dir):
        try:
            error_message = None
            hatracFile = '{}/{}'.format(input_dir, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return (hatracFile, error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR getHatracFile: "%s"' % str(ev)
            return (None,error_message)
        
    """
    Get the archive file directory
    """
    def getFileArchiveDirectory(self, hash, entry_id, file_type):
        entry_dir = f'{self.archive_parent}/{self.released_entry_dir}/{hash}/{entry_id}/{self.archive_category_dir_names[self.archive_category[file_type]]}'
    
        return entry_dir

    """
    Get the PDB_Accession_Code
    """
    def get_hash(self, accesion_code_row):
        h = accesion_code_row['Accession_Code'][1:3].lower()
        return h

        """
        try:
            r = re.search(r'^pdb_(\w{4})$', accesion_code_row['Accesssion_Code'])
            h = r.group(1)[1:3]
        except:
            h = accesion_code_row['PDB_Accession_Code'][1:3]
        """
        """
        h = accesion_code_row['Accession_Code'][1:3].lower()
        return h
        """

    """
    Get the primary accession code
    """
    def get_entry_id(self, accesion_code_row):
        return accesion_code_row['Accession_Code']

    """
    Get the manifest keys
    """
    def get_manifest_key(self, accesion_code_row):
        return accesion_code_row['Accession_Code']

    """
    Generate the released zip file and move it to the archive directory
    """
    def generateReleasedZip(self, filename, hash, entry_id, file_type, manifest_key):
        currentDirectory=os.getcwd()
        os.chdir(self.data_scratch)
        renamed_file = filename.lower()
        os.rename(filename,renamed_file)

        with open(renamed_file, 'rb') as f_in:
            with gzip.open(f'{renamed_file}.gz', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        
        """
        Move the file to the archive directory
        """
        archiveDirectory = self.getFileArchiveDirectory(hash, manifest_key.lower(), file_type)
        os.makedirs(archiveDirectory, exist_ok=True)
        try:
            os.remove(f'{archiveDirectory}/{renamed_file}.gz')
        except:
            pass
        shutil.move(f'{renamed_file}.gz', archiveDirectory)
        os.remove(renamed_file)
        os.chdir(currentDirectory)
        
        holding_key = self.holding_map[self.archive_category_dir_names[self.archive_category[file_type]]]
        file_path = archiveDirectory[len(self.archive_parent):]
        self.current_holdings[manifest_key][holding_key].append(f'{file_path}/{renamed_file}.gz')

    """
    Generate the holding zip file 
    """
    def generateHoldingZip(self, filename):
        currentDirectory=os.getcwd()
        os.chdir(f'{self.archive_parent}/{self.getHoldingSubDirectory()}')
        try:
            os.remove(f'{filename}.gz')
        except:
            pass
        with open(filename, 'rb') as f_in:
            with gzip.open(f'{filename}.gz', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        os.remove(filename)
        os.chdir(currentDirectory)

    """
    Insert a record
    """
    def insert_rows(self, rows, table, schema='PDB', defaults=None):
       url = f'/entity/{schema}:{table}'
       if defaults != None:
           url = f'{url}?defaults={defaults}'
       resp = self.catalog.post(
           url,
           json=rows
       )
       resp.raise_for_status()
       self.logger.debug('SUCCEEDED created in the table "%s" the rows "%s".' % (url, json.dumps(rows, indent=4))) 
       return resp.json()[0]

    """
    Update a record
    """
    def update_rows(self, columns, rows, table, schema='PDB'):
        columns = ','.join([urlquote(col) for col in columns])
        url = f'/attributegroup/{schema}:{table}/RID;{columns}'
        resp = self.catalog.put(
            url,
            json=rows
        )
        resp.raise_for_status()

    """
    Insert or update a record
    """
    def insert_or_update_rows(self, inserted_rows, updated_rows, table, columns, schema='PDB'):
        if len(inserted_rows) > 0:
            """
            The records does not exist
            """
            self.insert_rows(inserted_rows, table, schema)

        if len(updated_rows) > 0:
            """
            The records exist. Update them.
            """
            self.update_rows(columns, updated_rows, table, schema)

    """
    Generate the released_structures_LMD file
    """
    def generate_released_structures_LMD(self, releases=None):
        if releases == None:
            releases = {}
            for row in self.released_records:
                releases[self.get_manifest_key(row)] = f'{self.submission_date}T00:00:00+00:00'
            
        if len(releases) == 0:
            """
            No new archived entries
            """
            releases = {}

        
        """
        Write the released_structures_LMD file
        """
        os.makedirs(f'{self.archive_parent}/{self.getHoldingSubDirectory()}', exist_ok=True)
        with open(f'{self.archive_parent}/{self.getHoldingSubDirectory()}/released_structures_last_modified_dates.json', 'w') as outfile:
            json.dump(dict(sorted(releases.items())), outfile, indent=4)

        """
        zip the JSON file
        """
        self.generateHoldingZip('released_structures_last_modified_dates.json')
                
    """
    Generate the current_holdings file
    """
    def generate_current_holdings(self, holdings=None):
        if holdings == None:
            holdings = self.current_holdings
        
        if len(holdings) == 0:
            """
            No new holdings
            """
            holdings = {}

        """
        Write the generate_current_holdings file
        """
        os.makedirs(f'{self.archive_parent}/{self.getHoldingSubDirectory()}', exist_ok=True)
        ordered_holding = self.order_dict(holdings)
        with open(f'{self.archive_parent}/{self.getHoldingSubDirectory()}/current_file_holdings.json', 'w') as outfile:
            json.dump(ordered_holding, outfile, indent=4)

        """
        zip the JSON file
        """
        self.generateHoldingZip('current_file_holdings.json')

    """
    Generate the released_structures_LMD file
    """
    def generate_unreleased_entries(self):
        url = '/attribute/A:=PDB:entry/Workflow_Status=HOLD/B:=PDB:Accession_Code/$A/RID,Accession_Code,Deposit_Date,B:PDB_Accession_Code'
        self.logger.debug(f'Query for unreleased entries: "{url}"') 
        
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        
        if len(rows) == 0:
            """
            No new unreleased entries
            """
            return
        
        unreleased_entries = {}
        for row in rows:
            excluded = False
            rid = urlquote(row['RID'])
            url = f'/entity/PDB:entry/RID={rid}/PDB:Entry_Generated_File'
            response = self.catalog.get(url)
            response.raise_for_status()
            cif_file = response.json()
            """
            if len(response.json()) < 3:
                self.hold_warnings.append(rid)
                excluded = True
            else:
                for cif_file in response.json():
                    file_type = cif_file['File_Type']
                    if file_type != 'mmCIF':
                        continue
                    cif_file_name = cif_file['File_Name']
                    if cif_file_name != f'{row["Accession_Code"]}.cif':
                        self.hold_warnings.append(rid)
                        excluded = True
                        break
            
            if excluded == True:
                continue
            """
            
            unreleased_entries[row['Accession_Code']] = {'status_code': 'HOLD',
                                                         'deposit_date': f'{row["Deposit_Date"]}T00:00:00+00:00',
                                                         'prerelease_sequence_available_flag': 'N'}
        
        """
        for row in self.no_validation_rids:
            unreleased_entries[row['Accession_Code']] = {'status_code': 'REL',
                                                         'deposit_date': row['Deposit_Date'],
                                                         'prerelease_sequence_available_flag': 'N'}
        """
        
        """
        Write the unreleased_entries file
        """
        os.makedirs(f'{self.archive_parent}/{self.getHoldingSubDirectory()}', exist_ok=True)
        with open(f'{self.archive_parent}/{self.getHoldingSubDirectory()}/unreleased_entries.json', 'w') as outfile:
            json.dump(dict(sorted(unreleased_entries.items())), outfile, indent=4)

        """
        zip the JSON file
        """
        self.generateHoldingZip('unreleased_entries.json')
                
    """
    Store the  file into hatrac
    """
    def storeFileInHatrac(self, hatrac_namespace, file_name, file_path):
        try:
            newFile = '{}/{}'.format(file_path, file_name)
            file_size = os.path.getsize(newFile)
            hashes = hu.compute_file_hashes(newFile, hashes=['md5', 'sha256'])
            new_md5 = hashes['md5'][1]
            new_sha256 = hashes['sha256'][1]
            hexa_md5 = hashes['md5'][0]
            new_uri = '{}/{}'.format(hatrac_namespace, urlquote(file_name))
            chunked = True if file_size > DEFAULT_CHUNK_SIZE else False
            
            """
            Store the file in hatrac if it is not already
            """
            hatrac_URI = None
            try:
                outfile = '{}.hatrac'.format(newFile)
                r = self.store.get_obj(new_uri, destfilename=outfile)
                hatrac_URI = r.headers['Content-Location']
                old_hatrac_URI = hatrac_URI
                hashes = hu.compute_file_hashes(outfile, hashes=['md5', 'sha256'])
                old_hexa_md5 = hashes['md5'][0]
                os.remove(outfile)
            except:
                os.remove(outfile)
                old_hexa_md5 = None
            
            if hatrac_URI != None and hexa_md5 == old_hexa_md5:
                self.logger.debug('Skipping the upload of the file "%s" as it already exists hatrac.' % file_name)
            else:
                if mimetypes.inited == False:
                    mimetypes.init()
                content_type,encoding = mimetypes.guess_type(newFile)
                if content_type == None:
                    content_type = 'application/octet-stream'
                try:
                    hatrac_URI = self.store.put_loc(new_uri,
                                                    newFile,
                                                    headers={'Content-Type': content_type},
                                                    content_disposition = "filename*=UTF-8''%s" % urlquote(file_name),
                                                    md5 = new_md5,
                                                    sha256 = new_sha256,
                                                    content_type = content_type,
                                                    chunked = chunked
                                                    )
                except NotModified:
                    hatrac_URI = old_hatrac_URI
                    self.logger.debug(f'{file_name} NotModified')
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('Can not upload file "%s" in hatrac "%s". Error: "%s"' % (file_name, new_uri, str(ev)))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    subject = 'PDB-Dev Error archiving files.'
                    self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
                    return (None, None, None, None)
            return (hatrac_URI, file_name, file_size, hexa_md5)

        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
            return (None, None, None, None)

"""
Examples of the manifest files

released_structures_last_modified_dates.json
{
    "PDBDEV_00000001" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000002" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000003" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000004" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000005" : "2024-04-30T00:00:00+00:00"
}

current_file_holdings.json
{
"PDBDEV_00000001" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/structures/PDBDEV_00000001.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/validation_reports/PDBDEV_00000001_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/validation_reports/PDBDEV_00000001_summary_validation.pdf"
    ]
},
"PDBDEV_00000002" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/structures/PDBDEV_00000002.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/validation_reports/PDBDEV_00000002_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/validation_reports/PDBDEV_00000002_summary_validation.pdf"
    ]
},
"PDBDEV_00000003" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/structures/PDBDEV_00000003.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/validation_reports/PDBDEV_00000003_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/validation_reports/PDBDEV_00000003_summary_validation.pdf"
    ]
},
"PDBDEV_00000004" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/structures/PDBDEV_00000004.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/validation_reports/PDBDEV_00000004_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/validation_reports/PDBDEV_00000004_summary_validation.pdf"
    ]
},
"PDBDEV_00000005" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/structures/PDBDEV_00000005.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/validation_reports/PDBDEV_00000005_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/validation_reports/PDBDEV_00000005_summary_validation.pdf"
    ]
}
}

unreleased_entries.json
{
"PDBDEV_00000367" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-01-22T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000368" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-05T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000377" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-03-12T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000378" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000379" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
}
}

"""
       

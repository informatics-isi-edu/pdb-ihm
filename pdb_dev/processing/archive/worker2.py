#!/usr/bin/python3
# 
# Copyright 2017 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Worker for archiving mmCIF files.
"""

import os
import stat
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
import gzip
from email.mime.text import MIMEText
import socket
from socket import gaierror, EAI_AGAIN

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote, get_credential
from deriva.core.utils import hash_utils as hu
from deriva.core.utils.core_utils import DEFAULT_CHUNK_SIZE, NotModified
from deriva.utils.extras.data import get_key2rows, get_ermrest_query, insert_if_not_exist, update_table_rows

import re
import math
import mimetypes
from datetime import datetime as dt, timedelta, timezone
import pytz

mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'

"""
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/Vocab:System_Generated_File_Type/!Archive_Category::null::/Archive_Category,Name
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/A:=PDB:entry/Workflow_Status=REL/B:=PDB:Entry_Generated_File/File_Type=mmCIF/C:=left(A:RID)=(PDB:Entry_Latest_Archive:Entry)/Entry::null::;!B:File_URL=mmCIF_URL/$A/RID
"""


def dump_json_to_file(file_path, json_object):
    print("dump_json_to_file: file_path %s" % (file_path))
    fw = open(file_path, 'w')
    json.dump(json_object, fw, indent=4)
    fw.write(f'\n')
    fw.close()
    
def dumpJSON(fileneme, json_object):
    fw = open(f'/home/pdbihm/tmp/{fileneme}.json', 'w')
    json.dump(json_object, fw, indent=4)
    fw.write(f'\n')
    fw.close()
    
class ArchiveClient (object):
    # HT: initialize with ermrest query
    submission_date = None
    previous_submission_date = None

    system_generated_file_types = {}
    entry_latest_archive = {}
    new_releases = {}
    re_releases = {}
    entry_generated_files = {}
    released_entries = {}
    hold_entries = {}    
    entry_id2rid = {}
    entry_archive_insert_rids = []
    entry_archive_update_rids = []
    entry_archive_inserted = []    # update this variable after successfully inserting to entry_latest_archive
    entry_archive_updated = []     # update this variable after successfully update to entry_latest_archive
    pdb_archive_rid = None
    pdb_archive = None  # Read at the beginning of the script for rollback
    pdb_archive_inserted = None
    pdb_archive_updated = None
    
    """
    Network client for archiving mmCIF files.
    """
    def __init__(self, kwargs):
        self.scheme = 'https'
        self.host = kwargs.get("hostname")
        self.catalog_id = kwargs.get("catalog_id")
        self.archive_parent = kwargs.get("archive_parent")
        self.released_entry_dir = kwargs.get("released_entry_dir")
        self.holding_dir = kwargs.get("holding_dir")
        self.data_scratch = kwargs.get("data_scratch")
        self.credentials = kwargs.get("credentials")
        self.hatrac_namespace = kwargs.get("hatrac_namespace")
        self.holding_namespace = kwargs.get("holding_namespace")
        self.archive_category_dir_names = {}
        self.credentials = get_credential(self.host)
        self.store = HatracStore(
            self.scheme, 
            self.host,
            self.credentials
        )
        self.catalog = PollingErmrestCatalog(
            self.scheme, 
            self.host,
            self.catalog_id,
            self.credentials
        )
        self.catalog.dcctx['cid'] = 'pipeline/archive'
        self.email = kwargs.get("email")
        self.cutoff_time_pacific = kwargs.get('cutoff_time_pacific')
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')

        self.printConfiguration()
        self.submission_date = self.getArchiveDate()
        self.previous_submission_date = self.getPreviousArchiveDate(self.submission_date)

        
        
        self.set_entry_latest_archive()
        self.set_pdb_archive()
        self.set_archive_entries()
        #self.set_released_entries()
        self.set_system_generated_file_types()
        self.set_entry_generated_files()   # validate filename and update new_releases and re_releases
        self.set_entry_archive_lists()
        self.set_current_holdings()

    def set_entry_latest_archive(self):
        constraints = "E:=PDB:entry/$M"
        attributes = ["M:Entry","M:Submission_Time","M:mmCIF_URL","M:Submitted_Files","M:Archive","M:Submission_History","M:RCT","Structure_Id:=E:id","E:Accession_Code"]
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Latest_Archive", constraints=constraints, attributes=attributes)
        for row in rows:
            self.entry_latest_archive[row["Entry"]] = row
        print("entry_latest_archive[:10]: %s" % (json.dumps(list(self.entry_latest_archive.values())[:3], indent=4)))

    # read pdb_archive at the beginning in case it needs to roll back
    def set_pdb_archive(self):
        constraints = "Submission_Time=%s" % (urlquote(self.submission_date))
        rows = get_ermrest_query(self.catalog, "PDB", "PDB_Archive", constraints=constraints)
        if rows:
            self.pdb_archive = rows[0]
            self.pdb_archive_rid = rows[0]["RID"]
        print("pdb_archive: %s" % (json.dumps(self.pdb_archive, indent=4)))

    def set_new_releases(self):
        url1 = f'/attribute/' + \
            f'E:=PDB:entry/Workflow_Status=REL/' + \
            f'F:=(id)=(PDB:Entry_Generated_File:Structure_Id)/File_Type=mmCIF/' + \
            f'A:=left(E:RID)=(PDB:Entry_Latest_Archive:Entry)/A:RID::null::;(A:RCT::gt::{urlquote(self.previous_submission_date)}&A:Submission_Time={urlquote(self.submission_date)})/' + \
            f'$E/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_Name,F:File_URL,Latest_Archive_RID:=A:RID,A:Entry,A:Submission_Time,A:mmCIF_URL'
            
        self.logger.debug(f"Query for entries that haven't been archived: {url1}") 
        resp = self.catalog.get(url1)
        resp.raise_for_status()
        rows = resp.json()
        # Note: A:* might be null
        for row in rows:
            row["accession_code_hash"] = self.get_hash(row["Accession_Code"])
            # TODO: check mmCIF filename
            if row["File_Name"].startswith(row["Accession_Code"]):
                self.new_releases[row["RID"]] = row
            else:
                # TODO: add to warning list                
                pass
        print("new_releases: %s" % (json.dumps(self.new_releases, indent=4)))
        
    def set_re_releases(self):
        url2 = f'/attribute/' + \
            f'A:=PDB:Entry_Latest_Archive/A:RCT::leq::{urlquote(self.previous_submission_date)}/' + \
            f'E:=(A:Entry)=(PDB:entry:RID)/Workflow_Status=REL/' + \
            f'F:=left(A:mmCIF_URL)=(PDB:Entry_Generated_File:File_URL)/F:RID::null::;(F:File_Type=mmCIF&A:Submission_Time={urlquote(self.submission_date)})/' + \
            f'$A/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_Name,F:File_URL,Latest_Archive_RID:=A:RID,A:Entry,A:Submission_Time,A:mmCIF_URL'
        
        self.logger.debug(f"Query for entries that entries that mmCIF contents have changed: {url2}") 
        resp = self.catalog.get(url2)
        resp.raise_for_status()
        rows = resp.json()
        # Note: F:* might be null
        #print(json.dumps(rows, indent=4))
        for row in rows:
            row["accession_code_hash"] = self.get_hash(row["Accession_Code"])
            # TODO: check mmCIF filename
            if row["File_Name"] and not row["File_Name"].startswith(row["Accession_Code"]):
                # TODO: add to warning list
                pass                
            else:
                self.re_releases[row["RID"]] = row
        print("re_releases: %s" % (json.dumps(self.re_releases, indent=4)))
        
    def set_archive_entries(self):
        self.set_new_releases()
        self.set_re_releases()
        self.set_released_entries()
        self.archive_entries = {}
        for row in list(self.new_releases.values()) + list(self.re_releases.values()):
            self.archive_entries[row["RID"]] = row
        print("archive_entries: %s" % (json.dumps(self.archive_entries, indent=4)))

    """
      REL entries. No longer needed?
    """
    def set_released_entries(self):
        """
        constraints = "RID=ANY(%s)" %  ",".join([ urlquote(v) for v in entry_rids ])
        rows = get_ermrest_query(self.catalog, "PDB", "entry", constraints, attributes=["id","Accession_Code"])
        for row in rows:
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
        """
        for row in list(self.new_releases.values()) + list(self.re_releases.values()):
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
        print("released_entries: %s" % (json.dumps(self.released_entries, indent=4)))
        print("entry_id2rid: %s" % (json.dumps(self.entry_id2rid, indent=4)))        
        # TODO: check duplicate RIDs?

    """
      HOLD entries
    """
    def get_hold_status_entries(self):
        constraints = "Workflow_Status=HOLD"
        rows = get_ermrest_query(self.catalog, "PDB", "entry", constraints=constraints, attributes=["Accession_Code","Deposit_Date","Workflow_Status"])
        entries = {}
        hold_entries = {}
        for row in rows:
            entries[row["Accession_Code"]] = row
        for accession_code in sorted(entries.keys()):
            r = {
                "status_code" : row["Workflow_Status"],
                "deposit_date" : f'{row["Deposit_Date"]}T00:00:00+00:00',
                #"deposit_date_org" : dt.strptime(row["Deposit_Date"], '%Y-%m-%d %H:%M:%S%z'),
                "prerelease_sequence_available_flag" : "N",
            }
            hold_entries[accession_code] = r
        return hold_entries
    
    """
    Information about system generated file types
    """
    def set_system_generated_file_types(self):
        constraints = "A:=Vocab:Archive_Category/$M"
        attributes = ["File_Type:=M:Name","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "Vocab", "System_Generated_File_Type", constraints, attributes=attributes)
        for row in rows:
            self.system_generated_file_types[row["File_Type"]] = row
            self.archive_category_dir_names[row["Archive_Category"]] = row["Directory_Name"]

    """
    query based on released RIDs
      self.entry_generated_files : {
        <RID>: {"mmCIF": [....], "Validation: Full PDF": [...], ...   }
      }
    """
    def set_entry_generated_files(self):
        entry_rids = list(self.new_releases.keys()) + list(self.re_releases.keys())        
        constraints = "Structure_Id=ANY(%s)/T:=Vocab:System_Generated_File_Type/A:=Vocab:Archive_Category/$M" % ",".join([ urlquote(self.archive_entries[rid]["id"]) for rid  in entry_rids ])
        attributes = ["M:Structure_Id","M:File_Type","M:File_Name","M:File_URL","M:File_Bytes","M:File_MD5","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Generated_File", constraints=constraints, attributes=attributes)
        incorrect_filename_entry_rids = set()
        for row in rows:
            rid = self.entry_id2rid[row["Structure_Id"]]
            accession_code = self.archive_entries[rid]["Accession_Code"]
            file_type = row["File_Type"]
            row["Accession_Code"] = accession_code            
            row["archive_dir"] =  self.getFileArchiveSubDirectory2(accession_code, row["Directory_Name"])
            row["archive_path"] = "%s/%s.gz" % (row["archive_dir"], row["File_Name"].lower())
            self.entry_generated_files.setdefault(rid, {})
            self.entry_generated_files[rid][row["File_Type"]] = row
            if row["File_Type"] == "mmCIF":
                self.archive_entries[rid]["File_URL"] = row["File_URL"]
                self.archive_entries[rid]["File_Name"] = row["File_Name"]
                self.archive_entries[rid]["archive_dir"] = row["archive_dir"]
                self.archive_entries[rid]["archive_path"] = row["archive_path"]                
            # check filenames
            if not row["File_Name"].startswith(accession_code):
                incorrect_filename_entry_rids.add(rid)
        # TODO: remove these entries with incorrect filenames from the lists
        to_delete = set(self.new_releases.keys()).intersection(incorrect_filename_entry_rids)
        print("incorrect_filename: %s" % (incorrect_filename_entry_rids))
        print("to_delete_new_releases: %s" % (to_delete))
        for rid in to_delete:
            del self.new_releases[rid]
            del self.archive_entries[rid]
        to_delete = set(self.re_releases.keys()).intersection(incorrect_filename_entry_rids)
        print("to_delete_re_releases: %s" % (to_delete))        
        for rid in to_delete:
            del self.re_releases[rid]
            del self.archive_entries[rid]
        self.rel_warnings = incorrect_filename_entry_rids
        print("entry_generated_files: %s" % (json.dumps(self.entry_generated_files, indent=4)))
        print("archive_entries: %s" % (json.dumps(self.archive_entries, indent=4)))
        #print("re_releases: %s" % (json.dumps(self.re_releases, indent=4)))                
        
    def set_entry_archive_lists(self):
        for rid in self.new_releases.keys():
            if rid not in self.entry_latest_archive.keys():
                self.entry_archive_insert_rids.append(rid)
            else:
                self.entry_archive_update_rids.append(rid)
        self.entry_archive_update_rids.extend(self.re_releases.keys())

    
    def set_current_holdings(self):
        #curent_holdings = self.entry_latest_archive.copy()
        #print("entry_latest_archive rids: %s" % (self.entry_latest_archive.keys()))
        current_holdings = {}
        for rid, row in self.archive_entries.items():
            submitted_files = {}
            manifests = {}
            for file_type, generated_file in self.entry_generated_files[rid].items():
                submitted_files.setdefault(generated_file["Directory_Name"], [])
                submitted_files[generated_file["Directory_Name"]].append(generated_file["File_URL"])
                manifests.setdefault(generated_file["Archive_Category"], [])                
                manifests[generated_file["Archive_Category"]].append("/"+generated_file["archive_path"])
            current_holdings[rid] = {
                "Entry" : rid,
                "Submission_Time" : self.submission_date,
                "mmCIF_URL" : self.entry_generated_files[rid]["mmCIF"]["File_URL"],
                "Submitted_Files": submitted_files,
                "Archive" : self.pdb_archive_rid,
                "Submission_History": None,
                "archive_manifest": manifests,
                "Accession_Code": row["Accession_Code"],
            }
            # TODO: Fix history with incomplete submitted files
            if rid in self.entry_archive_update_rids: 
                submission_history = {
                    self.entry_latest_archive[rid]["Submission_Time"] : {
                        "mmCIF_URL": self.entry_latest_archive[rid]["mmCIF_URL"],
                        "Submission_Files": self.entry_latest_archive[rid]["Submitted_Files"],
                    }
                }
                print("submission_history [%s] : %s" % (rid, json.dumps(current_holdings, indent=4)))            
                if not current_holdings[rid]["Submission_History"]:
                    current_holdings[rid]["Submission_History"] = submission_history
                else: 
                    current_holdings[rid]["Submission_History"].update(submission_history)
                    
        print("current_holdings: %s" % (json.dumps(current_holdings, indent=4)))
        self.current_entry_latest_archive = current_holdings
        

        
    """
    Print the configuration
    """
    def printConfiguration(self):
        config = {
            'host': self.host,
            'catalog_id': self.catalog_id,
            'archive_parent': self.archive_parent,
            'released_entry_dir': self.released_entry_dir,
            'holding_dir': self.holding_dir,
            'data_scratch': self.data_scratch,
            'credentials': self.credentials,
            'hatrac_namespace': self.hatrac_namespace,
            'holding_namespace': self.holding_namespace,
            'email': self.email
            }
        print(json.dumps(config, indent=4))
        return 0

    """
    Send email notification
    """
    def sendMail(self, subject, text):
        if self.email['server'] and self.email['sender']:
            if self.host in ['dev.pdb-dev.org', 'dev-aws.pdb-dev.org']:
                subject = 'DEV {}'.format(subject)
            retry = 0
            ready = False
            receivers = self.email['receivers']
            while not ready:
                try:
                    msg = MIMEText('%s\n\n%s' % (text, self.email['footer']), 'plain')
                    msg['Subject'] = subject
                    msg['From'] = self.email['sender']
                    msg['To'] = receivers
                    s = smtplib.SMTP_SSL(self.email['server'], self.email['port'])
                    s.login(self.email['user'], self.email['password'])
                    s.sendmail(self.email['sender'], receivers.split(','), msg.as_string())
                    s.quit()
                    self.logger.debug(f'Sent email notification to {receivers}.')
                    ready = True
                except socket.gaierror as e:
                    if e.errno == socket.EAI_AGAIN:
                        time.sleep(100)
                        retry = retry + 1
                        ready = retry > 10
                    else:
                        ready = True
                    if ready:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    ready = True

    """
    Rollback the database depending on exceptions or book keeping?: HT: TODO
    1. Delete PDB.Archive row if the row was inserted
    2. Delete Entry_Latest_Archive with RIDs from self.entry_archive_insert 
    3. Update modified Entry_Latest_Archive with the original version from self.entry_latest_archive
    Note: the delete_table_rows address 404 (which is returned if the constraints are not found)
    """
    def rollbackArchive(self):
        entry_archive_rollback_payload = []
        try:
            """
            """
            # delete the self.pdb_archive_RID if inserted, otherwise update to original state
            if self.pdb_archive_rid:
                if self.pdb_archive:
                    pdb_archive_rollback_payload = update_table_rows(self.catalog, "PDB", "Pdb_Archive", keys=["RID"], payload=[self.pdb_archive])
                    self.logger.debug('SUCCEEDED updated the PDB_Archive row with RID = %s' % (self.pdb_archive_rid))                    
                else:
                    delete_table_rows(self.catalog, "PDB", "PDB_Archive", key="RID", values=["self.PDB_Archive_RID"])                
                    self.logger.debug('SUCCEEDED deleted the PDB_Archive row with RID = %s' % (self.PDB_Archive_RID))

            # delete inserted Entry_Latest_Archive
            if self.entry_archive_inserted:
                # Need another flag to indicate that insertion has been performed. Then delete using RID from self.entry_archive_insert
                # These rids should be exactly the same list as self.entry_archive_insert_rids, but let's get it from what returns from ermrest
                rids = [ row["RID"] for row in self.entry_archive_inserted ]
                delete_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", key="RID", values=rids)
                self.logger.debug('SUCCEEDED deleted the newly inserted rows in the Entry_Latest_Archive')

            # Update Entry_Latest_Archive to its original state
            if self.entry_archive_updated:
                update_archive_payload = [ self.entry_latest_archive[row["RID"]] for row in self.entry_archive_updated ]
                updated = update_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", keys=["RID"], payload=entry_archive_rollback_payload) 
                self.logger.debug('SUCCEEDED updating the Entry_Latest_Archive rows to the original sate')
        except:
            # HT: TODO: If failed to roll back, we should output the following into a file: archive_error_<date>
            #  1. self.PDB_Archive_row
            #  2. self.PDB_Archive_row_before
            #  2. self.entry_archive_inserted --> new rows added to the table
            #  3. self.entry_archive_updated --> new rows update to the table
            #  4. entry_archive_rollback_payload --> the payload attempt to roll back to
            #
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))

    """
    Process Archiving Files 
    """
    def processArchive(self):
        print("processArchive")
        try:
            """
            Get the:
                - file types with Archive_Category
                - archive directories and the associated file types
                - archive directories of the Archive_Category
            """
            self.archive_category = {}
            self.archive_file_types = []
            self.archive_category_dir_names = {}
            for k,v in self.system_generated_file_types.items():
                self.archive_category[k] = v['Archive_Category']
                self.archive_file_types.append(k)
                self.archive_category_dir_names[v['Archive_Category']] = v['Directory_Name']

            """
            Get the archive directories and the associated file types
            """
            self.archive_dirs = []
            self.holding_map = {}
            for k,v in self.archive_category_dir_names.items():
                self.archive_dirs.append(v)
                self.holding_map[v] = k

            """
            Archive files
            """
            print("calling archiveFiles")            
            return_code = self.archiveFiles()
            if return_code != 0:
                self.rollbackArchive()
            return return_code
        except:
            et, ev, tb = sys.exc_info()
            print('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))
            self.rollbackArchive()
           
            return 1

    """
    Archive files
    HT:
      - Prepare directory of files
      - Create manifest files
      - insert/Update PDB:PDB_Archive
      - Update PDB:Entry_Latest_Archive -> update "Archive" field with the PDB:Archive RID before performing updates
    """
    def archiveFiles(self):

        """
        Get the new entries to be released:
         - not released
         - or modified
        """
        rel_warnings = []
        self.hold_warnings = []
        
        """
        Clean up the release and holding directories
        """
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.released_entry_dir}')
        except:
            pass
        
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.holding_dir}')
        except:
            pass

        """
        Create archive sub directories
        """
        data_archive_dir = f'{self.archive_parent}/{self.released_entry_dir}'
        manifest_dir = f'{self.archive_parent}/{self.holding_dir}'
        os.makedirs(data_archive_dir, exist_ok=True)
        os.makedirs(manifest_dir, exist_ok=True)

        self.no_validation_rids = []
        
        """
        Get the new released files
        """
        for rid, row in self.archive_entries.items():
            accession_code = row['Accession_Code']
            for file_type,file_generated in self.entry_generated_files[rid].items():
                filename = file_generated['File_Name']
                file_url = file_generated['File_URL']
                renamed_file = filename.lower()
                file_path,error_message = self.getHatracFile(renamed_file, file_url, self.data_scratch)
                print("accession_code: %s, file_path: %s" % (accession_code, file_path))
                # Zip the file
                self.generateGzip(file_path, output_dir=data_archive_dir)

                # TODO: warning about validation reports?
                #self.no_validation_rids.append({"Accession_Code": row['Accession_Code'], "Deposit_Date": row['Deposit_Date']})

        """
        Generate the manifest files
        """
        print("current_entry_latest_archive: %s" % (self.current_entry_latest_archive))
        current_holdings = {}
        lmd_dict = {}        
        current_holding_fpath = "%s/current_file_holdings.json" % (self.data_scratch)
        lmd_fpath = "%s/released_structures_last_modified_dates.json" % (self.data_scratch)        

        for row in self.current_entry_latest_archive.values():
            #print("setting holdings: %s - %s \n" % (row["Accession_Code"], row["archive_manifest"]))
            current_holdings[row["Accession_Code"]] = row["archive_manifest"]
            #dt.strptime(row["Submission_Time", '%Y-%m-%d %H:%M:%S%z'),                
            lmd_dict[row["Accession_Code"]] = self.getTimeUTC(row["Submission_Time"])
        current_holding_payload = self.order_dict(current_holdings)
        lmd_payload = self.order_dict(lmd_dict)

        dump_json_to_file(current_holding_fpath, current_holding_payload)
        dump_json_to_file(lmd_fpath, lmd_payload)        
        self.generateGzip(current_holding_fpath, output_dir=manifest_dir)
        self.generateGzip(lmd_fpath, output_dir=manifest_dir)

        hold_entries_payload = self.get_hold_status_entries()
        hold_entries_fpath = "%s/unreleased_entries.json" % (self.data_scratch)
        dump_json_to_file(hold_entries_fpath, hold_entries_payload)
        self.generateGzip(hold_entries_fpath, output_dir=manifest_dir)

        print("current_holding_payload: %s" % (json.dumps(current_holding_payload, indent=4)))
        print("released_structures_last_modified_dates: %s" % (json.dumps(lmd_payload, indent=4)))
        print("hold_entries: %s" % (json.dumps(hold_entries_payload, indent=4)))        

        """
        Store in hatrac the manifest files
        """
        submission_datetime = dt.strptime(self.submission_date, '%Y-%m-%d %H:%M:%S%z')
        year = submission_datetime.year
        submission_date = f'{submission_datetime.date()}'
        hatrac_namespace = f'/{self.hatrac_namespace}/{self.holding_namespace}/{year}/{submission_date}'
        input_dir = manifest_dir
        
        file_name = "%s.gz" % (current_holding_fpath.rsplit("/", 1)[1])
        Current_File_Holdings_URL, Current_File_Holdings_Name, Current_File_Holdings_Bytes, Current_File_Holdings_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Current_File_Holdings_URL == None:
            return 1
 
        file_name = "%s.gz" % (lmd_fpath.rsplit("/", 1)[1])        
        Released_Structures_LMD_URL, Released_Structures_LMD_Name, Released_Structures_LMD_Bytes, Released_Structures_LMD_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Released_Structures_LMD_URL == None:
            return 1
 
        file_name = "%s.gz" % (hold_entries_fpath.rsplit("/", 1)[1])                
        Unreleased_Entries_URL, Unreleased_Entries_Name, Unreleased_Entries_Bytes, Unreleased_Entries_MD5 = self.storeFileInHatrac(hatrac_namespace, file_name, input_dir)
        if Unreleased_Entries_URL == None:
            return 1

        """
        Create or update the entry in the PDB_Archive table 
        """
        if not self.pdb_archive:
            current_pdb_archive = {
                "Submission_Time": self.submission_date,
            }
        else:
            current_pdb_archive = self.pdb_archive.copy()
        current_pdb_archive.update({
            'Submitted_Entries': len(self.archive_entries),
            'New_Released_Entries': len(self.new_releases),
            'Re_Released_Entries': len(self.re_releases),
            'Current_File_Holdings_Name': Current_File_Holdings_Name,
            'Current_File_Holdings_URL': Current_File_Holdings_URL,
            'Current_File_Holdings_Bytes': Current_File_Holdings_Bytes,
            'Current_File_Holdings_MD5': Current_File_Holdings_MD5,
            'Released_Structures_LMD_Name': Released_Structures_LMD_Name,
            'Released_Structures_LMD_URL': Released_Structures_LMD_URL,
            'Released_Structures_LMD_Bytes': Released_Structures_LMD_Bytes,
            'Released_Structures_LMD_MD5': Released_Structures_LMD_MD5,
            'Unreleased_Entries_Name': Unreleased_Entries_Name,
            'Unreleased_Entries_URL': Unreleased_Entries_URL,
            'Unreleased_Entries_Bytes': Unreleased_Entries_Bytes,
            'Unreleased_Entries_MD5': Unreleased_Entries_MD5
        })

        if not self.pdb_archive:
            print("inserting pdb_archive_payload: %s" % (json.dumps(current_pdb_archive, indent=4)))
            #self.pdb_archive_inserted = insert_row_if_not_exist(self.catalog, "PDB", "PDB_Archive", payload=[pdb_archive])
        else:
            print("updating pdb_archive_payload: %s" % (json.dumps(current_pdb_archive, indent=4)))            
            #self.pdb_archive_updated = update_table_row(self.catalog, "PDB", "PDB_Archive", payload=[pdb_archive])
        
        """
        Create or update the entry in the Entry_Latest_Archive table
        """

        payload = []
        for rid in self.entry_archive_insert_rids:
            payload.append(self.current_entry_latest_archive[rid])
        print("inserting entry_latest_archive_payload: %s" % (json.dumps(payload, indent=4)))            
        #self.archive_entries_inserted = insert_if_not_exist(self.catalog, "PDB", "Entry_Latest_Archive", payload=payload)

        payload = []
        for rid in self.entry_archive_update_rids:
            payload.append(self.current_entry_latest_archive[rid])            
        print("updating entry_latest_archive_payload: %s" % (json.dumps(payload, indent=4)))                        
        #self.archive_entries_updated = update_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", payload=payload)

        
        return 0
        
        total_warning = len(self.hold_warnings) + len(rel_warnings)
        if total_warning > 0:
            subject_rids = []
            for rel_warning in rel_warnings:
                subject_rids.append(rel_warning)
                if len(subject_rids) >= 3:
                    break
            if len(subject_rids) < 3:
                for hold_warning in self.hold_warnings:
                    subject_rids.append(hold_warning)
                    if len(subject_rids) >= 3:
                        break
                    
            subject = f'PDB-Dev weekly archive warnings: {", ".join(subject_rids)}'
            if total_warning > 3:
                subject= f'{subject}, ...'
                rids = []
                for rel_warning in rel_warnings:
                    rids.append(f'{rel_warning}: REL') 
                for hold_warning in self.hold_warnings:
                    rids.append(f'{hold_warning}: HOLD') 
            newline_char = "\n"
            rids = f'{newline_char.join(rids)}'
            message_body = f'The following entries were not archived due to inconsistent filenames or missing system generated files:\n\n{rids}'
            self.sendMail(subject, message_body)

        return 0

    """
    Get UTC Submission Time 
    """
    def getTimeUTC(self, submission_str):
        submission_obj = dt.fromisoformat(submission_str)
        #submission_utc = submission_obj.replace(hour=21,tzinfo=timezone.utc).isoformat()
        submission_utc = submission_obj.astimezone(timezone.utc).isoformat()
        return f'{submission_utc}'

    """
    Get the corresponding Zip file of the hatrac filename  
    """
    def getSubmittedZipFile(self, filename):
        if 'hatrac' not in filename:
            return filename
        parts = filename.split('/')
        name = f'{parts[-1].split(":")[0].lower()}.gz'
        ret='/'.join(parts[0:7]+[name])
        return ret

    """
    Sort JSON object 
    """
    def order_dict(self, dictionary):
        result = {}
        for k, v in sorted(dictionary.items()):
            if isinstance(v, dict):
                result[k] = self.order_dict(v)
            elif isinstance(v, list):
                result[k] = sorted(v)
            else:
                result[k] = v
        return result
    
    """
    Get the file archive subdirectory 
    """
    def getFileArchiveSubDirectory(self, hash, entry_id, archive_category):
        entry_dir = f'{self.released_entry_dir}/{hash}/{entry_id}/'
        return f'{entry_dir}{self.archive_category_dir_names[archive_category]}'

    def getFileArchiveSubDirectory2(self, accession_code, category_dir_name):
        hash = self.get_hash(accession_code)
        entry_dir = f'{self.released_entry_dir}/{hash}/{accession_code.lower()}'
        return f'{entry_dir}/{category_dir_name}'
    
    """
    Get the archive date
    HT: Turn to class method?
    """
    def getArchiveDate(self):
        """
        Get the closed Thursday
        If the current date is before Thursday 2PM America/Los_Angeles:
            then return the Thursday of the current week
            else return the Thursday of the next week
        """
        #now = dt.now(timezone.utc)
        self.cutoff_day_of_week = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_wday
        now = dt.now(pytz.timezone('America/Los_Angeles'))
        closed_day_of_week = now + timedelta(days=(self.cutoff_day_of_week - now.weekday())%7)
        weekday = now.weekday()
        cutoff_hour_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_hour
        cutoff_minute_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_min
        if weekday == self.cutoff_day_of_week:
            """
            It is Thursday. Check the time.
            """
            hour = now.hour
            minute = now.minute
            if hour > cutoff_hour_pacific or hour == cutoff_hour_pacific and minute > cutoff_minute_pacific:
                """
                The UTC time has passed 9PM. Get the Thursday of next week
                The America/Los_Angeles time has passed 3PM. Get the Thursday of next week
                """
                closed_day_of_week += timedelta(days=7)
        closed_day_of_week=closed_day_of_week.replace(hour=cutoff_hour_pacific,minute=cutoff_minute_pacific,second=0,microsecond=0)
        return f'{closed_day_of_week}'

    """
    Get previous archive date
    """
    def getPreviousArchiveDate(self, current_submission_time):
        """
        Get the latest Submission_Date that is less than the current submission Date. Otherwise, use Current_Submission_Date - 7 days
        """
        previous_submission_time = f'{dt.fromisoformat(current_submission_time) - timedelta(days=7)}'
        self.logger.debug(f'Submission Dates: {current_submission_time}, {previous_submission_time}') 
        url = f'/aggregate/A:=PDB:PDB_Archive/Submission_Time::lt::{urlquote(current_submission_time)}/previous_submission_time:=max(Submission_Time)'
        self.logger.debug(f"Query to find the maximum submission time:\n\nhttps://{self.host}/ermrest/catalog/{self.catalog_id}{url}\n") 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        self.logger.debug(f'Previous submission time\n\n{json.dumps(rows, indent=4)}\n')
        if len(rows) > 0 and rows[0]['previous_submission_time'] != None:
            previous_submission_time = rows[0]['previous_submission_time']
        return previous_submission_time
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, filename, file_url, input_dir):
        try:
            error_message = None
            hatracFile = '{}/{}'.format(input_dir, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return (hatracFile, error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR getHatracFile: "%s"' % str(ev)
            return (None,error_message)
        
    """
    Get the archive file directory
    """
    def getFileArchiveDirectory(self, hash, entry_id, file_type):
        entry_dir = f'{self.archive_parent}/{self.released_entry_dir}/{hash}/{entry_id}/{self.archive_category_dir_names[self.archive_category[file_type]]}'
    
        return entry_dir

    """
    Get the PDB_Accession_Code hash
    """
    def get_hash(self, accession_code):
        if len(accession_code) == 4:
            h = accession_code[1:3].lower()
            return h            
        else:
            raise Exception("ERROR: Accession_Code %s is not a PDB accession code. Expect legnth of 4" % (accession_code))
        """
        try:
            r = re.search(r'^pdb_(\w{4})$', accesion_code_row['Accesssion_Code'])
            h = r.group(1)[1:3]
        except:
            h = accesion_code_row['PDB_Accession_Code'][1:3]
        """
        """
        h = accesion_code_row['Accession_Code'][1:3].lower()
        return h
        """

    """
    Generate the holding zip file 
    """
    def generateGzip(self, input_file_path, output_dir=None, delete_input=True):
        input_dir, filename = input_file_path.rsplit("/", 1)
        print("generateGzip: %s %s" % (input_dir, filename))
        if not output_dir: output_dir = input_dir
        with open(input_file_path, 'rb') as f_in:
            with gzip.open(f'{output_dir}/{filename}.gz', 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        if delete_input: os.remove(input_file_path)
        
    """
    Insert a record
    """
    def insert_rows(self, rows, table, schema='PDB', defaults=None):
       url = f'/entity/{schema}:{table}'
       if defaults != None:
           url = f'{url}?defaults={defaults}'
       resp = self.catalog.post(
           url,
           json=rows
       )
       resp.raise_for_status()
       self.logger.debug('SUCCEEDED created in the table "%s" the rows "%s".' % (url, json.dumps(rows, indent=4))) 
       return resp.json()

    """
    Update a record
    """
    def update_rows(self, columns, rows, table, schema='PDB'):
        columns = ','.join([urlquote(col) for col in columns])
        url = f'/attributegroup/{schema}:{table}/RID;{columns}'
        resp = self.catalog.put(
            url,
            json=rows
        )
        resp.raise_for_status()

    """
    Insert or update a record
    """
    def insert_or_update_rows(self, inserted_rows, updated_rows, table, columns, schema='PDB'):
        if len(inserted_rows) > 0:
            """
            The records does not exist
            """
            self.insert_rows(inserted_rows, table, schema)

        if len(updated_rows) > 0:
            """
            The records exist. Update them.
            """
            self.update_rows(columns, updated_rows, table, schema)

                
    """
    Store the  file into hatrac
    """
    def storeFileInHatrac(self, hatrac_namespace, file_name, file_path):
        try:
            newFile = '{}/{}'.format(file_path, file_name)
            file_size = os.path.getsize(newFile)
            hashes = hu.compute_file_hashes(newFile, hashes=['md5', 'sha256'])
            new_md5 = hashes['md5'][1]
            new_sha256 = hashes['sha256'][1]
            hexa_md5 = hashes['md5'][0]
            new_uri = '{}/{}'.format(hatrac_namespace, urlquote(file_name))
            chunked = True if file_size > DEFAULT_CHUNK_SIZE else False
            
            """
            Store the file in hatrac if it is not already
            """
            hatrac_URI = None
            try:
                outfile = '{}.hatrac'.format(newFile)
                r = self.store.get_obj(new_uri, destfilename=outfile)
                hatrac_URI = r.headers['Content-Location']
                old_hatrac_URI = hatrac_URI
                hashes = hu.compute_file_hashes(outfile, hashes=['md5', 'sha256'])
                old_hexa_md5 = hashes['md5'][0]
                os.remove(outfile)
            except:
                os.remove(outfile)
                old_hexa_md5 = None
            
            if hatrac_URI != None and hexa_md5 == old_hexa_md5:
                self.logger.debug('Skipping the upload of the file "%s" as it already exists hatrac.' % file_name)
            else:
                if mimetypes.inited == False:
                    mimetypes.init()
                content_type,encoding = mimetypes.guess_type(newFile)
                if content_type == None:
                    content_type = 'application/octet-stream'
                try:
                    hatrac_URI = self.store.put_loc(new_uri,
                                                    newFile,
                                                    headers={'Content-Type': content_type},
                                                    content_disposition = "filename*=UTF-8''%s" % urlquote(file_name),
                                                    md5 = new_md5,
                                                    sha256 = new_sha256,
                                                    content_type = content_type,
                                                    chunked = chunked
                                                    )
                except NotModified:
                    hatrac_URI = old_hatrac_URI
                    self.logger.debug(f'{file_name} NotModified')
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('Can not upload file "%s" in hatrac "%s". Error: "%s"' % (file_name, new_uri, str(ev)))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    subject = 'PDB-Dev Error archiving files.'
                    self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
                    return (None, None, None, None)
            return (hatrac_URI, file_name, file_size, hexa_md5)

        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
            return (None, None, None, None)

"""
Examples of the manifest files

released_structures_last_modified_dates.json
{
    "PDBDEV_00000001" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000002" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000003" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000004" : "2024-04-30T00:00:00+00:00",
    "PDBDEV_00000005" : "2024-04-30T00:00:00+00:00"
}

current_file_holdings.json
{
"PDBDEV_00000001" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/structures/PDBDEV_00000001.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/validation_reports/PDBDEV_00000001_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000001/validation_reports/PDBDEV_00000001_summary_validation.pdf"
    ]
},
"PDBDEV_00000002" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/structures/PDBDEV_00000002.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/validation_reports/PDBDEV_00000002_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000002/validation_reports/PDBDEV_00000002_summary_validation.pdf"
    ]
},
"PDBDEV_00000003" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/structures/PDBDEV_00000003.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/validation_reports/PDBDEV_00000003_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000003/validation_reports/PDBDEV_00000003_summary_validation.pdf"
    ]
},
"PDBDEV_00000004" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/structures/PDBDEV_00000004.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/validation_reports/PDBDEV_00000004_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000004/validation_reports/PDBDEV_00000004_summary_validation.pdf"
    ]
},
"PDBDEV_00000005" : {
    "mmcif": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/structures/PDBDEV_00000005.cif"
    ],
    "validation_report": [
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/validation_reports/PDBDEV_00000005_full_validation.pdf",
        "/pdb_ihm/data/entries/000/PDBDEV_00000005/validation_reports/PDBDEV_00000005_summary_validation.pdf"
    ]
}
}

unreleased_entries.json
{
"PDBDEV_00000367" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-01-22T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000368" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-05T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000377" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-03-12T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000378" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000379" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
}
}

"""
       

#!/usr/bin/python3
# 
# Copyright 2017 University of Southern California
# 
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
# 
#    http://www.apache.org/licenses/LICENSE-2.0
# 
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
"""
Worker for archiving mmCIF files.
"""

import os
import stat
import subprocess
import json
from urllib.parse import urlparse
import sys
import traceback
import time
import shutil
import hashlib
import smtplib
import gzip
from email.mime.text import MIMEText
import socket
from socket import gaierror, EAI_AGAIN

import re
import math
import mimetypes
from datetime import datetime as dt, timedelta, timezone
import pytz

from deriva.core import PollingErmrestCatalog, HatracStore, urlquote, get_credential
from deriva.core.utils import hash_utils as hu
from deriva.core.utils.core_utils import DEFAULT_CHUNK_SIZE, NotModified
from deriva.utils.extras.data import get_key2rows, get_ermrest_query, insert_if_not_exist, update_table_rows, delete_table_rows
from deriva.core.utils.hash_utils import compute_hashes
from ...utils.shared import PDBDEV_CLI, cfg
#from pdb_dev.utils.shared import PDBDEV_CLI, cfg

mail_footer = 'Do not reply to this message.  This is an automated message generated by the system, which does not receive email messages.'
send_email_notification = True

"""
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/Vocab:System_Generated_File_Type/!Archive_Category::null::/Archive_Category,Name
https://dev-aws.pdb-dev.org/ermrest/catalog/99/attribute/A:=PDB:entry/Workflow_Status=REL/B:=PDB:Entry_Generated_File/File_Type=mmCIF/C:=left(A:RID)=(PDB:Entry_Latest_Archive:Entry)/Entry::null::;!B:File_URL=mmCIF_URL/$A/RID
"""

def dump_json_to_file(file_path, json_object):
    #print("dump_json_to_file: file_path %s" % (file_path))
    fw = open(file_path, 'w')
    json.dump(json_object, fw, indent=4)
    fw.write(f'\n')
    fw.close()
    
class ArchiveClient (object):
    submission_date = None
    previous_submission_date = None

    system_generated_file_types = {}
    entry_latest_archive = {}
    new_releases = {}
    re_releases = {}
    entry_generated_files = {}
    released_entries = {}
    hold_entries = {}    
    entry_id2rid = {}
    entry_archive_insert_rids = []
    entry_archive_update_rids = []
    entry_archive_inserted = []    # update this variable after successfully inserting to entry_latest_archive
    entry_archive_updated = []     # update this variable after successfully update to entry_latest_archive
    pdb_archive_rid = None
    pdb_archive = None  # Read at the beginning of the script for rollback
    pdb_archive_inserted = None
    pdb_archive_updated = None
    processing_issues = {
        "incorrect filenames" : [],
        "missing files" : [],
    }
    
    """
    Network client for archiving mmCIF files.
    """
    def __init__(self, kwargs):
        self.scheme = 'https'
        self.host = kwargs.get("hostname")
        self.catalog_id = kwargs.get("catalog_id")
        self.archive_parent = kwargs.get("archive_parent")
        self.released_entry_dir = kwargs.get("released_entry_dir")
        self.holding_dir = kwargs.get("holding_dir")
        self.log_dir = kwargs.get("log_dir")        
        self.data_scratch = kwargs.get("data_scratch")
        self.credentials = kwargs.get("credentials")
        self.hatrac_namespace = kwargs.get("hatrac_namespace")
        self.holding_hatrac_namespace = kwargs.get("holding_hatrac_namespace")
        self.verbose = kwargs.get("verbose")
        self.rollback = kwargs.get("rollback")
        self.dry_run = kwargs.get("dry_run")
        self.archive_category_dir_names = {}
        self.archive_dir_names_category = {}
        self.store = HatracStore(
            self.scheme, 
            self.host,
            self.credentials
        )
        self.catalog = PollingErmrestCatalog(
            self.scheme, 
            self.host,
            self.catalog_id,
            self.credentials
        )
        self.catalog.dcctx['cid'] = 'pipeline/archive'
        self.email = kwargs.get("email")
        self.cutoff_time_pacific = kwargs.get('cutoff_time_pacific')
        self.logger = kwargs.get("logger")
        self.logger.debug('Client initialized.')
        
        #self.printConfiguration()
        self.submission_time = self.getArchiveDate()
        self.previous_submission_time = self.getPreviousArchiveDate(self.submission_time)
        if self.verbose: print("submission_time: %s, previous_submission_time: %s" % (self.submission_time, self.previous_submission_time))


    """
    Information about system generated file types
    """
    def set_system_generated_file_types(self):
        constraints = "A:=Vocab:Archive_Category/$M"
        attributes = ["File_Type:=M:Name","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "Vocab", "System_Generated_File_Type", constraints, attributes=attributes)
        for row in rows:
            self.system_generated_file_types[row["File_Type"]] = row
            self.archive_category_dir_names[row["Archive_Category"]] = row["Directory_Name"]
            self.archive_dir_names_category[row["Directory_Name"]] = row["Archive_Category"]            
        #print("system_generated_file_type: %s" % (json.dumps(self.system_generated_file_types, indent=4)))


    """
    Read pdb_archive at the beginning in case it needs to roll back. If exists, set pdb_archive_rid
    """
    def set_pdb_archive(self):
        constraints = "Submission_Time=%s" % (urlquote(self.submission_time))
        rows = get_ermrest_query(self.catalog, "PDB", "PDB_Archive", constraints=constraints)
        if rows:
            self.pdb_archive = rows[0]
            self.pdb_archive_rid = rows[0]["RID"]            
            self.pdb_archive["Submission_Time"] = self.pdb_archive["Submission_Time"].replace("T", " ")

        #print("pdb_archive: %s" % (json.dumps(self.pdb_archive, indent=4)))

    """
    Query all entry_latest_archive prior to any processing. This will be used for rollback if needed and shouldn't be changed. 
    archive_manifest is derived from Submission_File column
    """
    def set_entry_latest_archive(self):
        constraints = "E:=PDB:entry/$M"
        attributes = ["Latest_Archive_RID:=M:RID","M:Entry","M:Submission_Time","M:mmCIF_URL","M:Submitted_Files","M:Archive","M:Submission_History","M:RCT","Structure_Id:=E:id","E:Accession_Code"]
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Latest_Archive", constraints=constraints, attributes=attributes)
        current_cycle_rids = []
        for row in rows:
            row["Submission_Time"] = row["Submission_Time"].replace("T", " ")
            self.entry_latest_archive[row["Entry"]] = row
            if self.getTimeUTC(row["Submission_Time"]) == self.getTimeUTC(self.submission_time): current_cycle_rids.append(row["Entry"])
            # -- derive archive_manifest to be used for current_holding manifest later.
            row["archive_manifest"] = {}
            for dir_name, hatrac_files in row["Submitted_Files"].items():
                archive_category = self.archive_dir_names_category[dir_name]
                archive_parent_path = "/"+self.getFileArchiveSubDirectory2(row["Accession_Code"], dir_name)
                row["archive_manifest"].setdefault(archive_category, [])                
                for fp in hatrac_files:
                    file_name = "%s.gz" % ( fp.rsplit("/", 1)[1].rsplit(":", 1)[0] )
                    archive_path = "%s/%s" % (archive_parent_path, file_name.lower())
                    row["archive_manifest"][archive_category].append(archive_path)

        #["4-3YG6"]            
        #print("entry_latest_archive (within cycles): %s" % (json.dumps({ rid: self.entry_latest_archive[rid] for rid in current_cycle_rids }, indent=4)))
        #print("entry_latest_archive: %s" % (json.dumps(list(self.entry_latest_archive.values())[:2], indent=4)))        

    """
    Query for new releases. The dict key is the entry RID.
    """
    def set_new_releases(self):
        url1 = f'/attribute/' + \
            f'E:=PDB:entry/Workflow_Status=REL/' + \
            f'F:=(id)=(PDB:Entry_Generated_File:Structure_Id)/File_Type=mmCIF/' + \
            f'A:=left(E:RID)=(PDB:Entry_Latest_Archive:Entry)/A:RID::null::;(A:RCT::gt::{urlquote(self.previous_submission_time)}&A:Submission_Time={urlquote(self.submission_time)})/' + \
            f'$E/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_Name,F:File_URL,Latest_Archive_RID:=A:RID,A:Entry,A:Submission_Time,A:mmCIF_URL'
            
        self.logger.debug(f"Query for entries that haven't been archived: {url1}") 
        resp = self.catalog.get(url1)
        resp.raise_for_status()
        rows = resp.json()
        # Note: A:* might be null
        for row in rows:
            row["accession_code_hash"] = self.get_hash(row["Accession_Code"])
            # check mmCIF filename
            if row["File_Name"].startswith(row["Accession_Code"]):
                self.new_releases[row["RID"]] = row
            else:
                # Add to error/warning list
                self.processing_issues["incorrect filenames"].append("Entry RID:%s, Accession_Code:%s: incorrect mmcif filename: %s" % (row["RID"], row["Accession_Code"], row["File_Name"] ))
        if self.verbose: print("new_releases: %s" % (json.dumps(self.new_releases, indent=4)))

    """
    Query for re-releases. The dict key is the entry RID.
    """
    def set_re_releases(self):
        url2 = f'/attribute/' + \
            f'A:=PDB:Entry_Latest_Archive/A:RCT::leq::{urlquote(self.previous_submission_time)}/' + \
            f'E:=(A:Entry)=(PDB:entry:RID)/Workflow_Status=REL/' + \
            f'F:=left(A:mmCIF_URL)=(PDB:Entry_Generated_File:File_URL)/F:RID::null::;(F:File_Type=mmCIF&A:Submission_Time={urlquote(self.submission_time)})/' + \
            f'$A/E:RID,E:id,E:Deposit_Date,E:Accession_Code,F:File_Name,F:File_URL,Latest_Archive_RID:=A:RID,A:Entry,A:Submission_Time,A:mmCIF_URL'

        self.logger.debug(f"Query for entries that entries that mmCIF contents have changed: {url2}") 
        resp = self.catalog.get(url2)
        resp.raise_for_status()
        rows = resp.json()
        # Note: F:* might be null
        #print(json.dumps(rows, indent=4))
        for row in rows:
            row["accession_code_hash"] = self.get_hash(row["Accession_Code"])
            # check mmCIF filename
            if row["File_Name"] and not row["File_Name"].startswith(row["Accession_Code"]):
                self.processing_issues["incorrect filenames"].append("Entry RID:%s, Accession_Code:%s: incorrect mmcif filename: %s" % (row["RID"], row["Accession_Code"], row["File_Name"] ))
            else:
                self.re_releases[row["RID"]] = row
        if self.verbose: print("re_releases: %s" % (json.dumps(self.re_releases, indent=4)))
        

    """
    REL entries. No longer needed since the same information is in self.archive_entries.
    self.released_entries are not used anywhere else
    """
    def set_released_entries(self):
        """
        constraints = "RID=ANY(%s)" %  ",".join([ urlquote(v) for v in entry_rids ])
        rows = get_ermrest_query(self.catalog, "PDB", "entry", constraints, attributes=["id","Accession_Code"])
        for row in rows:
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
        """
        for row in list(self.new_releases.values()) + list(self.re_releases.values()):
            self.released_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]
        #print("released_entries: %s" % (json.dumps(self.released_entries, indent=4)))
        #print("entry_id2rid: %s" % (json.dumps(self.entry_id2rid, indent=4)))        
        # TODO: check duplicate RIDs?

    """
    archive_entries: a set of new and re releases for this archive cycle
    This function also set up entry id2rid lookup dict
    """
    def set_archive_entries(self):
        self.set_new_releases()
        self.set_re_releases()
        #self.set_released_entries()
        self.archive_entries = {}
        for row in list(self.new_releases.values()) + list(self.re_releases.values()):
            self.archive_entries[row["RID"]] = row
            self.entry_id2rid[row["id"]] = row["RID"]            
        #if self.verbose: print("archive_entries: %s" % (json.dumps(self.archive_entries, indent=4)))
        if self.verbose: print("existing entry_latest_archives: %s" % (json.dumps(
                { rid: self.entry_latest_archive[rid] for rid in set(self.archive_entries.keys()).intersection(set(self.entry_latest_archive.keys())) },
                indent=4)))
        
    """
    HOLD entries
    """
    def get_hold_status_entries(self):
        constraints = "Workflow_Status=HOLD/!Accession_Code::null::"
        rows = get_ermrest_query(self.catalog, "PDB", "entry", constraints=constraints, attributes=["Accession_Code","Deposit_Date","Workflow_Status"])
        entries = {}
        for row in rows:
            entries[row["Accession_Code"]] = row
        hold_entries = {}
        for accession_code in sorted(entries.keys()):
            row = entries[accession_code]
            r = {
                "status_code" : row["Workflow_Status"],
                "deposit_date" : f'{row["Deposit_Date"]}T00:00:00+00:00',
                #"deposit_date_org" : dt.strptime(row["Deposit_Date"], '%Y-%m-%d %H:%M:%S%z'),
                "prerelease_sequence_available_flag" : "N",
            }
            hold_entries[accession_code] = r
        if self.verbose: print("HOLD entries: %s" % (hold_entries))
        return hold_entries
    
    """
    Get a list of generated files of the new/re released entries, and organized them accordingly. 
       self.entry_generated_files : {
         <RID>: {"mmCIF": [....], "Validation: Full PDF": [...], ...   }
       }
    """
    def set_entry_generated_files(self):
        # constraints = "E:=PDB:entry/Workflow_Status=REL/$M/T:=Vocab:System_Generated_File_Type/A:=Vocab:Archive_Category/$M" --> get all released entry files
        entry_rids = list(self.new_releases.keys()) + list(self.re_releases.keys())        
        constraints = "Structure_Id=ANY(%s)/T:=Vocab:System_Generated_File_Type/A:=Vocab:Archive_Category/$M" % ",".join([ urlquote(self.archive_entries[rid]["id"]) for rid  in entry_rids ])
        attributes = ["M:Structure_Id","M:File_Type","M:File_Name","M:File_URL","M:File_Bytes","M:File_MD5","Archive_Category:=A:Name","A:Directory_Name"]
        rows = get_ermrest_query(self.catalog, "PDB", "Entry_Generated_File", constraints=constraints, attributes=attributes)
        incorrect_filename_entry_rids = set()
        for row in rows:
            rid = self.entry_id2rid[row["Structure_Id"]]
            accession_code = self.archive_entries[rid]["Accession_Code"]
            file_type = row["File_Type"]
            row["Accession_Code"] = accession_code            
            row["archive_dir"] =  self.getFileArchiveSubDirectory2(accession_code, row["Directory_Name"])
            row["archive_path"] = "%s/%s.gz" % (row["archive_dir"], row["File_Name"].lower())
            if row["File_Type"] == "mmCIF":
                self.archive_entries[rid]["File_URL"] = row["File_URL"]
                self.archive_entries[rid]["File_Name"] = row["File_Name"]
                self.archive_entries[rid]["archive_dir"] = row["archive_dir"]
                self.archive_entries[rid]["archive_path"] = row["archive_path"]
            # check filenames
            if row["File_Name"].startswith(accession_code):
                self.entry_generated_files.setdefault(rid, {})
                self.entry_generated_files[rid][row["File_Type"]] = row
            else:
                self.processing_issues["incorrect filenames"].append("Entry RID:%s, Accession_Code:%s: incorrect filenames: %s" % (rid, accession_code, row["File_Name"] ))
                incorrect_filename_entry_rids.add(rid)

        # remove these entries with incorrect filenamess from the lists
        to_delete = set(self.new_releases.keys()).intersection(incorrect_filename_entry_rids)
        if self.verbose: print("incorrect_filename: %s" % (incorrect_filename_entry_rids))
        if self.verbose: print("to_remove_from_new_releases: %s" % (to_delete))
        for rid in to_delete:
            del self.new_releases[rid]
            del self.archive_entries[rid]
        to_delete = set(self.re_releases.keys()).intersection(incorrect_filename_entry_rids)
        if self.verbose: print("to_remove_from_re_releases: %s" % (to_delete))        
        for rid in to_delete:
            del self.re_releases[rid]
            del self.archive_entries[rid]
        if self.verbose: print("entry_generated_files: %s" % (json.dumps(self.entry_generated_files, indent=4)))
        #print("archive_entries: %s" % (json.dumps(self.archive_entries, indent=4)))


    """
    Get a list of rids for entries to be inserted and updated. If the entry latest archive metadata doesn't change, it is excluded from the list.
    This is to avoid unnessary insert/update.
    """
    def set_entry_archive_insert_update_lists(self):
        unchanged_rids = []
        for rid in list(self.new_releases.keys()) + list(self.re_releases.keys()):
            if rid not in self.entry_latest_archive.keys():
                self.entry_archive_insert_rids.append(rid)
            else:
                before = self.entry_latest_archive[rid]
                after = self.current_entry_latest_archive[rid]
                if after["mmCIF_URL"] == before["mmCIF_URL"] and after["Archive"] == before["Archive"]:
                    unchanged_rids.append(rid)
                    continue
                self.entry_archive_update_rids.append(rid)
        if self.verbose: print("insert_rids: %s \nupdate_rids: %s \nunchanged_rids: %s" % (self.entry_archive_insert_rids, self.entry_archive_update_rids, unchanged_rids))

    """
    Create current_entry_latest_archive is a copy of the entry_latest_archive queried at the beginning but with the addtion and updates of the new and re-released metadata.
    It can be used to generate the manifests later.
    """
    def set_current_entry_latest_archive(self):
        self.current_entry_latest_archive = self.entry_latest_archive.copy()
        for rid, row in self.archive_entries.items():
            submitted_files = {}
            archive_manifest = {}
            for file_type, generated_file in self.entry_generated_files[rid].items():
                submitted_files.setdefault(generated_file["Directory_Name"], [])
                submitted_files[generated_file["Directory_Name"]].append(generated_file["File_URL"])
                archive_manifest.setdefault(generated_file["Archive_Category"], [])                
                archive_manifest[generated_file["Archive_Category"]].append("/"+generated_file["archive_path"])
            if len(submitted_files) == 1:
                self.processing_issues["missing files"].append("Entry RID:%s, Accession_Code:%s: missing validation reports." % (rid, row["Accession_Code"]))
            self.current_entry_latest_archive[rid] = {
                "RID" : row["Latest_Archive_RID"],
                "Entry" : rid,
                "Submission_Time" : self.submission_time,
                "mmCIF_URL" : self.entry_generated_files[rid]["mmCIF"]["File_URL"],
                "Submitted_Files": submitted_files,
                "Archive" : self.pdb_archive_rid,
                "Submission_History": None,
                "archive_manifest": archive_manifest,
                "Accession_Code": row["Accession_Code"],
            }
            # set up submission_history appropriately
            if rid in self.entry_latest_archive.keys():
                submission_history = {
                    self.entry_latest_archive[rid]["Submission_Time"] : {
                        #"mmCIF_URL": self.entry_latest_archive[rid]["mmCIF_URL"],
                        "Submitted_Files": self.entry_latest_archive[rid]["Submitted_Files"],
                    }
                }
                if not self.entry_latest_archive[rid]["Submission_History"]:
                    self.current_entry_latest_archive[rid]["Submission_History"] = submission_history
                else:  # update the history in descending order (e.g. creating a new dict with the current submission at the beginning, following by old history)
                    if self.submission_time in self.entry_latest_archive[rid]["Submission_History"].keys():
                        self.current_entry_latest_archive[rid]["Submission_History"] = self.entry_latest_archive[rid]["Submission_History"].copy()
                        self.current_entry_latest_archive[rid]["Submission_History"].update(submission_history)
                    else: 
                        self.current_entry_latest_archive[rid]["Submission_History"] = dict(submission_history, **self.entry_latest_archive[rid]["Submission_History"])
                #print("+++++ rid:%s entry_submission_history:%s + %s \n --> current_history:%s " % (rid, self.entry_latest_archive[rid]["Submission_History"], submission_history, self.current_entry_latest_archive[rid]["Submission_History"]))
        #print("current_entry_latest_archive (archive): %s" % (json.dumps({ k:self.current_entry_latest_archive[k] for k in self.archive_entries.keys() }, indent=4)))
        #print("current_entry_latest_archive: %s" % (json.dumps({ k:self.current_entry_latest_archive[k] for k in list(self.current_entry_latest_archive.keys())[:2] }, indent=4)))
        
        
    """
    Print the configuration
    """
    def printConfiguration(self):
        config = {
            'host': self.host,
            'catalog_id': self.catalog_id,
            'archive_parent': self.archive_parent,
            'released_entry_dir': self.released_entry_dir,
            'holding_dir': self.holding_dir,
            'data_scratch': self.data_scratch,
            'credentials': self.credentials,
            'hatrac_namespace': self.hatrac_namespace,
            'holding_hatrac_namespace': self.holding_hatrac_namespace,
            'email': self.email
            }
        print(json.dumps(config, indent=4))
        #print("cfg: host:%s, catalog_id:%s, is_dev:%s" % (cfg.host, cfg.catalog_id, cfg.is_dev))
        return 0

    """
    Send email notification
    """
    def sendMail(self, subject, text):
        if send_email_notification == False:
            return
        
        if self.email['server'] and self.email['sender']:
            if self.host in ['dev.pdb-dev.org', 'dev-aws.pdb-dev.org']:
                subject = 'DEV {}'.format(subject)
            retry = 0
            ready = False
            receivers = self.email['receivers']
            while not ready:
                try:
                    msg = MIMEText('%s\n\n%s' % (text, self.email['footer']), 'plain')
                    msg['Subject'] = subject
                    msg['From'] = self.email['sender']
                    msg['To'] = receivers
                    s = smtplib.SMTP_SSL(self.email['server'], self.email['port'])
                    s.login(self.email['user'], self.email['password'])
                    s.sendmail(self.email['sender'], receivers.split(','), msg.as_string())
                    s.quit()
                    self.logger.debug(f'Sent email notification to {receivers}.')
                    ready = True
                except socket.gaierror as e:
                    if e.errno == socket.EAI_AGAIN:
                        time.sleep(100)
                        retry = retry + 1
                        ready = retry > 10
                    else:
                        ready = True
                    if ready:
                        et, ev, tb = sys.exc_info()
                        self.logger.error('got exception "%s"' % str(ev))
                        self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('got exception "%s"' % str(ev))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    ready = True

    """
    Check DB values:
    select * from "PDB"."PDB_Archive" order by "Submission_Time" desc limit 1;
    select * from "PDB".entry where "RID" in ('4-1YWP', '4-3YG6');
    select * from "PDB"."Entry_Latest_Archive" where "Entry" in ('4-1YWP', '3-4RHP');
    select * from "PDB"."Entry_Generated_File" where "File_Type" = 'mmCIF' and "Structure_Id" in ('D_4-1YWP', 'D_3-4RHP');
    """

    """
    Reset database for development purposes.
    """
    def reset_db(self):
        print("Resetting database")

        entry_latest_archive_payload = [
            {
                "RID": "4-B29A",
                "Entry": "3-4RHP",
                "Submission_Time": "2024-09-12 20:00:00-07:00",
                "mmCIF_URL": "/hatrac/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/final_mmCIF/9A7Z.cif:EVWWNJPB5PQVUYX3ZG443A7CXM",
                #"mmCIF_URL": "/hatrac/dev/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/final_mmCIF/9A7Z.cif:4Y6KSKAPYSNPQFV4LQMWPMBJJE",
                "Submitted_Files": {
                    "structures": [
                        "/hatrac/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/final_mmCIF/9A7Z.cif:EVWWNJPB5PQVUYX3ZG443A7CXM",                        
                    ],
                    "validation_reports": [
                        "/hatrac/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/validation_report/9A7Z_full_validation.pdf:GI2C4UBKPJ3TML2URIQSAZW6H4",
                        "/hatrac/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/validation_report/9A7Z_summary_validation.pdf:HTEWG4GEUTBVY5JYGBHL7N5FZE"
                    ]
                },
                "Archive": "4-B20W",
                "Structure_Id": "D_3-4RHP",
                "Accession_Code": "9A7Z"
            }
        ]
        update_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", keys=["RID"], payload=entry_latest_archive_payload)

        to_delete = get_ermrest_query(self.catalog, "PDB", "Entry_Latest_Archive", constraints="Submission_Time=%s/Entry=4-1YWP" % (urlquote(self.submission_time)))
        print("reset_db: Entry_Latest_Archive to_delete: %s" % (json.dumps(to_delete, indent=4)))
        if len(to_delete) > 0:
            delete_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", key="RID", values=[ row["RID"] for row in to_delete ])

        delete_table_rows(self.catalog, "PDB", "PDB_Archive", key="Submission_Time", values=[self.submission_time])
        
        # reset mmcif file
        update_table_rows(self.catalog, "PDB", "Entry_Generated_File", keys=["RID"], payload = [
            {
                "Structure_Id": "D_4-1YWP", # new-release
                "RID": "4-EFAW",
                "File_Type": "mmCIF",
                "File_Name": "9A8U.cif",
                "File_URL": "/hatrac/pdb/generated/uid/71d43780-e6af-4dd3-b2ca-e6de2a4e1e62/entry/id/D_4-1YWP/final_mmCIF/9A8U.cif:KAIJ3WWKDD2TVWKLIEEUZ7V2CY",
                "File_Bytes": 438150,
                "File_MD5": "2fcd7a87319b59e905ef79c07fb2a7e9",
                "Entry_RCB": "https://auth.globus.org/71d43780-e6af-4dd3-b2ca-e6de2a4e1e62",
                #"Archive_Category": "mmcif",                
            },
            {
                "Structure_Id": "D_3-4RHP",                
                "RID": "4-9RST",
                "File_Type": "mmCIF",
                "File_Name": "9A7Z.cif",
                "File_URL": "/hatrac/dev/pdb/generated/uid/cc42b37b-27fd-44ce-b027-ee8e6fc4b36f/entry/id/D_3-4RHP/final_mmCIF/9A7Z.cif:4Y6KSKAPYSNPQFV4LQMWPMBJJE",
                "File_Bytes": 438155,
                "File_MD5": "c72c288edcd1415211f6a28f3a2d9c2a",
                "Archive_Category": "mmcif",
                "Directory_Name": "structures",
                "Accession_Code": "9A7Z",
                "archive_dir": "pdb_ihm/data/entries/a7/9a7z/structures",
                "archive_path": "pdb_ihm/data/entries/a7/9a7z/structures/9a7z.cif.gz"
            },
        ])

        self.set_system_generated_file_types()
        self.set_pdb_archive()
        self.set_entry_latest_archive()
        
    """
    Rollback the database depending on exceptions or book keeping:
    1. Delete Entry_Latest_Archive with RIDs from self.entry_archive_insert 
    2. Update modified Entry_Latest_Archive with the original version from self.entry_latest_archive
    3. Delete PDB.Archive row if the row was inserted
    4. update PDB.Archive row to the original state
    Note:
      1. the delete_table_rows address 404 (which is returned if the constraints are not found)
      2. If there is an exception raised, dump json structures to file
    """
    def rollbackArchive(self):
        try:
            if self.verbose: print("Attemping to rollback")
            # update entry_latest_archive table first before deleting pdb_archive due to fkey constraint
            # -- Entry_Latest_Archive
            # delete inserted Entry_Latest_Archive 
            if self.entry_archive_inserted:
                if self.verbose: print("rollback: deleting entry_latest_archive with rid:%s " % ([row["Entry"] for row in self.entry_archive_inserted]))
                # Need another flag to indicate that insertion has been performed. Then delete using RID from self.entry_archive_insert
                # These rids should be exactly the same list as self.entry_archive_insert_rids, but let's get it from what returns from ermrest
                rids = [ row["RID"] for row in self.entry_archive_inserted ]
                delete_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", key="RID", values=rids)
                if self.verbose: print('SUCCEEDED deleted the newly inserted rows in the Entry_Latest_Archive: %s' % (rids))                
                self.logger.debug('SUCCEEDED deleted the newly inserted rows in the Entry_Latest_Archive')

            # Update Entry_Latest_Archive to its original state
            if self.entry_archive_updated:
                if self.verbose: print("rollback: recovering entry_latest_archive with rid:%s " % ([row["Entry"] for row in self.entry_archive_updated]))
                update_archive_payload = [ self.entry_latest_archive[row["Entry"]] for row in self.entry_archive_updated ]
                updated = update_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", keys=["RID"], payload=update_archive_payload)
                if self.verbose: print('SUCCEEDED updating the Entry_Latest_Archive rows to the original sate')                
                self.logger.debug('SUCCEEDED updating the Entry_Latest_Archive rows to the original sate')

            # -- PDB_Archive
            # delete the self.pdb_archive_RID if inserted, otherwise update to original state
            if self.pdb_archive_inserted:
                if self.verbose: print("rollback: deleting pdb_archive with rid:%s " % (self.pdb_archive_rid))
                delete_table_rows(self.catalog, "PDB", "PDB_Archive", key="RID", values=[self.pdb_archive_rid])
                if self.verbose: print('SUCCEEDED deleted the PDB_Archive row with RID = %s' % (self.pdb_archive_rid))
                self.logger.debug('SUCCEEDED deleted the PDB_Archive row with RID = %s' % (self.pdb_archive_rid))
            
            if self.pdb_archive_updated:
                if self.verbose: print("rollback: recovering pdb_archive with rid:%s " % (self.pdb_archive_rid))                
                pdb_archive_rollback_payload = update_table_rows(self.catalog, "PDB", "PDB_Archive", keys=["RID"], payload=[self.pdb_archive])
                if self.verbose: print('SUCCEEDED updated the PDB_Archive row with RID = %s' % (self.pdb_archive_rid))                                    
                self.logger.debug('SUCCEEDED updated the PDB_Archive row with RID = %s' % (self.pdb_archive_rid))                    

        except:
            # HT: TODO: If failed to roll back, we should output the following into a file: archive_error_<date>
            #  1. self.PDB_Archive_row
            #  2. self.PDB_Archive_row_before
            #  2. self.entry_archive_inserted --> new rows added to the table
            #  3. self.entry_archive_updated --> new rows update to the table
            #  4. entry_archive_rollback_payload --> the payload attempt to roll back to
            #
            dump_object = {
                "PDB_Archive_before": self.pdb_archive,
                "PDB_Archive_after" : self.current_pdb_archive,
                "Entry_Archive_inserted" : self.entry_archive_inserted,
                "Entry_Archive_updated" : self.entry_archive_updated,
                "Entry_Archive_update_rollback": [ self.entry_latest_archive[row["RID"]] for row in self.entry_archive_updated ]
            }
            submission_date = dt.strptime(self.submission_time, '%Y-%m-%d %H:%M:%S%z').date()
            rollback_fpath = f'{self.log_dir}/rollback_dump_{submission_date}.json'
            dump_json_to_file(rollback_fpath, dump_object)
            if self.verbse: print("ERROR: Unable to rollback. Will perform a dump at %s" % (rollback_fpath))
            self.logger.error("Unable to rollback. Will perform a dump at %s" % (rollback_fpath))            
            et, ev, tb = sys.exc_info()
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            if self.verbose: print("Exception: %s\n%s" % (str(ev), ''.join(traceback.format_exception(et, ev, tb))))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))

    """
    Process Archiving Files 
    """
    def processArchive(self):
        #print("processArchive")
        #self.reset_db()
        #sys.exit(1)
        try:
            """
            initilize
            """
            self.set_system_generated_file_types()
            self.set_pdb_archive()        
            self.set_entry_latest_archive()
            self.set_archive_entries()
            self.set_entry_generated_files()   # validate filename and update new_releases and re_releases
            self.set_current_entry_latest_archive()
            self.set_entry_archive_insert_update_lists()
            
            """
            Archive files
            """
            return_code = self.archiveFiles()
            if return_code != 0:
                self.rollbackArchive()
            return return_code
        except:
            self.rollbackArchive()
            et, ev, tb = sys.exc_info()
            print('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            self.logger.error('got exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % (''.join(traceback.format_exception(et, ev, tb))))
            return 1

    """
    Archive files
      - Prepare directory of files
      - Create manifest files
      - insert/Update PDB:PDB_Archive
      - Update PDB:Entry_Latest_Archive -> update "Archive" field with the PDB:Archive RID before performing updates
    """
    def archiveFiles(self):
        """
        Clean up the release and holding directories
        """
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.released_entry_dir}')
        except:
            pass
        
        try:
            shutil.rmtree(f'{self.archive_parent}/{self.holding_dir}')
        except:
            pass

        """
        Create archive sub directories
        """
        manifest_dir = f'{self.archive_parent}/{self.holding_dir}'
        os.makedirs(manifest_dir, exist_ok=True)

        
        """
        Download the new/re released files to data archive dir
        """
        if self.verbose: print("archive file parent folder: %s" % (self.archive_parent))
        for rid, row in self.archive_entries.items():
            accession_code = row['Accession_Code']
            for file_type,file_generated in self.entry_generated_files[rid].items():
                filename = file_generated['File_Name']
                file_url = file_generated['File_URL']
                output_dir = "%s/%s" % (self.archive_parent, file_generated['archive_dir'])
                renamed_file = filename.lower()
                file_path,error_message = self.getHatracFile(file_url, self.data_scratch, renamed_file)
                # Zip the file
                gz_fpath = self.generateGzip(file_path, output_dir=output_dir)
                if self.verbose: print("  accession_code: %s, file_path: %s, gz_file_path: %s" % (accession_code, file_path, gz_fpath))

        """
        Generate the manifest files
        """
        current_holdings = {}
        lmd_dict = {}        
        current_holding_fpath = "%s/current_file_holdings.json" % (self.data_scratch)
        lmd_fpath = "%s/released_structures_last_modified_dates.json" % (self.data_scratch)        

        for row in self.current_entry_latest_archive.values():
            #print("setting holdings: %s - %s \n" % (row["Accession_Code"], row["archive_manifest"]))
            current_holdings[row["Accession_Code"]] = row["archive_manifest"]
            #dt.strptime(row["Submission_Time", '%Y-%m-%d %H:%M:%S%z'),                
            lmd_dict[row["Accession_Code"]] = self.getTimeUTC(row["Submission_Time"])
        current_holding_payload = self.order_dict(current_holdings)
        lmd_payload = self.order_dict(lmd_dict)

        dump_json_to_file(current_holding_fpath, current_holding_payload)
        dump_json_to_file(lmd_fpath, lmd_payload)        
        current_holding_gz_fpath = self.generateGzip(current_holding_fpath, output_dir=manifest_dir)
        lmd_gz_fpath = self.generateGzip(lmd_fpath, output_dir=manifest_dir)

        hold_entries_payload = self.get_hold_status_entries()
        num_hold_entries = len(hold_entries_payload)
        hold_entries_fpath = "%s/unreleased_entries.json" % (self.data_scratch)
        dump_json_to_file(hold_entries_fpath, hold_entries_payload)
        hashes = hu.compute_file_hashes(hold_entries_fpath, hashes=['md5'])
        (hold_entries_md5_hex, hold_entries_md5_base64) = hashes['md5']
        hold_entries_gz_fpath = self.generateGzip(hold_entries_fpath, output_dir=manifest_dir)

        if self.verbose:
            print("current_holding_file: %s, gz: %s" % (current_holding_fpath, current_holding_gz_fpath))
            print("released_structures_last_modified_date_file: %s, gz: %s" % (lmd_fpath, lmd_gz_fpath))
            print("unreleased_entries_file: %s, gz: %s" % (hold_entries_fpath, hold_entries_gz_fpath))
            
            print("current_holding_payload[:2] (%d): %s" % (len(current_holding_payload), json.dumps({ k:current_holding_payload[k] for k in list(current_holding_payload.keys())[:2] }, indent=4)))
            print("released_structures_last_modified_dates[:2] (%d): %s" % (len(lmd_payload), json.dumps({ k:lmd_payload[k] for k in list(lmd_payload.keys())[:2] }, indent=4)))
            print("hold_entries[:2] (%d): %s" % (len(hold_entries_payload), json.dumps({ k:hold_entries_payload[k] for k in list(hold_entries_payload.keys())[:2] }, indent=4)))
            print("hold_entries_md5_hex: %s, hold_entries_md5_base64: %s" % (hold_entries_md5_hex, hold_entries_md5_base64))

        
        """
        Initialize hatrac information
        """
        submission_datetime = dt.strptime(self.submission_time, '%Y-%m-%d %H:%M:%S%z')
        year = submission_datetime.year
        submission_date = f'{submission_datetime.date()}'
        hatrac_namespace = f'{self.holding_hatrac_namespace}/{year}/{submission_date}'
        input_dir = manifest_dir

        """
        Create or update the entry in the PDB_Archive table 
        """
        self.current_pdb_archive = self.pdb_archive.copy() if self.pdb_archive else {}
        self.current_pdb_archive.update({
            "Submission_Time": self.submission_time,
            'Submitted_Entries': len(self.archive_entries),
            'New_Released_Entries': len(self.new_releases),
            'Re_Released_Entries': len(self.re_releases),
            'Unreleased_Entries': num_hold_entries,
        })

        # Flags to check whether there are changes the manifests:
        released_entries_modified = False
        hold_entries_modified = False
        if (not self.pdb_archive) or len(self.entry_archive_insert_rids) or len(self.entry_archive_update_rids):
            released_entries_modified = True
        if self.pdb_archive is None or (not self.pdb_archive["Unreleased_Entries_Unzip_MD5"]) or hold_entries_md5_hex != self.pdb_archive["Unreleased_Entries_Unzip_MD5"]:
            hold_entries_modified = True
        #print("Flags: released_entries_modified: %s, hold_entries_modified: %s self.pdb_archive: %s" % (released_entries_modified, hold_entries_modified, self.pdb_archive))

        # -- if released entries have db changes, reupload manifest files and update file information
        if released_entries_modified:
            Current_File_Holdings_URL, Current_File_Holdings_Name, Current_File_Holdings_Bytes, Current_File_Holdings_MD5 = self.storeFileInHatrac(hatrac_namespace, current_holding_gz_fpath)
            if Current_File_Holdings_URL == None: return 1
            if self.verbose: print("Hatrac uploading %s to %s (%d, %s)" % (Current_File_Holdings_Name, Current_File_Holdings_URL, Current_File_Holdings_Bytes, Current_File_Holdings_MD5))
           
            Released_Structures_LMD_URL, Released_Structures_LMD_Name, Released_Structures_LMD_Bytes, Released_Structures_LMD_MD5 = self.storeFileInHatrac(hatrac_namespace, lmd_gz_fpath)
            if Released_Structures_LMD_URL == None: return 1
            if self.verbose: print("Hatrac uploading %s to %s (%d, %s)" % (Released_Structures_LMD_Name, Released_Structures_LMD_URL, Released_Structures_LMD_Bytes, Released_Structures_LMD_MD5))
            
            self.current_pdb_archive.update({
                'Current_File_Holdings_Name': Current_File_Holdings_Name,
                'Current_File_Holdings_URL': Current_File_Holdings_URL,
                'Current_File_Holdings_Bytes': Current_File_Holdings_Bytes,
                'Current_File_Holdings_MD5': Current_File_Holdings_MD5,
                'Released_Structures_LMD_Name': Released_Structures_LMD_Name,
                'Released_Structures_LMD_URL': Released_Structures_LMD_URL,
                'Released_Structures_LMD_Bytes': Released_Structures_LMD_Bytes,
                'Released_Structures_LMD_MD5': Released_Structures_LMD_MD5,
            })

        # -- if released entries have db changes, reupload manifest files and update file information            
        if hold_entries_modified:
            Unreleased_Entries_URL, Unreleased_Entries_Name, Unreleased_Entries_Bytes, Unreleased_Entries_MD5 = self.storeFileInHatrac(hatrac_namespace, hold_entries_gz_fpath)
            if Unreleased_Entries_URL == None: return 1
            if self.verbose: print("Hatrac uploading %s to %s (%d, %s)" % (Unreleased_Entries_Name, Unreleased_Entries_URL, Unreleased_Entries_Bytes, Unreleased_Entries_MD5))
            
            self.current_pdb_archive.update({
                'Unreleased_Entries_Name': Unreleased_Entries_Name,
                'Unreleased_Entries_URL': Unreleased_Entries_URL,
                'Unreleased_Entries_Bytes': Unreleased_Entries_Bytes,
                'Unreleased_Entries_MD5': Unreleased_Entries_MD5,
                'Unreleased_Entries_Unzip_MD5': hold_entries_md5_hex,                
            })

        # Note: if dry_run is true, do not execute the ermrest changes
        if not self.pdb_archive: # there is no existing pdb_archive record
            if self.dry_run:
                print("inserting pdb_archive: %s" % (json.dumps(self.current_pdb_archive, indent=4)))
            else:
                self.pdb_archive_inserted = insert_if_not_exist(self.catalog, "PDB", "PDB_Archive", payload=[self.current_pdb_archive])[0]
                if self.verbose: print("pdb_archive_inserted: %s" % (json.dumps(self.pdb_archive_inserted, indent=4)))
                self.pdb_archive_rid = self.pdb_archive_inserted["RID"]
                # Update "Archive" in archive_entries
                for row in self.current_entry_latest_archive.values():
                    row["Archive"] = self.pdb_archive_rid
        else:
            # - check whether the pdb_archive is modified to only perform updates if needed
            if released_entries_modified or hold_entries_modified:
                if self.dry_run:
                    print("updating pdb_archive: %s" % (json.dumps(self.current_pdb_archive, indent=4)))
                else:
                    self.pdb_archive_updated = update_table_rows(self.catalog, "PDB", "PDB_Archive", payload=[self.current_pdb_archive])
                    if self.verbose: print("pdb_archive_updated: %s" % (json.dumps(self.pdb_archive_updated, indent=4)))
            else:
                if self.verbose: print("pdb_archive_updated: []")

        #print("current_entry_latest_archive: %s" % (json.dumps(self.current_entry_latest_archive, indent=4)))
        
        """
        Create or update the entry in the Entry_Latest_Archive table
        """
        payload = []
        for rid in self.entry_archive_insert_rids:
            payload.append(self.current_entry_latest_archive[rid])
        if self.dry_run:
            print("inserting entry_latest_archive_payload: %s" % (json.dumps(payload, indent=4)))
        else:
            self.entry_archive_inserted = insert_if_not_exist(self.catalog, "PDB", "Entry_Latest_Archive", payload=payload)
            if self.verbose: print("entry_archive_inserted: %s" % (json.dumps(self.entry_archive_inserted, indent=4)))
        
        payload = []
        for rid in self.entry_archive_update_rids:
            payload.append(self.current_entry_latest_archive[rid])
        if self.dry_run:
            print("updating entry_latest_archive_payload: %s" % (json.dumps(payload, indent=4)))
        else:
            self.entry_archive_updated = update_table_rows(self.catalog, "PDB", "Entry_Latest_Archive", payload=payload)
            if self.verbose: print("entry_archive_updated: %s" % (json.dumps(self.entry_archive_updated, indent=4)))

        # send email or warning entries
        if self.processing_issues["incorrect filenames"] or self.processing_issues["missing files"]:
            subject = f'PDB-Dev weekly archive errors/warnings: {", ".join(subject_rids)}'
            message_body = "The ERRORS and WARNINGS are reported below: (incorrect filenames are errors while missing files are warning): %s" % (json.dumps(self.processing_issues))
            if self.verbose: print("The ERRORS and WARNINGS are reported below: (incorrect filenames are errors while missing files are warning): %s" % (json.dumps(self.processing_issues)))
            self.sendMail(subject, message_body)

        # rollback if set
        if self.rollback: raise Exception("-- Throw exception to roll back ermrest update")
        
        return 0


    """
    Get UTC Submission Time 
    """
    def getTimeUTC(self, submission_str):
        submission_obj = dt.fromisoformat(submission_str)
        #submission_utc = submission_obj.replace(hour=21,tzinfo=timezone.utc).isoformat()
        submission_utc = submission_obj.astimezone(timezone.utc).isoformat()
        return f'{submission_utc}'

    """
    Get the corresponding Zip file of the hatrac filename  
    """
    def getSubmittedZipFile(self, filename):
        if 'hatrac' not in filename:
            return filename
        parts = filename.split('/')
        name = f'{parts[-1].split(":")[0].lower()}.gz'
        ret='/'.join(parts[0:7]+[name])
        return ret

    """
    Sort JSON object 
    """
    def order_dict(self, dictionary):
        result = {}
        for k, v in sorted(dictionary.items()):
            if isinstance(v, dict):
                result[k] = self.order_dict(v)
            elif isinstance(v, list):
                result[k] = sorted(v)
            else:
                result[k] = v
        return result

    """
    Get the archive file directory. NOT USED
    """
    def getFileArchiveDirectory(self, hash, entry_id, file_type):
        entry_dir = f'{self.archive_parent}/{self.released_entry_dir}/{hash}/{entry_id}/{self.archive_category_dir_names[self.archive_category[file_type]]}'
    
        return entry_dir
    
    """
    Get the file archive subdirectory. NOT USED 
    """
    def getFileArchiveSubDirectory(self, hash, entry_id, archive_category):
        entry_dir = f'{self.released_entry_dir}/{hash}/{entry_id}/'
        return f'{entry_dir}{self.archive_category_dir_names[archive_category]}'


    def getFileArchiveSubDirectory2(self, accession_code, category_dir_name):
        hash = self.get_hash(accession_code)
        entry_dir = f'{self.released_entry_dir}/{hash}/{accession_code.lower()}'
        return f'{entry_dir}/{category_dir_name}'
    
    """
    Get the archive date and time
    The datetime generated is in pacific time which is the same as Deriva server timezone
    """
    def getArchiveDate(self):
        """
        Get the closed Thursday
        If the current date is before Thursday 2PM America/Los_Angeles:
            then return the Thursday of the current week
            else return the Thursday of the next week
        """
        #now = dt.now(timezone.utc)
        self.cutoff_day_of_week = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_wday
        now = dt.now(pytz.timezone('America/Los_Angeles'))
        closed_day_of_week = now + timedelta(days=(self.cutoff_day_of_week - now.weekday())%7)
        weekday = now.weekday()
        cutoff_hour_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_hour
        cutoff_minute_pacific = time.strptime(self.cutoff_time_pacific, "%A %H:%M").tm_min
        if weekday == self.cutoff_day_of_week:
            """
            It is Thursday. Check the time.
            """
            hour = now.hour
            minute = now.minute
            if hour > cutoff_hour_pacific or hour == cutoff_hour_pacific and minute > cutoff_minute_pacific:
                """
                The UTC time has passed 9PM. Get the Thursday of next week
                The America/Los_Angeles time has passed 3PM. Get the Thursday of next week
                """
                closed_day_of_week += timedelta(days=7)
        closed_day_of_week=closed_day_of_week.replace(hour=cutoff_hour_pacific,minute=cutoff_minute_pacific,second=0,microsecond=0)
        return f'{closed_day_of_week}'

    """
    Get previous archive date
    """
    def getPreviousArchiveDate(self, current_submission_time):
        """
        Get the latest Submission_Date that is less than the current submission Date. Otherwise, use Current_Submission_Date - 7 days
        """
        previous_submission_time = f'{dt.fromisoformat(current_submission_time) - timedelta(days=7)}'
        self.logger.debug(f'Submission Dates: {current_submission_time}, {previous_submission_time}') 
        url = f'/aggregate/A:=PDB:PDB_Archive/Submission_Time::lt::{urlquote(current_submission_time)}/previous_submission_time:=max(Submission_Time)'
        self.logger.debug(f"Query to find the maximum submission time:\n\nhttps://{self.host}/ermrest/catalog/{self.catalog_id}{url}\n") 
        resp = self.catalog.get(url)
        resp.raise_for_status()
        rows = resp.json()
        self.logger.debug(f'Previous submission time\n\n{json.dumps(rows, indent=4)}\n')
        if len(rows) > 0 and rows[0]['previous_submission_time'] != None:
            previous_submission_time = rows[0]['previous_submission_time'].replace("T", " ")
        return previous_submission_time
        
    """
    Extract the file from hatrac
    """
    def getHatracFile(self, file_url, input_dir, filename):
        try:
            error_message = None
            hatracFile = '{}/{}'.format(input_dir, filename)
            self.store.get_obj(file_url, destfilename=hatracFile)
            self.logger.debug('File "%s", %d bytes.' % (hatracFile, os.stat(hatracFile).st_size)) 
            return (hatracFile, error_message)
        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, '%s\n' % ''.join(traceback.format_exception(et, ev, tb)))
            error_message = 'ERROR getHatracFile: "%s"' % str(ev)
            return (None,error_message)
        

    """
    Get the PDB_Accession_Code hash
    """
    def get_hash(self, accession_code):
        if len(accession_code) == 4:
            h = accession_code[1:3].lower()
            return h            
        else:
            raise Exception("ERROR: Accession_Code %s is not a PDB accession code. Expect legnth of 4" % (accession_code))

    """
    Generate the holding zip file 
    """
    def generateGzip(self, input_file_path, output_dir=None, delete_input=True):
        input_dir, filename = input_file_path.rsplit("/", 1)
        if not output_dir: output_dir = input_dir
        os.makedirs(output_dir, exist_ok=True)        
        output_fpath = f'{output_dir}/{filename}.gz'
        with open(input_file_path, 'rb') as f_in:
            with gzip.open(output_fpath, 'wb') as f_out:
                shutil.copyfileobj(f_in, f_out)
        if delete_input: os.remove(input_file_path)
        
        #print("generateGzip: %s %s" % (input_dir, filename))
        return output_fpath
    

    """
    Store the  file into hatrac
    """
    def storeFileInHatrac(self, hatrac_namespace, file_path):
        try:
            file_name = file_path.rsplit("/", 1)[1]
            newFile = file_path
            file_size = os.path.getsize(newFile)
            hashes = hu.compute_file_hashes(newFile, hashes=['md5', 'sha256'])
            new_md5 = hashes['md5'][1]
            new_sha256 = hashes['sha256'][1]
            hexa_md5 = hashes['md5'][0]
            new_uri = '{}/{}'.format(hatrac_namespace, urlquote(file_name))
            chunked = True if file_size > DEFAULT_CHUNK_SIZE else False
            
            """
            Store the file in hatrac if it is not already
            """
            hatrac_URI = None
            try:
                outfile = '{}.hatrac'.format(newFile)
                r = self.store.get_obj(new_uri, destfilename=outfile)
                hatrac_URI = r.headers['Content-Location']
                old_hatrac_URI = hatrac_URI
                hashes = hu.compute_file_hashes(outfile, hashes=['md5', 'sha256'])
                old_hexa_md5 = hashes['md5'][0]
                os.remove(outfile)
            except:
                os.remove(outfile)
                old_hexa_md5 = None
            
            if hatrac_URI != None and hexa_md5 == old_hexa_md5:
                self.logger.debug('Skipping the upload of the file "%s" as it already exists hatrac.' % file_name)
            else:
                if mimetypes.inited == False:
                    mimetypes.init()
                content_type,encoding = mimetypes.guess_type(newFile)
                if content_type == None:
                    content_type = 'application/octet-stream'
                try:
                    hatrac_URI = self.store.put_loc(
                        new_uri,
                        newFile,
                        headers={'Content-Type': content_type},
                        content_disposition = "filename*=UTF-8''%s" % urlquote(file_name),
                        md5 = new_md5,
                        sha256 = new_sha256,
                        content_type = content_type,
                        chunked = chunked
                    )
                except NotModified:
                    hatrac_URI = old_hatrac_URI
                    self.logger.debug(f'{file_name} NotModified')
                except:
                    et, ev, tb = sys.exc_info()
                    self.logger.error('Can not upload file "%s" in hatrac "%s". Error: "%s"' % (file_name, new_uri, str(ev)))
                    self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
                    subject = 'PDB-Dev Error archiving files.'
                    self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
                    return (None, None, None, None)
            return (hatrac_URI, file_name, file_size, hexa_md5)

        except:
            et, ev, tb = sys.exc_info()
            self.logger.error('got unexpected exception "%s"' % str(ev))
            self.logger.error('%s' % ''.join(traceback.format_exception(et, ev, tb)))
            subject = 'PDB-Dev Error archiving files.'
            self.sendMail(subject, 'Can not upload file "{}" in hatrac at location "{}":\n{}\n'.format(file_name, new_uri, ''.join(traceback.format_exception(et, ev, tb))))
            return (None, None, None, None)


"""
Examples of the manifest files

released_structures_last_modified_dates.json
{
    "8ZZ1": "2024-08-08T21:00:00+00:00",
    "8ZZ2": "2024-08-08T21:00:00+00:00",
    "8ZZ3": "2024-08-08T21:00:00+00:00",
    "8ZZ4": "2024-08-08T21:00:00+00:00",
    "8ZZ5": "2024-08-08T21:00:00+00:00"
}

current_file_holdings.json
{
    "8ZZ1": {
        "mmcif": [
            "/pdb_ihm/data/entries/zz/8zz1/structures/8zz1.cif.gz"
        ],
        "validation_report": [
            "/pdb_ihm/data/entries/zz/8zz1/validation_reports/8zz1_full_validation.pdf.gz",
            "/pdb_ihm/data/entries/zz/8zz1/validation_reports/8zz1_summary_validation.pdf.gz"
        ]
    },
    "8ZZ2": {
        "mmcif": [
            "/pdb_ihm/data/entries/zz/8zz2/structures/8zz2.cif.gz"
        ],
        "validation_report": [
            "/pdb_ihm/data/entries/zz/8zz2/validation_reports/8zz2_full_validation.pdf.gz",
            "/pdb_ihm/data/entries/zz/8zz2/validation_reports/8zz2_summary_validation.pdf.gz"
        ]
    },
    "8ZZ3": {
        "mmcif": [
            "/pdb_ihm/data/entries/zz/8zz3/structures/8zz3.cif.gz"
        ],
        "validation_report": [
            "/pdb_ihm/data/entries/zz/8zz3/validation_reports/8zz3_full_validation.pdf.gz",
            "/pdb_ihm/data/entries/zz/8zz3/validation_reports/8zz3_summary_validation.pdf.gz"
        ]
    },
}

unreleased_entries.json
{
"PDBDEV_00000367" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-01-22T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000368" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-05T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000377" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-03-12T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000378" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
},
"PDBDEV_00000379" : {
    "status_code" : "HOLD",
    "deposit_date" : "2024-02-02T00:00:00+00:00",
    "prerelease_sequence_available_flag" : "N"
}
}

"""
       
